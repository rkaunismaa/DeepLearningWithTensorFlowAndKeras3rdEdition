{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b438a25-0bf6-4b3e-bb68-b45d992b71a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RNN topologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd2d08a-ce58-4a15-9640-d9fc3ca48834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second pass ...\n",
    "# Run Date: Wednesday, February 15, 2023\n",
    "# Run Time: 00:26:11\n",
    "\n",
    "# First pass ...\n",
    "# Run Date: Wednesday, February 15, 2023\n",
    "# Run Time: 00:26:08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019db76d-b055-4a87-ba62-828e06581789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import date\n",
    "\n",
    "startTime = time.time()\n",
    "todaysDate = date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dff387-2468-4e64-b660-f9699bca7439",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example - One-to-many - Learning to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a6d6a-9af1-4533-baeb-31c2313fb37d",
   "metadata": {},
   "source": [
    "(code source: Chapter_5/alice_text_generator.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba767e2-c48f-46cd-87b4-62414b3e7aba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 15:30:00.229798: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-15 15:30:00.342273: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac328a4-e248-4c9e-b151-dc3530389fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "CHECKPOINT_DIR = os.path.join(DATA_DIR, \"checkpoints\")\n",
    "LOG_DIR = os.path.join(DATA_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3319ea7a-2aa6-47ec-9088-cb29e0f87417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_read(urls):\n",
    "    texts = []\n",
    "    for i, url in enumerate(urls):\n",
    "        p = tf.keras.utils.get_file(\"ex1-{:d}.txt\".format(i), url,\n",
    "            cache_dir=\".\")\n",
    "        text = open(p, mode=\"r\", encoding=\"utf-8\").read()\n",
    "        # remove byte order mark\n",
    "        text = text.replace(\"\\ufeff\", \"\")\n",
    "        # remove newlines\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = re.sub(r'\\s+', \" \", text)\n",
    "        # add it to the list\n",
    "        texts.extend(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68d34958-7bf4-4d8f-b821-ff123de7cf74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download and read into local data structure (list of chars)\n",
    "texts = download_and_read([\n",
    "    \"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\",\n",
    "    \"https://www.gutenberg.org/files/12/12-0.txt\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6deddc3-e79f-4c3e-aa7b-e276eeac7ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 92\n"
     ]
    }
   ],
   "source": [
    "# create the vocabulary\n",
    "vocab = sorted(set(texts))\n",
    "print(\"vocab size: {:d}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fc720d2-6c98-4e52-a07a-74bd41f05c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create mapping from vocab chars to ints\n",
    "char2idx = {c:i for i, c in enumerate(vocab)}\n",
    "idx2char = {i:c for c, i in char2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d101d31-4b5e-4351-9151-e2d11dda699e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 15:30:01.680780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-15 15:30:01.680963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-15 15:30:01.682711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-15 15:30:01.682859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-15 15:30:01.682994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-15 15:30:01.683126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-15 15:30:01.683237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2006] Ignoring visible gpu device (device: 1, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1) with core count: 5. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\n",
      "2023-02-15 15:30:01.683526: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-15 15:30:01.683872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-15 15:30:01.684015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-15 15:30:01.684240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-15 15:30:02.124851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-15 15:30:02.125044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-15 15:30:02.125182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-15 15:30:02.125305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6649 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# numericize the texts\n",
    "texts_as_ints = np.array([char2idx[c] for c in texts])\n",
    "data = tf.data.Dataset.from_tensor_slices(texts_as_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57666b6b-cc16-412d-bb06-c80328fca323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_train_labels(sequence):\n",
    "    input_seq = sequence[0:-1]\n",
    "    output_seq = sequence[1:]\n",
    "    return input_seq, output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa122978-1647-4451-874d-042a2f71d508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of characters to show before asking for prediction\n",
    "# sequences: [None, 100]\n",
    "seq_length = 100\n",
    "sequences = data.batch(seq_length + 1, drop_remainder=True)\n",
    "sequences = sequences.map(split_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70d0eaf9-4abb-4561-b2ba-df7d84f0a17d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# set up for training\n",
    "# batches: [None, 64, 100]\n",
    "batch_size = 64\n",
    "steps_per_epoch = len(texts) // seq_length // batch_size\n",
    "dataset = sequences.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81f97dbc-062b-4183-8512-0fd70814d432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CharGenModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, num_timesteps, \n",
    "            embedding_dim, **kwargs):\n",
    "        super(CharGenModel, self).__init__(**kwargs)\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim\n",
    "        )\n",
    "        self.rnn_layer = tf.keras.layers.GRU(\n",
    "            num_timesteps,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "            recurrent_activation=\"sigmoid\",\n",
    "            stateful=True,\n",
    "            return_sequences=True\n",
    "        )\n",
    "        self.dense_layer = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.rnn_layer(x)\n",
    "        x = self.dense_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e9df3d6-4c47-482f-b466-5dbf29637892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define network\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cc00f5f-8c61-4fbc-9dd6-25398f2af2c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"char_gen_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  23552     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  107400    \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  9292      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140,244\n",
      "Trainable params: 140,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CharGenModel(vocab_size, seq_length, embedding_dim)\n",
    "model.build(input_shape=(batch_size, seq_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b598567-e57d-47b1-a570-bdc077a878ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(labels, predictions):\n",
    "    return tf.losses.sparse_categorical_crossentropy(\n",
    "        labels,\n",
    "        predictions,\n",
    "        from_logits=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47484dac-356b-423f-837e-3b01476e45b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.optimizers.Adam(), loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "398832fc-cb79-4ea9-8c18-37e1cf9e33b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(model, prefix_string, char2idx, idx2char,\n",
    "        num_chars_to_generate=1000, temperature=1.0):\n",
    "    input = [char2idx[s] for s in prefix_string]\n",
    "    input = tf.expand_dims(input, 0)\n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    for i in range(num_chars_to_generate):\n",
    "        preds = model(input)\n",
    "        preds = tf.squeeze(preds, 0) / temperature\n",
    "        # predict char returned by model\n",
    "        pred_id = tf.random.categorical(preds, num_samples=1)[-1, 0].numpy()\n",
    "        text_generated.append(idx2char[pred_id])\n",
    "        # pass the prediction as the next input to the model\n",
    "        input = tf.expand_dims([pred_id], 0)\n",
    "\n",
    "    return prefix_string + \"\".join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9abaf2c-5b88-448b-be94-5d18e4296d52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 15:30:04.028134: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 3s 9ms/step - loss: 3.5114\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 2.7755\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 2.4769\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 2.3431\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 2.2303\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 2.1364\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 2.0655\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.9968\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.9399\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.9003\n",
      "after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1after epoch: 1\n",
      "Alice KnijedsX Be't vand goblied Queen, said herpeam: it with worke with the itcount a over gook for it ther wasse fare a buged it aty at mods had and had one it the roorsentwos sime woll nore cratis apy, and of stor qulding nothow it fir the leagree of iis ave seet wether inied, youiry inal tumeread the rad, underenborgull te off, shuthing Qeeeanibbone. The bedris beeved in anev—I said the never hinn't of as inse, younch evint wasuting frow. He ore looking was for to ining if and its, wivess!” “A’s in tich \"thing, might I cay was arcupt! I coreadr: ald apile, ane a fast unds_ I had thard-Anch, ave a ands you sumeevere: ornwa hall [ursededfer!\" peash in the ‘Buzn’t toor methe begateran. \"“of queat eadn I wold withty, \"I voul, the Dorage of, the hadmighthins, you marged tile,\" seem the Quveer itly, “y ming as croppen pikning that be!” Alice. Here mac_ple, and of lookingaind _telf, and hade tolims wouther-_m it, and lettlaitise. \"SHow no out, Ang dow tro) he rigraplousear that talking weples. \n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 1.8546\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.8168\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 1.7860\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.7552\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.7238\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.7083\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.6773\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 1.6583\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.6445\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 1.6247\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
      "after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2after epoch: 2\n",
      "Alice plass, and last Streated: \"a melasth, and fouting of the fore back ibote rathy is any inea \"I Iplaif a with a mughroty wlich. \"O was al Litech. Betif this need. The thought Guten like thought,\" said the Gutenbergher-winked that no some? No, most therest to mistainingoving with all marues. Oh what the sead gove the Queen it works it hus on warn it?\" say. “Thall bat meap, and nony rumge Hadable ferelth her fouts. “Whiting fortle, with he can the wilder thather in the said or odl at lettle all me—voing Alice. \"Project. “Dicit do her alk roussived any one Tiching Lione wonders it UTw, pering, and Alice (and whatk of The Gnabl sakinuthing the oppelt guth to happening about the Mock Alice?” wih is.” IY and there tree up eatifuld them, and in runking mysave of the caudd of the kent of I didulle nome fee hich say, and salder, of the staplo—turious her ineroance, polfor that, if a _dor pety. Hows or anyersation of oution. \"I underledvered, “I'll the_ coulf she-tren got awied agreap to concerste\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 1.6096\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.5955\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.5803\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.5706\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.5548\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.5494\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 1.5294\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.5298\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.5180\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.5091\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
      "after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3after epoch: 3\n",
      "Alice that Alice about happen more with ‘It stand. ELESPTGOIMEV GUSART..S FVISE TQ. The with the hour in firster eyes for no and canar. “Is shomed as now. “Thought for’t agroes off me little reaking warting the Whoppicap a said!” “I was Projuce went over arme soment the Nlarp, off meer her see, go that say of nith on, as if the repupted a Rest state: _much all in the dist eanto so through turny of not quest and as you tamile that right as she long reall of Hole over hands on. \"* * * * * * You _I_ stoss of keepice of it at a little goves, out as partly, and - Alice.\" The finding ease—in) meantar, as talking of dear ow it your tone: ‘whemp to bied it are_, “one with at the copyred in Plice—and the tlas to the for put. Thenfular of the parse. “I none, the rade works to pet ow. Ther_ like the moctle began in the saying tor time in the gettying!” They corsend it was side, look as she shood Aly—and the orthem, but you came I'll put when it could hearly my H. Vaitting, faut ins—and Alice asse to ca\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 1.5002\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 1.4933\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.4887\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.4755\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.4761\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.4641\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 1.4579\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.4539\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 1.4509\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.4439\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
      "after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4after epoch: 4\n",
      "Alice there of U'kine a nothing jossable, feel, diffendary ackion—” A very complied you wizang lust she could be severely, “as he could, and I like,” she thill next of esplaket, and wop_.” She be nece!\" The Queen. She try-do it a lowidnen, For and of all go win got of the put in the Looking all: and imperes?\" \"Well, \"to might greature, will frif, spoved herself. “The Steaky goecome, it would clease, at them. Hourte air. “However, sime filetaim. 1.F.3, (And leave it, got as things to taking his like her song away of my little voice it at thing say, \"he pretwis, \"talking-tmee remisiny size the rad a little written Seemereed herself,” she said. “It’s a lubl promanmen turns of anyto-day troiblade on what It I larges. And what eah would that, his to it?” Alice did it liek of eng’s felmantly, but you see it herer, shritingon't because!” “In a partried and wsos the sudden mouse,” she Red Queen,” said the juxt such shoulder id goesly to-ve—I said: “it works. “That’s set,” snee.\" \"How are “Ding. \"So \n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 1.4386\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.4334\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.4277\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.4241\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 1.4170\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.4146\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 1.4094\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.4075\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.4021\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 1.4016\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
      "after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5\n",
      "Alice this is all you to the wonly to the Marner three by sice Brea caused ‘Thright began it of reightenbles, rememenough--I never,\" Alice works refured again. \"A like a last hish I _noter work it.” Alice same brild excals of the mouth the useragies, down a good very long her, so I've_. “That away!” let your chood R,” Wion whenges it agal the table, Alice must kinding for beattoed, atgo to chootedly spaked!\" Alice took as well this, too sorts mach at wings: e to cours.\" This camich it side,” the Queen say, and conjouset: The trif the other could been booked appribution or things than ‘what a who wonderries can the comply boge the what hum ang the hedgict kin at his poor the great of accuaddd, as twos—them that it wouldn't be stoors.' At she take—both fleestill a long a means of its!\" \"You must to generaluling one another side a shopers can't afreated her except the produnce with manner of March) \"then statising very spoke. \"That’s glabution: \"unfortuntid; \"Conswly: it much from, who atchies \n",
      "---\n",
      "CPU times: user 56.4 s, sys: 2.12 s, total: 58.5 s\n",
      "Wall time: 51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we will train our model for 50 epochs, and after every 10 epochs\n",
    "# we want to see how well it will generate text\n",
    "num_epochs = 50\n",
    "for i in range(num_epochs // 10):\n",
    "    model.fit(\n",
    "        dataset.repeat(),\n",
    "        epochs=10,\n",
    "        steps_per_epoch=steps_per_epoch\n",
    "        # callbacks=[checkpoint_callback, tensorboard_callback]\n",
    "    )\n",
    "    checkpoint_file = os.path.join(\n",
    "        CHECKPOINT_DIR, \"model_epoch_{:d}\".format(i+1))\n",
    "    model.save_weights(checkpoint_file)\n",
    "\n",
    "    # create a generative model using the trained model so far\n",
    "    gen_model = CharGenModel(vocab_size, seq_length, embedding_dim)\n",
    "    gen_model.load_weights(checkpoint_file)\n",
    "    gen_model.build(input_shape=(1, seq_length))\n",
    "\n",
    "    print(\"after epoch: {:d}\".format(i+1)*10)\n",
    "    print(generate_text(gen_model, \"Alice \", char2idx, idx2char))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1f0df70-9797-4368-8aae-a430a3dbd325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_logs():\n",
    "    shutil.rmtree(CHECKPOINT_DIR, ignore_errors=True)\n",
    "    shutil.rmtree(LOG_DIR, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf31dc7b-70c7-48dd-a877-7811332393f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1af3f-ebe8-4276-a06c-1fa5acdd992f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example - Many-to-one - Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51cba11-fd72-4af3-aaa1-e443f7764c50",
   "metadata": {},
   "source": [
    "(code source: Chapter_5/lstm_sentiment_analysis.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d83ee7e1-4354-434f-a782-236d2af8321a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e9c8573-fb70-4ddd-b537-de100fbec397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_logs(data_dir):\n",
    "    logs_dir = os.path.join(data_dir, \"logs\")\n",
    "    shutil.rmtree(logs_dir, ignore_errors=True)\n",
    "    return logs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ac6b09b-2440-48c1-a5de-39f4b3e2da3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# clean up log area\n",
    "data_dir = \"./data\"\n",
    "logs_dir = clean_logs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff1cd0bb-80fe-4a09-a9cf-692e2d053353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    local_file = local_file.replace(\"%20\", \" \")\n",
    "    p = tf.keras.utils.get_file(local_file, url, \n",
    "        extract=True, cache_dir=\".\")\n",
    "    local_folder = os.path.join(\"datasets\", local_file.split('.')[0])\n",
    "    labeled_sentences = []\n",
    "    for labeled_filename in os.listdir(local_folder):\n",
    "        if labeled_filename.endswith(\"_labelled.txt\"):\n",
    "            with open(os.path.join(local_folder, labeled_filename), \"r\") as f:\n",
    "                for line in f:\n",
    "                    sentence, label = line.strip().split('\\t')\n",
    "                    labeled_sentences.append((sentence, label))\n",
    "    return labeled_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1a4a5d2-606e-483f-9b8d-64c6e28f3fff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download and read data into data structures\n",
    "labeled_sentences = download_and_read(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "602960e5-26f7-4739-8095-cfa10e8d3c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [s for (s, l) in labeled_sentences]\n",
    "labels = [int(l) for (s, l) in labeled_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2839c9f8-77c0-4a85-bbf0-c1b9af4fd56c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 5271\n"
     ]
    }
   ],
   "source": [
    "# tokenize sentences\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_counts)\n",
    "print(\"vocabulary size: {:d}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e320827-4135-4209-a915-5918d8ce87d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for (k, v) in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2458c4e-54d2-4e6b-8e1a-ef0ab0fa80a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seq_lengths = np.array([len(s.split()) for s in sentences])\n",
    "# print([(p, np.percentile(seq_lengths, p)) for p \n",
    "#     in [75, 80, 90, 95, 99, 100]])\n",
    "# [(75, 16.0), (80, 18.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]\n",
    "max_seqlen = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2217a47c-92f8-4695-bf30-dedf13cb2598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "sentences_as_ints = tokenizer.texts_to_sequences(sentences)\n",
    "sentences_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sentences_as_ints, maxlen=max_seqlen)\n",
    "labels_as_ints = np.array(labels)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (sentences_as_ints, labels_as_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce5b47ff-0e22-4bee-b985-7b984ea1f680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = len(sentences) // 3\n",
    "val_size = (len(sentences) - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0eb1fb0a-f96b-4f49-97b6-2dec18b6d2dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92497665-4f16-472d-aaf6-4d4cfe9ee617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, max_seqlen, **kwargs):\n",
    "        super(SentimentAnalysisModel, self).__init__(**kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, max_seqlen)\n",
    "        self.bilstm = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(max_seqlen)\n",
    "        )\n",
    "        self.dense = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.bilstm(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f2566ff-06be-4492-b96b-d3a81c380303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sentiment_analysis_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     multiple                  337408    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  multiple                 66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_6 (Dense)             multiple                  8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             multiple                  65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 411,777\n",
      "Trainable params: 411,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "# vocab_size + 1 to account for PAD character\n",
    "model = SentimentAnalysisModel(vocab_size+1, max_seqlen)\n",
    "model.build(input_shape=(batch_size, max_seqlen))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10db6204-10d9-4e7c-8dff-e3fa128eacde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bff8b4dd-2523-4c3a-bf00-af67bd6c50ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 3s 24ms/step - loss: 0.6918 - accuracy: 0.5233 - val_loss: 0.6823 - val_accuracy: 0.7050\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.6325 - accuracy: 0.7333 - val_loss: 0.5106 - val_accuracy: 0.7900\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.4137 - accuracy: 0.8300 - val_loss: 0.2944 - val_accuracy: 0.9050\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.2584 - accuracy: 0.9144 - val_loss: 0.1797 - val_accuracy: 0.9400\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.1629 - accuracy: 0.9472 - val_loss: 0.1608 - val_accuracy: 0.9450\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1305 - accuracy: 0.9600 - val_loss: 0.1063 - val_accuracy: 0.9800\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0904 - accuracy: 0.9778 - val_loss: 0.0464 - val_accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0612 - accuracy: 0.9822 - val_loss: 0.0324 - val_accuracy: 0.9950\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0459 - accuracy: 0.9878 - val_loss: 0.0436 - val_accuracy: 0.9950\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0311 - accuracy: 0.9944 - val_loss: 0.1018 - val_accuracy: 0.9850\n",
      "CPU times: user 4.97 s, sys: 119 ms, total: 5.09 s\n",
      "Wall time: 4.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train\n",
    "best_model_file = os.path.join(data_dir, \"best_model.h5\")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "num_epochs = 10\n",
    "history = model.fit(train_dataset, epochs=num_epochs, \n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31adfb2d-34fd-46aa-8084-a8ba4b3aac41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate with test set\n",
    "best_model = SentimentAnalysisModel(vocab_size+1, max_seqlen)\n",
    "best_model.build(input_shape=(batch_size, max_seqlen))\n",
    "best_model.load_weights(best_model_file)\n",
    "best_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b99e40fc-bccc-417c-9bda-118c59e3bb28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 3ms/step - loss: 0.0653 - accuracy: 0.9850\n",
      "test loss: 0.065, test accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = best_model.evaluate(test_dataset)\n",
    "print(\"test loss: {:.3f}, test accuracy: {:.3f}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0280ba0-c23e-4cf8-8521-85586a6a8fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "0\t0\tall in all i'd expected a better consumer experience from motorola\n",
      "0\t0\ti don't recommend unless your car breaks down in front of it and you are starving\n",
      "0\t0\tvery much disappointed with this company\n",
      "1\t1\tcool phone\n",
      "0\t0\tbuy a different phone but not this\n",
      "0\t0\tonce your food arrives it's meh\n",
      "1\t1\tthe vanilla ice cream was creamy and smooth while the profiterole choux pastry was fresh enough\n",
      "1\t1\tthe best scene in the movie was when gerardo is trying to find a song that keeps running through his head\n",
      "1\t1\twe thought you'd have to venture further away to get good sushi but this place really hit the spot that night\n",
      "0\t0\tthen i had to continue pairing it periodically since it somehow kept dropping\n",
      "0\t0\tawkward to use and unreliable\n",
      "1\t1\ti use this product in a motor control center where there is a lot of high voltage humming from the equipment and it works great\n",
      "0\t0\ti know that jim o'connor was very energetic and that nobody could be as much as him but george was well dull\n",
      "0\t0\tthis place is not worth your time let alone vegas\n",
      "1\t1\tvery very fun chef\n",
      "0\t0\tthe place was fairly clean but the food simply wasn't worth it\n",
      "1\t1\ti own 2 of these cases and would order another\n",
      "1\t1\ti posted more detailed comments under the grey or black phone but i have the fire red and it's a great color\n",
      "0\t0\tthe film lacks any real scares or tension some of the medical terminology used throughout is a bit iffy to say the least i say that as an insulin dependant diabetic myself\n",
      "0\t0\ti never walked out of a movie faster\n",
      "1\t1\tall of the tapas dishes were delicious\n",
      "0\t0\ti have been to very few places to eat that under no circumstances would i ever return to and this tops the list\n",
      "0\t0\tyour servers suck wait correction our server heimer sucked\n",
      "1\t1\twill be back again\n",
      "1\t1\tdelicious\n",
      "1\t1\tup there with the best of melville\n",
      "0\t0\ti probably won't be coming back here\n",
      "1\t1\tnice leather\n",
      "1\t1\tloved it friendly servers great food wonderful and imaginative menu\n",
      "1\t1\tthat just screams legit in my book somethat's also pretty rare here in vegas\n",
      "0\t0\tit is just the sort of pap that is screened in the afternoon to punish the unemployed for not having jobs\n",
      "0\t0\tthese are the central themes of the film and they are handled ineptly stereotypically and with no depth of imagination\n",
      "1\t1\tthey were excellent\n",
      "0\t0\teven allowing for poor production values for the time 1971 and the format some kind of mini series this is baaaaaad\n",
      "1\t1\ti love the look and feel of samsung flipphones\n",
      "0\t0\tstay away from the q\n",
      "1\t1\tthe food was very good\n",
      "0\t0\tafter all the rave reviews i couldn't wait to eat here what a disappointment\n",
      "1\t1\tgood service very clean and inexpensive to boot\n",
      "0\t0\tfriend's pasta also bad he barely touched it\n",
      "0\t0\tthe only very disappointing thing was there was no speakerphone\n",
      "1\t1\tour server was fantastic and when he found out the wife loves roasted garlic and bone marrow he added extra to our meal and another marrow to go\n",
      "1\t1\tour waiter was very attentive friendly and informative\n",
      "1\t1\ttheir steaks are 100 recommended\n",
      "0\t0\tthe desserts were a bit strange\n",
      "1\t1\tit rocked my world and is certainly a must see for anyone with no social or physical outlets\n",
      "0\t0\tpoor voice clarity\n",
      "1\t1\tthe bartender was also nice\n",
      "0\t0\thowever my girl was complain that some time the phone doesn't wake up like normal phone does\n",
      "1\t1\tas always the evening was wonderful and the food delicious\n",
      "1\t1\tvery good phone\n",
      "1\t1\tthe eargels channel the sound directly into your ear and seem to increase the sound volume and clarity\n",
      "0\t0\tyou could drive a semi truck into these holes\n",
      "0\t0\tthe plot doesn't hang together at all and the acting is absolutely appalling\n",
      "1\t1\tray charles is legendary\n",
      "0\t0\ti didn't think that the instructions provided were helpful to me\n",
      "0\t0\tthen there's the plot holes\n",
      "1\t1\tbut this understated film leaves a lasting impression\n",
      "0\t0\ti wish i could return the unit and get back my money\n",
      "0\t0\tshrimp when i unwrapped it i live only 1 2 a mile from brushfire it was literally ice cold\n",
      "1\t1\tplus it's only 8 bucks\n",
      "0\t0\tthe new characters weren't all that memorable and i found myself forgetting who was who\n",
      "0\t0\tthe ear buds only play music in one ear\n",
      "0\t0\twhat a waste of time\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "CPU times: user 1.77 s, sys: 39.9 ms, total: 1.81 s\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# predict on batches\n",
    "labels, predictions = [], []\n",
    "idx2word[0] = \"PAD\"\n",
    "is_first_batch = True\n",
    "for test_batch in test_dataset:\n",
    "    inputs_b, labels_b = test_batch\n",
    "    pred_batch = best_model.predict(inputs_b)\n",
    "    predictions.extend([(1 if p > 0.5 else 0) for p in pred_batch])\n",
    "    labels.extend([l for l in labels_b])\n",
    "    if is_first_batch:\n",
    "        for rid in range(inputs_b.shape[0]):\n",
    "            words = [idx2word[idx] for idx in inputs_b[rid].numpy()]\n",
    "            words = [w for w in words if w != \"PAD\"]\n",
    "            sentence = \" \".join(words)\n",
    "            print(\"{:d}\\t{:d}\\t{:s}\".format(labels[rid], predictions[rid], sentence))\n",
    "        is_first_batch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8a1207f-e2ec-4dfa-81e9-4ac1dec9fd20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.994\n",
      "confusion matrix\n",
      "[[523   2]\n",
      " [  4 471]]\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy score: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf6239-5142-492c-8471-d7f739a56916",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example - Many-to-many - POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf672d1e-c737-4552-ad25-44aed3f73c59",
   "metadata": {},
   "source": [
    "(code source: Chapter_5/gru_pos_tagger.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "869a1ca3-0b18-4864-8319-fe204deb8691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88383930-8deb-4102-aa33-0ce9b4057070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1383289b-6b73-491a-a445-fc635661005f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d06df98-3596-461a-b9aa-3e4fddddb4f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbf2abac-d659-4d67-aac2-037e5916a1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_PAIRS = None\n",
    "EMBEDDING_DIM = 128\n",
    "RNN_OUTPUT_DIM = 256\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7e9695b-5abf-402e-a07e-e478be96dc3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set random seed\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf608cda-5511-4770-b3dd-55416e64c6f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_logs(data_dir):\n",
    "    logs_dir = os.path.join(data_dir, \"logs\")\n",
    "    shutil.rmtree(logs_dir, ignore_errors=True)\n",
    "    return logs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16ed27ca-5be0-4cc5-b8b1-19823a32417b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean up log area\n",
    "data_dir = \"./data\"\n",
    "logs_dir = clean_logs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6dfa8600-7672-491a-92e9-d4c5d2f08136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_read(dataset_dir, num_pairs=None):\n",
    "    sent_filename = os.path.join(dataset_dir, \"treebank-sents.txt\")\n",
    "    poss_filename = os.path.join(dataset_dir, \"treebank-poss.txt\")\n",
    "    if not(os.path.exists(sent_filename) and os.path.exists(poss_filename)):\n",
    "        import nltk    \n",
    "\n",
    "        if not os.path.exists(dataset_dir):\n",
    "            os.makedirs(dataset_dir)\n",
    "        fsents = open(sent_filename, \"w\")\n",
    "        fposs = open(poss_filename, \"w\")\n",
    "        sentences = nltk.corpus.treebank.tagged_sents()\n",
    "        for sent in sentences:\n",
    "            fsents.write(\" \".join([w for w, p in sent]) + \"\\n\")\n",
    "            fposs.write(\" \".join([p for w, p in sent]) + \"\\n\")\n",
    "\n",
    "        fsents.close()\n",
    "        fposs.close()\n",
    "    sents, poss = [], []\n",
    "    with open(sent_filename, \"r\") as fsent:\n",
    "        for idx, line in enumerate(fsent):\n",
    "            sents.append(line.strip())\n",
    "            if num_pairs is not None and idx >= num_pairs:\n",
    "                break\n",
    "    with open(poss_filename, \"r\") as fposs:\n",
    "        for idx, line in enumerate(fposs):\n",
    "            poss.append(line.strip())\n",
    "            if num_pairs is not None and idx >= num_pairs:\n",
    "                break\n",
    "    return sents, poss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34f42a16-062b-4a61-98b5-fc3058b1ee7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of records: 3914\n"
     ]
    }
   ],
   "source": [
    "# download and read source and target data into data structure\n",
    "sents, poss = download_and_read(\"./datasets\", num_pairs=NUM_PAIRS)\n",
    "assert(len(sents) == len(poss))\n",
    "print(\"# of records: {:d}\".format(len(sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e760b0be-2b56-4e0a-8cce-12c3b4b9a146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_build_vocab(texts, vocab_size=None, lower=True):\n",
    "    if vocab_size is None:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)\n",
    "    else:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size+1, oov_token=\"UNK\", lower=lower)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    if vocab_size is not None:\n",
    "        # additional workaround, see issue 8092\n",
    "        # https://github.com/keras-team/keras/issues/8092\n",
    "        tokenizer.word_index = {e:i for e, i in tokenizer.word_index.items() \n",
    "            if i <= vocab_size+1 }\n",
    "    word2idx = tokenizer.word_index\n",
    "    idx2word = {v:k for k, v in word2idx.items()}\n",
    "    return word2idx, idx2word, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca8bfcef-c565-413c-8b6e-908887e901fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab sizes (source): 9001, (target): 39\n"
     ]
    }
   ],
   "source": [
    "# vocabulary sizes\n",
    "word2idx_s, idx2word_s, tokenizer_s = tokenize_and_build_vocab(\n",
    "    sents, vocab_size=9000)\n",
    "word2idx_t, idx2word_t, tokenizer_t = tokenize_and_build_vocab(\n",
    "    poss, vocab_size=38, lower=False)\n",
    "source_vocab_size = len(word2idx_s)\n",
    "target_vocab_size = len(word2idx_t)\n",
    "print(\"vocab sizes (source): {:d}, (target): {:d}\".format(\n",
    "    source_vocab_size, target_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b77429a-d687-4357-a17b-8207c4e276cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # max sequence length - these should be identical on source and\n",
    "# # target so we can just analyze one of them and choose max_seqlen\n",
    "# sequence_lengths = np.array([len(s.split()) for s in sents])\n",
    "# print([(p, np.percentile(sequence_lengths, p)) \n",
    "#     for p in [75, 80, 90, 95, 99, 100]])\n",
    "# # [(75, 33.0), (80, 35.0), (90, 41.0), (95, 47.0), (99, 58.0), (100, 271.0)]\n",
    "max_seqlen = 271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d38a7aac-1908-470d-b99e-319d3dadc6d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "sents_as_ints = tokenizer_s.texts_to_sequences(sents)\n",
    "sents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sents_as_ints, maxlen=max_seqlen, padding=\"post\")\n",
    "poss_as_ints = tokenizer_t.texts_to_sequences(poss)\n",
    "poss_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    poss_as_ints, maxlen=max_seqlen, padding=\"post\")\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (sents_as_ints, poss_as_ints))\n",
    "idx2word_s[0], idx2word_t[0] = \"PAD\", \"PAD\"\n",
    "poss_as_catints = []\n",
    "for p in poss_as_ints:\n",
    "    poss_as_catints.append(tf.keras.utils.to_categorical(p, \n",
    "        num_classes=target_vocab_size, dtype=\"int32\"))\n",
    "poss_as_catints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    poss_as_catints, maxlen=max_seqlen)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (sents_as_ints, poss_as_catints))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21da511f-c21d-4830-936c-474407c602cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split into training, validation, and test datasets\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = len(sents) // 3\n",
    "val_size = (len(sents) - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "357de099-763d-431b-bb87-5398dc18482c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create batches\n",
    "batch_size = BATCH_SIZE\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ebb4e13-9f04-4ba8-9976-fbb7b4d09069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class POSTaggingModel(tf.keras.Model):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size,\n",
    "            embedding_dim, max_seqlen, rnn_output_dim, **kwargs):\n",
    "        super(POSTaggingModel, self).__init__(**kwargs)\n",
    "        self.embed = tf.keras.layers.Embedding(\n",
    "            source_vocab_size, embedding_dim, input_length=max_seqlen)\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(rnn_output_dim, return_sequences=True))\n",
    "        self.dense = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(target_vocab_size))\n",
    "        self.activation = tf.keras.layers.Activation(\"softmax\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.rnn(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1cc33c8-d60f-4b3d-b472-eece8da1c7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pos_tagging_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     multiple                  1152128   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  multiple                 0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  multiple                 592896    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  multiple                 20007     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " activation (Activation)     multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,765,031\n",
      "Trainable params: 1,765,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "embedding_dim = EMBEDDING_DIM\n",
    "rnn_output_dim = RNN_OUTPUT_DIM\n",
    "\n",
    "model = POSTaggingModel(source_vocab_size, target_vocab_size,\n",
    "    embedding_dim, max_seqlen, rnn_output_dim)\n",
    "model.build(input_shape=(batch_size, max_seqlen))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ee609ad-e736-4603-9785-750aaf25f0d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def masked_accuracy():\n",
    "    def masked_accuracy_fn(ytrue, ypred):\n",
    "        ytrue = tf.keras.backend.argmax(ytrue, axis=-1)\n",
    "        ypred = tf.keras.backend.argmax(ypred, axis=-1)\n",
    " \n",
    "        mask = tf.keras.backend.cast(\n",
    "            tf.keras.backend.not_equal(ypred, 0), tf.int32)\n",
    "        matches = tf.keras.backend.cast(\n",
    "            tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask\n",
    "        numer = tf.keras.backend.sum(matches)\n",
    "        denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask), 1)\n",
    "        accuracy =  numer / denom\n",
    "        return accuracy\n",
    "\n",
    "    return masked_accuracy_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d5a3305-f65c-477e-aebc-b253a392c86e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\", masked_accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dfa0c12e-198d-476f-9858-43e709a29be0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19/19 [==============================] - 4s 84ms/step - loss: 1.4125 - accuracy: 0.8654 - masked_accuracy_fn: 0.0016 - val_loss: 0.3124 - val_accuracy: 0.9171 - val_masked_accuracy_fn: 0.1278\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.3283 - accuracy: 0.9188 - masked_accuracy_fn: 0.0882 - val_loss: 0.3215 - val_accuracy: 0.9242 - val_masked_accuracy_fn: 0.1497\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.3173 - accuracy: 0.9257 - masked_accuracy_fn: 0.1873 - val_loss: 0.3290 - val_accuracy: 0.9178 - val_masked_accuracy_fn: 0.1820\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.2993 - accuracy: 0.9208 - masked_accuracy_fn: 0.1542 - val_loss: 0.3022 - val_accuracy: 0.9157 - val_masked_accuracy_fn: 0.1006\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.2761 - accuracy: 0.9210 - masked_accuracy_fn: 0.1274 - val_loss: 0.2567 - val_accuracy: 0.9262 - val_masked_accuracy_fn: 0.1365\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.2554 - accuracy: 0.9265 - masked_accuracy_fn: 0.1481 - val_loss: 0.2577 - val_accuracy: 0.9243 - val_masked_accuracy_fn: 0.1459\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.2507 - accuracy: 0.9258 - masked_accuracy_fn: 0.1417 - val_loss: 0.2438 - val_accuracy: 0.9278 - val_masked_accuracy_fn: 0.1516\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.2441 - accuracy: 0.9288 - masked_accuracy_fn: 0.1700 - val_loss: 0.2402 - val_accuracy: 0.9296 - val_masked_accuracy_fn: 0.1727\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.2396 - accuracy: 0.9325 - masked_accuracy_fn: 0.2149 - val_loss: 0.2424 - val_accuracy: 0.9342 - val_masked_accuracy_fn: 0.2423\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.2362 - accuracy: 0.9356 - masked_accuracy_fn: 0.2645 - val_loss: 0.2331 - val_accuracy: 0.9366 - val_masked_accuracy_fn: 0.2970\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.2257 - accuracy: 0.9381 - masked_accuracy_fn: 0.2872 - val_loss: 0.2067 - val_accuracy: 0.9448 - val_masked_accuracy_fn: 0.3422\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.2163 - accuracy: 0.9407 - masked_accuracy_fn: 0.3138 - val_loss: 0.2200 - val_accuracy: 0.9388 - val_masked_accuracy_fn: 0.3357\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.2090 - accuracy: 0.9421 - masked_accuracy_fn: 0.3366 - val_loss: 0.1986 - val_accuracy: 0.9452 - val_masked_accuracy_fn: 0.3900\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.2017 - accuracy: 0.9446 - masked_accuracy_fn: 0.3738 - val_loss: 0.1910 - val_accuracy: 0.9487 - val_masked_accuracy_fn: 0.3902\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.1913 - accuracy: 0.9488 - masked_accuracy_fn: 0.4199 - val_loss: 0.1846 - val_accuracy: 0.9515 - val_masked_accuracy_fn: 0.4765\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.1827 - accuracy: 0.9517 - masked_accuracy_fn: 0.4542 - val_loss: 0.1687 - val_accuracy: 0.9560 - val_masked_accuracy_fn: 0.4831\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.1745 - accuracy: 0.9545 - masked_accuracy_fn: 0.4873 - val_loss: 0.1735 - val_accuracy: 0.9548 - val_masked_accuracy_fn: 0.5453\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.1652 - accuracy: 0.9571 - masked_accuracy_fn: 0.5128 - val_loss: 0.1555 - val_accuracy: 0.9594 - val_masked_accuracy_fn: 0.5652\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.1605 - accuracy: 0.9582 - masked_accuracy_fn: 0.5352 - val_loss: 0.1430 - val_accuracy: 0.9624 - val_masked_accuracy_fn: 0.5811\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.1515 - accuracy: 0.9604 - masked_accuracy_fn: 0.5527 - val_loss: 0.1429 - val_accuracy: 0.9631 - val_masked_accuracy_fn: 0.6184\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.1444 - accuracy: 0.9620 - masked_accuracy_fn: 0.5643 - val_loss: 0.1453 - val_accuracy: 0.9617 - val_masked_accuracy_fn: 0.5884\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.1437 - accuracy: 0.9618 - masked_accuracy_fn: 0.5713 - val_loss: 0.1295 - val_accuracy: 0.9661 - val_masked_accuracy_fn: 0.6513\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.1337 - accuracy: 0.9642 - masked_accuracy_fn: 0.5907 - val_loss: 0.1399 - val_accuracy: 0.9612 - val_masked_accuracy_fn: 0.5696\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.1292 - accuracy: 0.9649 - masked_accuracy_fn: 0.5985 - val_loss: 0.1233 - val_accuracy: 0.9657 - val_masked_accuracy_fn: 0.5760\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.1269 - accuracy: 0.9647 - masked_accuracy_fn: 0.6007 - val_loss: 0.1263 - val_accuracy: 0.9636 - val_masked_accuracy_fn: 0.5738\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.1227 - accuracy: 0.9654 - masked_accuracy_fn: 0.6080 - val_loss: 0.1108 - val_accuracy: 0.9677 - val_masked_accuracy_fn: 0.6717\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.1177 - accuracy: 0.9664 - masked_accuracy_fn: 0.6190 - val_loss: 0.1116 - val_accuracy: 0.9670 - val_masked_accuracy_fn: 0.6359\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.1176 - accuracy: 0.9664 - masked_accuracy_fn: 0.6218 - val_loss: 0.1227 - val_accuracy: 0.9641 - val_masked_accuracy_fn: 0.6114\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.1090 - accuracy: 0.9687 - masked_accuracy_fn: 0.6385 - val_loss: 0.1032 - val_accuracy: 0.9704 - val_masked_accuracy_fn: 0.7007\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.1059 - accuracy: 0.9692 - masked_accuracy_fn: 0.6492 - val_loss: 0.1002 - val_accuracy: 0.9703 - val_masked_accuracy_fn: 0.6946\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.1063 - accuracy: 0.9691 - masked_accuracy_fn: 0.6482 - val_loss: 0.0954 - val_accuracy: 0.9714 - val_masked_accuracy_fn: 0.6353\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.1043 - accuracy: 0.9694 - masked_accuracy_fn: 0.6580 - val_loss: 0.1041 - val_accuracy: 0.9688 - val_masked_accuracy_fn: 0.6650\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.0979 - accuracy: 0.9713 - masked_accuracy_fn: 0.6689 - val_loss: 0.0915 - val_accuracy: 0.9735 - val_masked_accuracy_fn: 0.7119\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.0969 - accuracy: 0.9715 - masked_accuracy_fn: 0.6780 - val_loss: 0.0962 - val_accuracy: 0.9712 - val_masked_accuracy_fn: 0.6665\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.0960 - accuracy: 0.9718 - masked_accuracy_fn: 0.6844 - val_loss: 0.0855 - val_accuracy: 0.9745 - val_masked_accuracy_fn: 0.7233\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.0936 - accuracy: 0.9724 - masked_accuracy_fn: 0.6878 - val_loss: 0.0870 - val_accuracy: 0.9738 - val_masked_accuracy_fn: 0.6876\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.0918 - accuracy: 0.9729 - masked_accuracy_fn: 0.6963 - val_loss: 0.0911 - val_accuracy: 0.9723 - val_masked_accuracy_fn: 0.6603\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.0885 - accuracy: 0.9739 - masked_accuracy_fn: 0.7057 - val_loss: 0.0850 - val_accuracy: 0.9744 - val_masked_accuracy_fn: 0.7098\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.0860 - accuracy: 0.9745 - masked_accuracy_fn: 0.7089 - val_loss: 0.0838 - val_accuracy: 0.9756 - val_masked_accuracy_fn: 0.7566\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.0849 - accuracy: 0.9748 - masked_accuracy_fn: 0.7157 - val_loss: 0.0798 - val_accuracy: 0.9761 - val_masked_accuracy_fn: 0.7480\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 1s 55ms/step - loss: 0.0829 - accuracy: 0.9754 - masked_accuracy_fn: 0.7185 - val_loss: 0.0738 - val_accuracy: 0.9783 - val_masked_accuracy_fn: 0.7264\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.0826 - accuracy: 0.9756 - masked_accuracy_fn: 0.7213 - val_loss: 0.0768 - val_accuracy: 0.9771 - val_masked_accuracy_fn: 0.7438\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.0811 - accuracy: 0.9757 - masked_accuracy_fn: 0.7273 - val_loss: 0.0722 - val_accuracy: 0.9787 - val_masked_accuracy_fn: 0.7757\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.0771 - accuracy: 0.9771 - masked_accuracy_fn: 0.7412 - val_loss: 0.0765 - val_accuracy: 0.9767 - val_masked_accuracy_fn: 0.7861\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.0791 - accuracy: 0.9762 - masked_accuracy_fn: 0.7344 - val_loss: 0.0677 - val_accuracy: 0.9800 - val_masked_accuracy_fn: 0.7962\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.0756 - accuracy: 0.9773 - masked_accuracy_fn: 0.7428 - val_loss: 0.0739 - val_accuracy: 0.9777 - val_masked_accuracy_fn: 0.7485\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.0750 - accuracy: 0.9774 - masked_accuracy_fn: 0.7441 - val_loss: 0.0757 - val_accuracy: 0.9776 - val_masked_accuracy_fn: 0.7655\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.0709 - accuracy: 0.9788 - masked_accuracy_fn: 0.7589 - val_loss: 0.0662 - val_accuracy: 0.9802 - val_masked_accuracy_fn: 0.7309\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.0707 - accuracy: 0.9786 - masked_accuracy_fn: 0.7574 - val_loss: 0.0621 - val_accuracy: 0.9813 - val_masked_accuracy_fn: 0.7150\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.0702 - accuracy: 0.9787 - masked_accuracy_fn: 0.7604 - val_loss: 0.0593 - val_accuracy: 0.9821 - val_masked_accuracy_fn: 0.8356\n",
      "CPU times: user 46.2 s, sys: 1.26 s, total: 47.5 s\n",
      "Wall time: 54.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train\n",
    "num_epochs = NUM_EPOCHS\n",
    "\n",
    "best_model_file = os.path.join(data_dir, \"best_model.h5\")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    best_model_file, \n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "history = model.fit(train_dataset, \n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "083d8bc4-845c-4eda-bd85-9d9aec9ccfbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate with test set\n",
    "best_model = POSTaggingModel(source_vocab_size, target_vocab_size,\n",
    "    embedding_dim, max_seqlen, rnn_output_dim)\n",
    "best_model.build(input_shape=(batch_size, max_seqlen))\n",
    "best_model.load_weights(best_model_file)\n",
    "best_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\", masked_accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1e0d7818-27c6-43db-9052-cdc9bf821753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 21ms/step - loss: 0.0636 - accuracy: 0.9807 - masked_accuracy_fn: 0.7834\n",
      "test loss: 0.064, test accuracy: 0.981, masked test accuracy: 0.783\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_masked_acc = best_model.evaluate(test_dataset)\n",
    "print(\"test loss: {:.3f}, test accuracy: {:.3f}, masked test accuracy: {:.3f}\".format(\n",
    "    test_loss, test_acc, test_masked_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1a028e54-3add-4612-a1d9-f06a42ec26ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict on batches\n",
    "labels, predictions = [], []\n",
    "is_first_batch = True\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0a1b9eae-aeda-44ee-9ab4-1f8bd7cbb10d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 8ms/step\n",
      "labeled  : signs/NNS of/IN a/DT slowing/VBG economy/NN are/VBP increasing/VBG pressure/NN ich/NONE 2/IN on/DT the/NNP federal/NNP reserve/NONE to/TO cut/VB short/JJ term/NN interest/NNS rates/CC but/PRP it/NONE exp/VBZ 1/RB is/JJ n't/IN clear/DT whether/JJ the/NN central/MD bank/VB will/RB\n",
      "predicted: signs/NNS of/IN a/DT slowing/VBG economy/NN are/VBP increasing/VBG pressure/NN ich/NONE 2/IN on/DT the/NNP federal/NNP reserve/TO to/VB cut/JJ short/JJ term/NN interest/NNS rates/CC but/PRP it/NONE exp/VBZ 1/VBZ is/RB n't/JJ clear/IN whether/DT the/NNP central/MD bank/MD will/VB\n",
      " \n",
      "labeled  : the/DT chicago/NNP report/NN raised/VBD the/DT possibility/NN that/IN the/DT october/NNP survey/NN of/IN the/DT national/NNP association/NNP of/IN purchasing/NNP management/NNP would/MD also/RB show/VB a/DT reading/NN above/IN 50/CD PAD/NN\n",
      "predicted: the/DT chicago/NNP report/NNP raised/VBD the/DT possibility/NN that/IN the/DT october/NNP survey/NN of/IN the/DT national/NNP association/NNP of/IN purchasing/NNP management/NN would/MD also/RB show/VB a/DT reading/NN above/NN 50/CD\n",
      " \n",
      "labeled  : in/IN addition/NN of/IN course/NN some/DT of/IN the/DT japanese/JJ investments/NNS involved/VBD outright/JJ purchase/NN of/IN small/JJ u/NNP s/NNS\n",
      "predicted: in/IN addition/NN of/IN course/NN some/DT of/IN the/DT japanese/JJ investments/NNS involved/VBN outright/JJ purchase/NN of/IN small/JJ u/NNP\n",
      " \n",
      "labeled  : the/DT sale/NN of/IN shares/NNS to/TO the/DT mcalpine/NNP family/NN along/IN with/IN the/DT recent/JJ sale/NN of/IN 750/CD 000/NNS shares/IN of/NNP meridian/NN stock/TO to/NNP UNK/NNP UNK/NNP holding/NNP plc/IN of/NNP UNK/NNP england/CC and/DT a/JJ recent/JJ public/NN offering/VBP have/VBN increased/NNP meridian/POS 's/JJ net/NN worth/TO to/CD 8/CD 5/NONE million/VBD u/NONE said/NONE 0/NNP t/NNP 1/NN william/NN UNK/NN chief/IN executive/NNP officer/JJ of/NNP\n",
      "predicted: the/DT sale/NN of/IN shares/NNS to/TO the/DT mcalpine/NNP family/NN along/IN with/IN the/DT recent/JJ sale/NN of/IN 750/CD 000/NNS shares/IN of/NNP meridian/NN stock/TO to/NNP UNK/NNP UNK/NNP holding/NNP plc/NNP of/NNP UNK/NNP england/CC and/DT a/JJ recent/JJ public/NN offering/VBP have/VBN increased/NNP meridian/POS 's/JJ net/NN worth/TO to/CD 8/CD 5/CD million/NONE u/NONE said/NONE 0/NONE t/NONE 1/NNP william/NNP UNK/NN chief/NN executive/NN officer/NNP of/NNP\n",
      " \n",
      "labeled  : the/DT financial/NNS services/NN company/MD will/VB pay/CD 0/NN 82/IN share/DT for/NNP each/NN\n",
      "predicted: the/DT financial/JJ services/NNS company/NN will/MD pay/VB 0/NONE 82/IN share/IN for/NN each/NN\n",
      " \n",
      "labeled  : for/IN stock/NN UNK/NNS the/DT underlying/VBG investment/NN may/MD be/VB a/DT stock/NN index/NNS futures/NN contract/CC or/DT the/NN cash/NN value/IN of/DT a/NN stock/NN\n",
      "predicted: for/IN stock/NN UNK/NN the/DT underlying/VBG investment/NN may/MD be/VB a/DT stock/NN index/NN futures/NNS contract/CC or/DT the/NN cash/NN value/IN of/DT a/NN\n",
      " \n",
      "labeled  : sales/NNS however/RB were/VBD little/RB changed/VBN 1/NONE at/IN 2/CD 46/CD billion/NNS guilders/VBN compared/IN with/CD 2/CD 42/NNS\n",
      "predicted: sales/NNS however/RB were/VBD little/RB changed/VBN 1/NONE at/IN 2/CD 46/CD billion/CD guilders/NNS compared/IN with/CD 2/CD 42/CD\n",
      " \n",
      "labeled  : the/DT senate/NNP plans/VBZ 1/NONE to/TO take/VB up/RP the/DT measure/NN quickly/RB and/CC is/VBZ expected/VBN 1/NONE to/TO pass/VB it/PRP\n",
      "predicted: the/DT senate/NNP plans/VBZ 1/NONE to/TO take/VB up/RP the/DT measure/NN quickly/RB and/CC is/VBZ expected/VBN 1/NONE to/TO pass/VB it/PRP\n",
      " \n",
      "labeled  : he/PRP was/VBD previously/RB president/NN of/IN the/DT company/NN 's/POS eastern/NNP edison/NNP co/NNP unit/NN\n",
      "predicted: he/PRP was/VBD previously/RB president/NN of/IN the/DT company/NN 's/POS eastern/NNP edison/NNP co/NNP unit/NN\n",
      " \n",
      "labeled  : under/IN the/DT state/NN 's/POS education/NNP improvement/NNP act/NNP low/JJ test/NN scores/NNS can/MD block/VB students/NNS '/POS promotions/NNS or/CC force/VB entire/JJ districts/NNS into/IN wrenching/JJ state/JJ supervised/NNS interventions/'' ''/WDT that/NONE t/MD 86/VB can/NNS\n",
      "predicted: under/IN the/DT state/NN 's/POS education/NN improvement/NNP act/NNP low/JJ test/NN scores/NNS can/MD block/VB students/NNS '/POS promotions/NNS or/CC force/VB entire/JJ districts/NNS into/IN wrenching/JJ state/JJ supervised/NNS interventions/'' ''/WDT that/WDT t/MD 86/MD\n",
      " \n",
      "labeled  : mr/NNP thomas/NNP currently/RB chairman/NN of/IN the/DT equal/NNP employment/NNP opportunity/NNP commission/NNP would/MD add/VB another/DT conservative/JJ voice/NN to/TO the/DT closely/RB divided/VBN court/NN\n",
      "predicted: mr/NNP thomas/NNP currently/RB chairman/NN of/IN the/DT equal/JJ employment/NNP opportunity/NNP commission/NNP would/MD add/VB another/DT conservative/JJ voice/NN to/TO the/DT closely/RB divided/VBN court/NN\n",
      " \n",
      "labeled  : pick/NONE a/VB country/DT any/NN country/DT PAD/NN\n",
      "predicted: pick/NONE a/DT country/DT any/DT country/NN\n",
      " \n",
      "labeled  : revenue/NN rose/VBD 42/CD to/NN 133/TO 7/CD million/CD u/NONE from/IN 94/CD million/CD u/NONE\n",
      "predicted: revenue/NN rose/VBD 42/CD to/TO 133/TO 7/CD million/CD u/CD from/CD 94/CD million/CD\n",
      " \n",
      "labeled  : south/NNP korea/NNP registered/VBD a/DT trade/NN deficit/NN of/IN 101/CD million/CD u/NONE in/IN october/NNP reflecting/NONE the/VBG country/DT 's/NN economic/POS sluggishness/JJ according/NN to/VBG government/TO figures/NN released/NNS wednesday/VBD PAD/NONE PAD/NNP\n",
      "predicted: south/NNP korea/NNP registered/VBD a/DT trade/NN deficit/NN of/IN 101/CD million/CD u/NONE in/NONE october/NONE reflecting/NONE the/DT country/NN 's/POS economic/JJ sluggishness/JJ according/NN to/TO government/NN figures/NNS released/NNS wednesday/NNP\n",
      " \n",
      "labeled  : a/DT bank/NN spokeswoman/NN also/RB declined/VBD 1/NONE to/TO comment/VB on/IN any/DT merger/JJ related/NNS matters/CC but/VBD said/NONE 0/DT the/NN company/VBD decided/NONE 2/TO to/VB drop/PRP its/NN opposition/TO to/DT the/JJ interstate/NN banking/NN legislation/IN because/VBG prevailing/NN sentiment/VBZ is/IN in/NN favor/IN of/NN passage/''\n",
      "predicted: a/DT bank/NN spokeswoman/NN also/RB declined/VBD 1/NONE to/TO comment/VB on/IN any/DT merger/JJ related/NNS matters/CC but/VBD said/VBD 0/DT the/NN company/VBD decided/NONE 2/TO to/VB drop/PRP its/NN opposition/TO to/DT the/JJ interstate/NN banking/NN legislation/NN because/NN prevailing/NN sentiment/VBZ is/IN in/NN favor/NN of/NN passage/''\n",
      " \n",
      "labeled  : sales/NNS by/IN these/DT subsidiaries/NNS in/IN the/DT fiscal/JJ year/NN ending/VBG last/JJ march/NNP were/VBD more/JJR than/IN 17/CD billion/CD u/NONE\n",
      "predicted: sales/NNS by/IN these/DT subsidiaries/NNS in/IN the/DT fiscal/JJ year/NN ending/VBG last/JJ march/NN were/VBD more/JJR than/IN 17/CD billion/CD u/NONE\n",
      " \n",
      "labeled  : here/RB are/VBP t/NONE 1/NN price/NNS trends/IN on/DT the/NN world/POS 's/JJ major/NN stock/NNS markets/IN as/NONE UNK/VBN 2/NONE by/IN morgan/NNP stanley/NNP capital/NNP international/NNP perspective/NNP UNK/NNP\n",
      "predicted: here/RB are/VBP t/NONE 1/IN price/NNS trends/IN on/DT the/NN world/POS 's/JJ major/JJ stock/NNS markets/IN as/IN UNK/IN 2/IN by/NNP morgan/NNP stanley/NNP capital/NNP international/NNP perspective/NNP\n",
      " \n",
      "labeled  : the/DT s/NNP p/NN index/VBD started/NONE 1/VBG sliding/IN in/NN price/IN in/NNP september/CD 1976/CC and/VBD fell/CD 12/NN in/IN 1977/CD despite/IN a/DT 15/CD expansion/NN in/NN dividends/IN that/NNS year/DT PAD/NN\n",
      "predicted: the/DT s/NN p/NN index/VBD started/NONE 1/IN sliding/IN in/NN price/NN in/NNP september/CC 1976/CC and/CD fell/CD 12/CD in/IN 1977/CD despite/IN a/NN 15/NN expansion/NN in/IN dividends/NNS that/IN\n",
      " \n",
      "labeled  : the/DT mcalpine/NNP family/NN which/WDT t/NONE 1/VBZ operates/DT a/NN number/IN of/JJ UNK/NNS companies/VBG including/DT a/JJ london/NN based/CC engineering/NN and/NN construction/RB company/VBD also/TO lent/NNP to/NNP meridian/CD national/NONE\n",
      "predicted: the/DT mcalpine/NNP family/NN which/WDT t/NONE 1/VBZ operates/DT a/NN number/IN of/NNS UNK/NNS companies/DT including/DT a/NN london/NN based/NN engineering/NN and/NN construction/NN company/RB also/NNP lent/NNP to/NNP meridian/NNP\n",
      " \n",
      "labeled  : it/PRP exp/NONE 1/VBZ 's/DT a/NN shame/NONE 0/PRP their/NN meeting/RB never/VBD took/NN\n",
      "predicted: it/PRP exp/NONE 1/VBZ 's/DT a/NN shame/NONE 0/PRP their/NN meeting/RB never/VBD took/NN\n",
      " \n",
      "labeled  : it/PRP exp/NONE 1/VBZ 's/RB also/VBG refreshing/NONE to/TO read/VB a/DT japanese/JJ author/NN who/WP t/NONE 52/RB clearly/VBZ does/RB n't/VB belong/TO to/DT the/JJ self/JJ aggrandizing/'' we/NN japanese/IN ''/NNS school/WP of/NONE writers/VBP who/DT t/NN 53/IN perpetuate/DT the/JJ notion/NN of/JJ the/IN unique/NNS\n",
      "predicted: it/PRP exp/NONE 1/VBZ 's/RB also/RB refreshing/NONE to/TO read/VB a/DT japanese/JJ author/NN who/WP t/NONE 52/RB clearly/VBZ does/RB n't/VB belong/TO to/DT the/JJ self/JJ aggrandizing/'' we/JJ japanese/JJ ''/NN school/IN of/NNS writers/WP who/NONE t/IN 53/IN perpetuate/DT the/JJ notion/NN of/DT the/NNS\n",
      " \n",
      "labeled  : many/JJ are/VBP far/RB enough/RB from/IN residential/JJ areas/NNS to/NONE pass/TO public/VB muster/JJ yet/NN close/CC enough/RB to/RB permit/NONE family/TO visits/VB PAD/NN PAD/NNS\n",
      "predicted: many/JJ are/VBP far/RB enough/RB from/IN residential/JJ areas/NNS to/NONE pass/TO public/VB muster/JJ yet/NN close/CC enough/RB to/NONE permit/TO family/VB visits/VB PAD/NN\n",
      " \n",
      "labeled  : for/IN example/NN campbell/NNP is/VBZ a/DT distant/JJ third/JJ in/IN the/DT u/NNP k/VBN frozen/NNS foods/NN market/WRB where/PRP it/RB recently/VBD paid/CD 24/NNS times/NNS earnings/IN for/NNP freshbake/NNPS foods/NNP plc/CC and/VBD wound/RP up/IN with/RB far/JJR more/NN capacity/IN than/PRP it/MD could/VB use/NONE t/NONE\n",
      "predicted: for/IN example/NN campbell/NNP is/VBZ a/DT distant/JJ third/JJ in/IN the/DT u/NNP k/CD frozen/VBN foods/NN market/NN where/WRB it/VBD recently/VBN paid/CD 24/NNS times/NNS earnings/IN for/NNP freshbake/NNP foods/NNP plc/CC and/NN wound/IN up/IN with/RB far/JJR more/IN capacity/IN than/PRP it/MD could/VB use/NONE\n",
      " \n",
      "labeled  : wedtech/NNP did/VBD n't/RB just/RB use/VB old/JJ fashioned/VBN UNK/NN\n",
      "predicted: wedtech/NNP did/VBD n't/RB just/RB use/VB old/JJ fashioned/VBN\n",
      " \n",
      "labeled  : most/RBS important/JJ ms/NNP ganes/NNP noted/VBD 0/NONE t/NONE 2/JJ brazilian/NNS officials/VBD said/IN that/DT no/NN decision/VBZ has/RB as/RB yet/VBN been/VBN made/NONE 1/IN on/DT the/NN suspension/IN of/NNS exports/''\n",
      "predicted: most/JJS important/JJ ms/NNP ganes/NNP noted/VBD 0/NONE t/NONE 2/JJ brazilian/NNS officials/NNS said/IN that/DT no/NN decision/VBZ has/RB as/RB yet/VBN been/VBN made/NONE 1/IN on/DT the/NN suspension/IN of/NNS\n",
      " \n",
      "labeled  : when/WRB scoring/NNP high/NNP first/RB came/VBD out/RB in/IN 1979/CD t/NONE 1/PRP it/VBD was/DT a/NN publication/IN of/NNP random/NNP\n",
      "predicted: when/WRB scoring/NNP high/NNP first/RB came/VBD out/IN in/IN 1979/CD t/NONE 1/PRP it/VBD was/NN a/NN publication/IN of/NNP random/NNP\n",
      " \n",
      "labeled  : furukawa/NNP said/VBD 0/NONE the/DT purchase/NN of/IN the/DT french/JJ and/CC german/JJ plants/NNS together/RB will/MD total/VB about/RB 40/CD billion/CD yen/NNS lrb/LRB 280/CD million/CD u/NONE rrb/RRB\n",
      "predicted: furukawa/NNP said/VBD 0/NONE the/DT purchase/NN of/IN the/DT french/JJ and/CC german/JJ plants/NNS together/RB will/MD total/VB about/IN 40/CD billion/CD yen/NNS lrb/LRB 280/CD million/CD u/NONE\n",
      " \n",
      "labeled  : at/IN st/NNP mary/NNP 's/POS church/NNP in/IN ilminster/NNP somerset/NNP the/DT bells/NNS have/VBP fallen/VBN 1/NONE silent/JJ following/VBG a/DT dust/NN up/IN over/NN church/NN\n",
      "predicted: at/IN st/NNP mary/NNP 's/POS church/NN in/IN ilminster/NNP somerset/NNP the/DT bells/NNS have/VBP fallen/VBN 1/NONE silent/JJ following/VBG a/DT dust/NN up/IN over/IN church/NN\n",
      " \n",
      "labeled  : it/PRP said/VBD that/IN the/DT temptation/NN ich/NONE 2/IN for/NNS managements/NONE to/TO ease/VB this/DT profit/NN pressure/NN by/IN 1/NONE taking/VBG greater/JJR risks/NNS is/VBZ an/DT additional/JJ rating/NN factor/NN ''/''\n",
      "predicted: it/PRP said/VBD that/IN the/DT temptation/NN ich/NONE 2/IN for/NNS managements/NONE to/TO ease/VB this/DT profit/NN pressure/NN by/IN 1/NONE taking/VBG greater/JJR risks/NNS is/VBZ an/DT additional/JJ rating/NN factor/NN ''/''\n",
      " \n",
      "labeled  : last/JJ week/NN robert/NNP m/NNP bradley/NNP one/CD of/IN the/DT big/NNP board/NNP 's/POS most/RBS respected/VBN floor/NN traders/NNS and/CC head/NN of/IN a/DT major/JJ traders/NNS '/POS organization/NN surrendered/VBD\n",
      "predicted: last/JJ week/NN robert/NNP m/NNP bradley/NNP one/CD of/IN the/DT big/NNP board/NNP 's/POS most/JJS respected/VBN floor/NN traders/NNS and/CC head/NN of/IN a/DT major/JJ traders/NNS '/POS organization/NN surrendered/VBD\n",
      " \n",
      "labeled  : history/NN after/IN all/DT is/VBZ not/RB on/IN his/PRP side/NN\n",
      "predicted: history/NN after/IN all/DT is/VBZ not/RB on/IN his/PRP side/NN\n",
      " \n",
      "labeled  : san/NNP francisco/NNP voters/NNS rejected/VBD a/DT new/JJ ballpark/NN two/CD years/NNS ago/IN\n",
      "predicted: san/NNP francisco/NNP voters/NNS rejected/VBD a/DT new/JJ ballpark/NN two/CD years/NNS ago/IN\n",
      " \n",
      "labeled  : UNK/NNP water/NNP co/NNP offering/VBG of/IN 150/CD 000/NNS shares/IN of/JJ common/NN stock/IN via/NNP legg/NNP mason/NNP wood/NNP UNK/NNP inc/CC and/NNP UNK/NNP UNK/NNP UNK/NNP UNK/NNP\n",
      "predicted: UNK/NNP water/NN co/CC offering/NN of/IN 150/CD 000/NNS shares/NNS of/JJ common/NN stock/IN via/NNP legg/NNP mason/NNP wood/NNP UNK/NNP inc/NNP and/NNP UNK/NNP UNK/NNP UNK/NNP UNK/NNP\n",
      " \n",
      "labeled  : nec/NNP one/CD of/IN its/PRP largest/JJS domestic/JJ competitors/NNS said/VBD 0/NONE it/PRP bid/VBD one/CD yen/NN in/IN two/CD separate/JJ public/JJ auctions/NNS since/IN 1987/CD\n",
      "predicted: nec/NNP one/CD of/IN its/PRP largest/JJS domestic/JJ competitors/NNS said/VBD 0/NONE it/PRP bid/NN one/CD yen/NN in/IN two/CD separate/JJ public/JJ auctions/NNS since/IN 1987/CD\n",
      " \n",
      "labeled  : an/DT egyptian/JJ pharaoh/NNP could/MD n't/RB have/VB justified/VBN his/PRP pyramids/NNS any/RB better/RBR\n",
      "predicted: an/DT egyptian/JJ pharaoh/NNP could/MD n't/RB have/VB justified/VBN his/PRP pyramids/NNS any/VBG better/JJR\n",
      " \n",
      "labeled  : it/PRP 's/VBZ made/VBN 2/NONE only/RB in/IN years/NNS when/WRB the/DT grapes/NNS ripen/VBP perfectly/RB t/NONE 1/LRB lrb/DT the/NN last/VBD was/CD 1979/RRB rrb/CC and/VBZ comes/IN from/DT a/JJ single/NN acre/IN of/NNS grapes/WDT that/NONE t/VBD 166/DT yielded/JJ a/CD mere/NNS 75/IN cases/CD\n",
      "predicted: it/PRP 's/VBZ made/VBN 2/NONE only/RB in/IN years/NNS when/WRB the/DT grapes/NNS ripen/VBP perfectly/RB t/NONE 1/RB lrb/DT the/NN last/NN was/CD 1979/CD rrb/CC and/NN comes/IN from/DT a/JJ single/NN acre/IN of/NNS grapes/WDT that/NONE t/NONE 166/DT yielded/JJ a/CD\n",
      " \n",
      "labeled  : b/LS current/JJ annual/JJ yield/NN\n",
      "predicted: b/JJ current/JJ annual/JJ yield/NN\n",
      " \n",
      "labeled  : the/DT department/NN previously/RB estimated/VBD that/IN durable/NNS goods/NNS orders/VBD fell/CD 0/NN 1/IN in/NNP\n",
      "predicted: the/DT department/NN previously/RB estimated/VBN that/IN durable/NNS goods/NNS orders/VBD fell/CD 0/IN 1/IN\n",
      " \n",
      "labeled  : in/IN addition/NN a/DT big/JJ loan/NN that/IN first/NNP boston/NNP made/VBD t/NONE 2/TO to/NNP ohio/NNP mattress/NNP co/VBD was/RB n't/VBN repaid/IN on/NN time/WRB when/PRP its/CD 450/CD million/NONE u/NN junk/NN financing/IN for/DT a/NN buy/IN out/DT of/NN the/NN bedding/VBD company/VBN was/NONE withdrawn/NONE\n",
      "predicted: in/IN addition/NN a/DT big/JJ loan/NN that/IN first/NNP boston/NNP made/VBD t/NONE 2/TO to/NNP ohio/NNP mattress/NNP co/VBD was/RB n't/VBN repaid/IN on/NN time/PRP when/PRP its/CD 450/CD million/NONE u/NN junk/NN financing/IN for/DT a/NN buy/IN out/IN of/DT the/NN bedding/VBD company/VBD was/VBN withdrawn/NONE\n",
      " \n",
      "labeled  : a/DT successor/NN was/VBD n't/RB named/VBN 1/NONE which/WDT t/NONE 35/VBD fueled/NN speculation/IN that/NNP mr/NNP bernstein/MD may/VB have/VBN clashed/IN with/NNP s/NNP i/NNP newhouse/WP jr/NN whose/NN family/NNP company/NNPS advance/NNP publications/NONE inc/VBZ t/NNP 2/NNP\n",
      "predicted: a/DT successor/NN was/VBD n't/RB named/VBN 1/NONE which/WDT t/NONE 35/NONE fueled/NN speculation/IN that/NNP mr/NNP bernstein/MD may/VB have/VBN clashed/IN with/NNP s/NNP i/NNP newhouse/NNP jr/NNP whose/NN family/NN company/NNP advance/NNP publications/NNP inc/NNP\n",
      " \n",
      "labeled  : at/IN the/DT prices/NNS 0/NONE we/PRP were/VBD charged/VBN 78/NONE t/NONE 1/EX there/MD should/VB have/VBN been/DT some/NN return/IN for/DT the/NN\n",
      "predicted: at/IN the/DT prices/NNS 0/VBD we/PRP were/VBD charged/VBN 78/NONE t/NONE 1/MD there/MD should/VB have/DT been/DT some/NN return/IN for/DT the/NN\n",
      " \n",
      "labeled  : what/WP t/NONE 14/VBZ matters/VBZ is/WP what/NNS advertisers/VBP are/VBG paying/NONE t/IN 15/NN per/CC page/IN and/DT in/NN that/PRP department/VBP we/VBG are/RB doing/DT fine/NN this/'' fall/VBD ''/NONE said/NNP t/NNP\n",
      "predicted: what/WP t/NONE 14/VBZ matters/VBZ is/NNS what/WP advertisers/VBP are/NONE paying/NONE t/TO 15/NN per/NN page/CC and/IN in/IN that/NN department/PRP we/VBP are/VBP doing/NN fine/NN this/NN fall/NN ''/NONE said/NONE\n",
      " \n",
      "labeled  : the/DT firmness/NN in/IN heating/NN oil/NN was/VBD attributed/VBN 1/NONE to/TO UNK/JJR weather/NN in/IN parts/NNS of/IN the/DT u/NNP s/CC and/TO to/DT the/JJS latest/JJ weekly/NN report/IN by/DT the/NNP american/NNP petroleum/NNP institute/WDT which/NONE t/VBD 2/DT showed/NN a/IN decline/NNS in/IN inventories/DT of/NN\n",
      "predicted: the/DT firmness/NN in/IN heating/NN oil/NN was/VBD attributed/VBN 1/NONE to/TO UNK/VB weather/NN in/IN parts/NNS of/IN the/DT u/NNP s/NN and/NN to/DT the/DT latest/JJ weekly/NN report/IN by/DT the/NNP american/NNP petroleum/NNP institute/NNP which/WDT t/DT 2/DT showed/DT a/NN decline/IN in/NNS inventories/DT of/DT\n",
      " \n",
      "labeled  : the/DT consumer/NN confidence/NN survey/NN 2/NONE covering/VBG 5/CD 000/NNP u/NNS s/VBZ households/VBN is/NONE UNK/IN 2/DT in/JJ the/CD first/NNS two/IN weeks/DT of/NN each/IN month/DT for/NNP the/NNP conference/IN board/NNP by/NNP national/NNP family/NNP opinion/DT inc/NNP a/NNP toledo/NN ohio/NN\n",
      "predicted: the/DT consumer/NN confidence/NN survey/NN 2/NONE covering/VBG 5/CD 000/NONE u/NNP s/NNS households/VBZ is/JJ UNK/IN 2/IN in/DT the/JJ first/CD two/NNS weeks/IN of/DT each/NN month/IN for/DT the/NN conference/NNP board/NNP by/NNP national/NNP family/NNP opinion/NNP inc/NNP a/NNP toledo/NNP\n",
      " \n",
      "labeled  : elsewhere/RB share/NN prices/NNS closed/VBD higher/JJR in/IN singapore/NNP taipei/NNP and/CC UNK/NNP were/VBD mixed/VBN in/IN hong/NNP kong/NNP lower/JJR in/IN seoul/NNP and/CC little/RB changed/VBN in/IN UNK/NNP\n",
      "predicted: elsewhere/RB share/NN prices/NNS closed/VBD higher/JJR in/IN singapore/NNP taipei/NNP and/CC UNK/NNP were/VBD mixed/VBN in/IN hong/NNP kong/NNP lower/JJR in/IN seoul/NNP and/CC little/RB changed/VBN in/IN\n",
      " \n",
      "labeled  : mr/NNP hahn/NNP attributes/VBZ the/DT gains/NNS to/TO the/DT philosophy/NN of/IN concentrating/NONE on/VBG what/IN a/WP company/DT knows/NN t/VBZ 1/NONE best/JJS\n",
      "predicted: mr/NNP hahn/NNP attributes/VBZ the/DT gains/NNS to/TO the/DT philosophy/NN of/IN concentrating/NONE on/NONE what/VBG a/DT company/NN knows/VBZ t/NONE 1/NNS\n",
      " \n",
      "labeled  : freudtoy/NNP a/DT pillow/NN bearing/VBG the/DT likeness/NN of/IN sigmund/NNP freud/NNP is/VBZ marketed/VBN 1/NONE as/IN a/DT 24/CD 95/NONE u/NN tool/IN for/JJ do/NN\n",
      "predicted: freudtoy/NNP a/DT pillow/NN bearing/VBG the/DT likeness/NN of/IN sigmund/NNP freud/NNP is/VBZ marketed/VBN 1/NONE as/IN a/DT 24/CD 95/NN u/NN tool/IN for/PRP do/PRP\n",
      " \n",
      "labeled  : index/NN traders/NNS who/WP t/NONE 71/VBP buy/DT all/CD 500/NNS stocks/IN in/DT the/NNP s/CD p/RB 500/VBP often/RB do/RB n't/VB even/WP know/DT what/NNS the/NONE companies/PRP 0/VBP they/NONE own/RB t/VBP 1/NONE actually/VBZ do/NONE t/NONE 2/NNP complains/NNP 0/NN t/IN 3/NNP andrew/NNP sigler/NNP\n",
      "predicted: index/NNS traders/NNS who/WP t/NONE 71/VBP buy/DT all/NNS 500/NNS stocks/IN in/DT the/NNP s/CD p/CD 500/RB often/RB do/RB n't/RB even/VBP know/VBP what/DT the/NNS companies/NNS 0/PRP they/PRP own/NONE t/NONE 1/NONE actually/NONE do/NONE t/NONE 2/NN complains/NONE 0/NONE t/NNP 3/NNP\n",
      " \n",
      "labeled  : light/NN trucks/NNS and/CC vans/NNS will/MD face/VB the/DT same/JJ safety/NN requirements/NNS as/IN automobiles/NNS under/IN new/JJ proposals/NNS by/IN the/DT transportation/NNP department/NNP\n",
      "predicted: light/JJ trucks/NNS and/CC vans/NNS will/MD face/VB the/DT same/JJ safety/NN requirements/NNS as/IN automobiles/NNS under/IN new/JJ proposals/NNS by/IN the/DT transportation/NNP department/NNP\n",
      " \n",
      "labeled  : indeed/RB for/IN many/JJ japanese/JJ trading/NN companies/NNS the/DT favorite/JJ u/NNP s/JJ small/NN business/VBZ is/CD one/WP whose/NN research/CC and/NN development/NONE t/MD 1/VB can/VBN be/NONE milked/IN 117/JJ for/JJ future/NN\n",
      "predicted: indeed/RB for/IN many/JJ japanese/JJ trading/NN companies/NNS the/DT favorite/JJ u/NNP s/NN small/NN business/VBZ is/CD one/CD whose/NN research/CC and/NN development/NONE t/MD 1/MD can/VB be/IN milked/IN 117/IN for/JJ future/JJ\n",
      " \n",
      "labeled  : usx/NNP said/VBD 0/NONE it/PRP has/VBZ been/VBN cooperating/VBG with/IN osha/NNP since/IN the/DT agency/NN began/VBD 1/NONE investigating/VBG the/DT clairton/NNP and/CC fairless/NNP works/NNS\n",
      "predicted: usx/NNP said/VBD 0/NONE it/PRP has/VBZ been/VBN cooperating/VBG with/IN osha/NNP since/IN the/DT agency/NN began/VBD 1/NONE investigating/VBG the/DT clairton/NNP and/CC fairless/NNP works/NNS\n",
      " \n",
      "labeled  : the/DT UNK/NN 's/POS UNK/NN enables/VBZ it/PRP to/TO be/VB UNK/VBN 1/NONE in/IN smaller/JJR UNK/NNS than/IN are/NONE now/VBP possible/RB for/JJ cataract/IN surgery/NN the/NN eye/DT care/NN and/NN skin/CC care/NN concern/NN said/NN 0/VBD t/NONE 3/NONE\n",
      "predicted: the/DT UNK/NN 's/POS UNK/NN enables/VBZ it/PRP to/TO be/VB UNK/VB 1/NONE in/IN smaller/JJR UNK/IN than/IN are/RB now/RB possible/JJ for/IN cataract/IN surgery/NN the/DT eye/DT care/NN and/CC skin/CC care/NN concern/NN said/VBD 0/NONE t/NONE\n",
      " \n",
      "labeled  : most/JJS of/IN the/DT picture/NN is/VBZ taken/VBN up/RP with/IN endless/JJ scenes/NNS of/IN many/JJ people/NNS either/CC fighting/VBG or/CC eating/VBG and/CC drinking/VBG 1/NONE to/TO celebrate/VB victory/NN\n",
      "predicted: most/JJS of/IN the/DT picture/NN is/VBZ taken/VBN up/RP with/IN endless/JJ scenes/NNS of/IN many/JJ people/NNS either/CC fighting/VBG or/CC eating/VBG and/CC drinking/VBG 1/NONE to/TO celebrate/VB victory/NN\n",
      " \n",
      "labeled  : but/CC they/PRP have/VBP obtained/VBN 8300/JJ forms/NNS without/IN court/NN permission/NN and/CC used/VBD the/DT information/NN 1/NONE to/TO help/VB develop/VB criminal/JJ cases/NNS\n",
      "predicted: but/CC they/PRP have/VBP obtained/VBN 8300/JJ forms/NNS without/IN court/NN permission/NN and/CC used/VBN the/DT information/NN 1/NONE to/TO help/VB develop/VB criminal/JJ cases/NNS\n",
      " \n",
      "labeled  : this/DT year/NN it/PRP is/VBZ expected/VBN 1/NONE to/TO be/VB a/DT net/JJ UNK/NN and/CC is/VBZ said/VBN 1/NONE to/TO be/VB seeking/VBG 2/NONE to/TO buy/VB about/IN 200/CD 000/NNS tons/IN of/NN sugar/NONE 3/TO to/VB meet/JJ internal/NNS needs/NNS analysts/VBD said/NONE 0/NONE\n",
      "predicted: this/DT year/NN it/PRP is/VBZ expected/VBN 1/NONE to/TO be/VB a/DT net/JJ UNK/NN and/CC is/VBZ said/VBN 1/NONE to/TO be/VB seeking/VBG 2/NONE to/TO buy/VB about/IN 200/CD 000/IN tons/IN of/NN sugar/TO 3/TO to/VB meet/JJ internal/NNS needs/NNS analysts/VBD said/NONE 0/NONE\n",
      " \n",
      "labeled  : 20/CD million/CD swiss/JJ francs/NNS of/IN 6/CD 1/CD 2/NN privately/RB placed/VBN notes/NNS due/JJ nov/NNP 29/CD 1996/CD priced/VBN at/NONE 99/IN 1/CD 2/CD via/IN dai/NNP ichi/NNP kangyo/NNP bank/LRB lrb/NNP schweiz/RRB\n",
      "predicted: 20/CD million/CD swiss/JJ francs/NNS of/IN 6/CD 1/NONE 2/VBG privately/RB placed/VBN notes/NNS due/JJ nov/NNP 29/CD 1996/CD priced/VBN at/IN 99/CD 1/CD 2/CD via/IN dai/NNP ichi/NNP kangyo/NNP bank/NNP lrb/NNP schweiz/NNP\n",
      " \n",
      "labeled  : prices/NNS closed/VBD mostly/RB higher/JJR in/IN relatively/RB light/JJ trading/NN as/IN farmers/NNS continued/VBD 1/NONE to/TO withhold/VB their/PRP crops/NNS from/IN the/DT marketplace/NN in/IN the/DT hope/NN of/IN higher/JJR prices/NNS 0/NONE t/NONE 2/TO to/VB\n",
      "predicted: prices/NNS closed/VBD mostly/RB higher/JJR in/IN relatively/RB light/JJ trading/NN as/IN farmers/NNS continued/VBN 1/NONE to/TO withhold/VB their/PRP crops/NNS from/IN the/DT marketplace/NN in/IN the/DT hope/NN of/IN higher/JJR prices/NNS 0/NONE t/NONE 2/TO to/VB\n",
      " \n",
      "labeled  : ralston/NNP said/VBD 0/NONE its/PRP UNK/NNP battery/NN unit/NN was/VBD hurt/VBN 1/NONE by/IN continuing/VBG economic/JJ problems/NNS in/IN south/NNP america/NNP\n",
      "predicted: ralston/NNP said/VBD 0/NONE its/PRP UNK/NN battery/NN unit/NN was/VBD hurt/VBN 1/NONE by/IN continuing/VBG economic/JJ problems/NNS in/IN south/NNP america/NNP\n",
      " \n",
      "labeled  : the/DT december/NNP contract/NN advanced/VBD 2/CD 50/NNS cents/DT a/NN pound/TO to/CD 1/NONE\n",
      "predicted: the/DT december/NNP contract/NN advanced/VBD 2/CD 50/CD cents/DT a/NN pound/TO to/CD 1/CD\n",
      " \n",
      "labeled  : further/RB he/PRP said/VBD 0/NONE t/NONE 1/DT the/NN company/VBZ does/RB n't/VB have/DT the/NN capital/VBN needed/NONE to/NONE build/TO the/VB business/DT over/NN the/IN next/DT year/JJ or/NN two/CC PAD/CD\n",
      "predicted: further/RB he/PRP said/VBD 0/NONE t/NONE 1/DT the/NN company/VBZ does/RB n't/VB have/DT the/NN capital/NONE needed/NONE to/VB build/VB the/DT business/DT over/NN the/DT next/JJ year/NN or/CC two/CD\n",
      " \n",
      "labeled  : terms/NNS were/VBD n't/RB disclosed/VBN 1/NONE\n",
      "predicted: terms/NNS were/VBD n't/RB disclosed/VBN 1/NONE\n",
      " \n",
      "labeled  : you/PRP may/MD come/VB by/IN the/DT agency/NN 3/NONE to/TO read/VB but/CC not/RB copy/VB either/DT manually/RB or/CC by/IN photocopying/NONE ''/VBG a/'' voice/DT official/NNP explained/NN t/VBD 1/NONE when/WRB i/PRP asked/VBD t/NONE\n",
      "predicted: you/PRP may/MD come/VB by/IN the/DT agency/NN 3/NN to/TO read/VB but/CC not/RB copy/VB either/DT manually/RB or/CC by/IN photocopying/NONE ''/'' a/DT voice/DT official/NN explained/NNP t/VBZ 1/VBD when/WRB i/PRP asked/VBD t/NONE\n",
      " \n",
      "labeled  : although/IN the/DT purchasing/NN managers/NNS '/POS index/NN continues/VBZ 1/NONE to/TO indicate/VB a/DT slowing/VBG economy/NN it/PRP is/VBZ n't/RB signaling/VBG an/DT imminent/JJ recession/NN said/VBD 0/NONE t/NONE 2/NNP robert/NNP bretz/NN chairman/IN of/DT the/NN association/POS 's/NN survey/NN committee/CC and/NN director/IN of/NNS materials/NN management/IN at/NNP pitney/NNP bowes/NNP inc/NNP stamford/NNP\n",
      "predicted: although/IN the/DT purchasing/NN managers/NNS '/POS index/NN continues/VBZ 1/NONE to/TO indicate/VB a/DT slowing/NN economy/NN it/PRP is/VBZ n't/RB signaling/VBG an/DT imminent/JJ recession/NN said/VBD 0/NONE t/NONE 2/NNP robert/NNP bretz/NN chairman/IN of/DT the/NNP association/POS 's/NN survey/NN committee/CC and/NN director/IN of/NNS materials/IN management/IN at/NNP pitney/NNP bowes/NNP inc/NNP\n",
      " \n",
      "labeled  : dialing/NONE 900/VBG brings/CD callers/VBZ a/NNS growing/DT number/VBG of/NN services/IN PAD/NNS\n",
      "predicted: dialing/NONE 900/CD brings/CD callers/VBZ a/NNS growing/VBG number/NN of/IN services/NNS\n",
      " \n",
      "labeled  : under/IN a/DT 1934/CD law/NN the/DT johnson/NNP debt/NNP default/NNP act/NNP as/RB amended/NONE 1/VBN it/NONE exp/PRP 2/NONE 's/VBZ illegal/JJ for/IN americans/NNS to/TO extend/VB credit/NN to/TO countries/NNS in/IN default/NN to/TO the/DT u/NNP s/NN government/IN unless/PRP they/VBP are/NNS members/IN of/DT the/NNP world/NNP bank/CC and/NNP international/NNP monetary/NNP\n",
      "predicted: under/IN a/DT 1934/CD law/NN the/DT johnson/NNP debt/NN default/NNP act/NNP as/NONE amended/NONE 1/NONE it/NONE exp/NONE 2/VBZ 's/JJ illegal/JJ for/NNS americans/NNS to/VB extend/VB credit/TO to/VB countries/IN in/IN default/TO to/TO the/DT u/NNP s/NN government/IN unless/PRP they/VBP are/NNS members/IN of/DT the/NNP world/NNP bank/CC and/NNP international/NNP monetary/NNP fund/NNP\n",
      " \n",
      "labeled  : w/NNP n/NNP whelen/CC co/NNP of/IN georgetown/NNP del/NNP and/CC its/PRP president/NN william/NNP n/NNP whelen/NNP jr/NNP also/RB of/IN georgetown/NNP were/VBD barred/VBN 1/NONE from/IN 2/NONE transacting/VBG principal/JJ trades/NNS for/IN 90/CD days/NNS and/CC were/VBD jointly/RB fined/VBN 1/NONE 15/CD 000/NONE\n",
      "predicted: w/NNP n/NNP whelen/NNP co/NNP of/IN georgetown/NNP del/NNP and/CC its/PRP president/NN william/NNP n/NNP whelen/NNP jr/NNP also/RB of/IN georgetown/NNP were/VBD barred/VBN 1/NONE from/IN 2/NONE transacting/VBG principal/JJ trades/NNS for/IN 90/CD days/NNS and/CC were/VBD jointly/RB fined/VBN 1/NONE 15/CD 000/NONE\n",
      " \n",
      "labeled  : UNK/NNP markets/NNS were/VBD closed/VBN for/IN a/DT holiday/NN\n",
      "predicted: UNK/NNS markets/NNS were/VBD closed/VBD for/IN a/DT holiday/NN\n",
      " \n",
      "labeled  : copperweld/NN said/VBD 0/NONE it/PRP does/VBZ n't/RB expect/VB a/DT protracted/JJ strike/NN\n",
      "predicted: copperweld/NN said/VBD 0/NONE it/PRP does/VBZ n't/RB expect/VB a/DT protracted/JJ strike/NN\n",
      " \n",
      "labeled  : the/DT development/NN traders/NNS said/VBD 0/NONE t/NONE 1/VBD showed/IN that/EX there/VBZ is/JJR more/IN than/JJ ample/NN liquidity/JJ available/IN for/NN investment/IN despite/DT the/NN market/POS 's/JJ recent/JJ UNK/NN\n",
      "predicted: the/DT development/NN traders/NNS said/VBD 0/NONE t/NONE 1/NONE showed/NN that/VBZ there/EX is/IN more/IN than/JJ ample/NN liquidity/NN available/IN for/NN investment/IN despite/DT the/NN market/NN 's/JJ recent/JJ UNK/NN\n",
      " \n",
      "labeled  : last/JJ march/NNP after/IN 1/NONE attending/VBG a/DT teaching/NN seminar/NN in/IN washington/NNP mrs/NNP yeargin/NNP says/VBZ 0/NONE she/PRP returned/VBD to/TO greenville/NNP two/CD days/NNS before/IN annual/JJ testing/NN 2/NONE feeling/VBG that/IN she/PRP had/VBD n't/RB prepared/VBN her/PRP low/JJ ability/NN geography/NNS students/RB\n",
      "predicted: last/JJ march/NNP after/IN 1/NONE attending/VBG a/DT teaching/NN seminar/NN in/IN washington/NNP mrs/NNP yeargin/NNP says/VBZ 0/NONE she/PRP returned/VBD to/TO greenville/NNP two/CD days/NNS before/IN annual/JJ testing/NN 2/NONE feeling/VBG that/IN she/PRP had/VBD n't/RB prepared/VBN her/PRP low/JJ ability/NN geography/NNS\n",
      " \n",
      "labeled  : the/DT radio/NN show/NN enraged/VBD us/PRP ''/'' says/VBZ t/NONE 1/NNP mrs/NNP\n",
      "predicted: the/DT radio/NN show/NN enraged/VBD us/PRP ''/'' says/VBZ t/NONE 1/NNP mrs/NNP\n",
      " \n",
      "labeled  : in/IN a/DT meeting/NN with/IN premier/NNP li/NNP peng/NNP on/IN monday/NNP mr/NNP nixon/NNP said/VBD that/IN he/PRP hoped/VBD 0/NONE he/PRP would/MD n't/RB encounter/VB guards/NNS with/IN machine/NN guns/NNS during/IN his/PRP visit/NN to/TO the/DT u/NNP s/NNP\n",
      "predicted: in/IN a/DT meeting/NN with/IN premier/NNP li/NNP peng/NNP on/IN monday/NNP mr/NNP nixon/NNP said/VBD that/IN he/PRP hoped/VBD 0/NONE he/PRP would/MD n't/RB encounter/VB guards/NNS with/IN machine/NN guns/NNS during/IN his/PRP visit/NN to/TO the/DT u/NNP\n",
      " \n",
      "labeled  : the/DT stock/NN would/MD be/VB redeemed/VBN 1/NONE in/IN five/CD years/NNS subject/JJ to/TO terms/NNS of/IN the/DT surviving/VBG company/NN 's/POS debt/NN\n",
      "predicted: the/DT stock/NN would/MD be/VB redeemed/VBN 1/NONE in/IN five/CD years/NNS subject/JJ to/TO terms/NNS of/IN the/DT surviving/NN company/NN 's/POS debt/NN\n",
      " \n",
      "labeled  : back/RB downtown/NN the/DT execs/NNS squeezed/VBD in/RP a/DT few/JJ meetings/NNS at/IN the/DT hotel/NN before/IN 1/NONE boarding/VBG the/DT buses/NNS again/RB\n",
      "predicted: back/RB downtown/NN the/DT execs/NNS squeezed/VBD in/NONE a/DT few/JJ meetings/NNS at/IN the/DT hotel/NN before/IN 1/NONE boarding/VBG the/DT buses/NNS again/RB\n",
      " \n",
      "labeled  : there/EX 's/VBZ no/DT question/NN that/IN some/DT of/IN those/DT workers/NNS and/CC managers/NNS contracted/VBD asbestos/JJ related/NNS diseases/'' ''/VBD said/NONE t/NNP 1/NNP darrell/NN phillips/NN vice/IN president/JJ of/NNS human/IN resources/NNP for/CC hollingsworth/NNP\n",
      "predicted: there/EX 's/VBZ no/DT question/NN that/IN some/DT of/IN those/DT workers/NNS and/CC managers/NNS contracted/VBD asbestos/JJ related/NNS diseases/'' ''/'' said/NONE t/NNP 1/NNP darrell/NN phillips/NN vice/NN president/IN of/NNS human/NNS resources/IN for/CC hollingsworth/NNP\n",
      " \n",
      "labeled  : department/NN officials/NNS say/VBP that/IN hhs/NNP secretary/NNP louis/NNP sullivan/NNP will/MD support/VB dr/NNP mason/NNP 's/POS ruling/NN which/WDT t/NONE 112/MD will/VB be/VBN issued/NONE 1/RB soon/IN in/DT the/NN form/IN of/DT a/NN letter/TO to/DT the/JJ acting/NN director/IN of/DT the/NNP national/NNPS institutes/IN of/NNP\n",
      "predicted: department/NN officials/NNS say/VBP that/IN hhs/NNP secretary/NNP louis/NNP sullivan/NNP will/MD support/VB dr/NNP mason/NNP 's/POS ruling/NN which/WDT t/NONE 112/MD will/VB be/VBN issued/NONE 1/RB soon/IN in/DT the/NN form/IN of/DT a/NN letter/TO to/DT the/JJ acting/NN director/IN of/DT the/NNP national/NNP institutes/IN\n",
      " \n",
      "labeled  : 80/CD 8/CD million/NONE u/IN of/NN single/NN family/NNS program/CD bonds/JJ 1989/CC fourth/JJ and/NN fifth/RB series/VBN tentatively/NONE priced/IN by/DT a/NNP merrill/NNP lynch/NNP capital/NNPS markets/NN group/NONE to/TO yield/VB from/IN 6/CD 25/NN in/IN 1992/CD for/IN fourth/JJ series/NN bonds/NNS to/TO 7/CD 74/NN in/IN 2029/CD for/IN fifth/JJ series/NN bonds/NNS\n",
      "predicted: 80/CD 8/CD million/CD u/IN of/JJ single/NN family/NN program/NNS bonds/NNS 1989/JJ fourth/CC and/NN fifth/NN series/RB tentatively/VBN priced/IN by/DT a/NNP merrill/NNP lynch/NNP capital/NNP markets/NN group/NN to/TO yield/TO from/IN 6/CD 25/CD in/IN 1992/IN for/IN fourth/JJ series/NN bonds/NNS to/TO 7/CD 74/CD in/IN 2029/IN for/IN fifth/NN series/NNS bonds/NNS\n",
      " \n",
      "labeled  : the/DT purchasing/NNP management/NNP association/NNP of/IN chicago/NNP 's/POS october/NNP index/NN rose/VBD to/TO 51/CD 6/NN after/IN three/CD previous/JJ months/NNS of/IN UNK/NNS below/IN 50/CD PAD/NN\n",
      "predicted: the/DT purchasing/NNP management/NN association/NNP of/IN chicago/NNP 's/POS october/NNP index/NN rose/VBD to/TO 51/CD 6/CD after/IN three/CD previous/JJ months/NNS of/IN UNK/CD below/IN 50/CD\n",
      " \n",
      "labeled  : he/PRP described/VBD the/DT situation/NN as/IN an/DT escrow/NN problem/NN a/DT timing/NN issue/NN ''/'' which/WDT he/PRP said/VBD 0/NONE t/NONE 1/VBD was/RB rapidly/VBN rectified/NONE 2/IN with/DT no/NNS losses/TO to/NNS\n",
      "predicted: he/PRP described/VBD the/DT situation/NN as/IN an/DT escrow/NN problem/NN a/DT timing/NN issue/NN ''/'' which/WDT he/PRP said/VBD 0/NONE t/NONE 1/RB was/RB rapidly/VBN rectified/NONE 2/IN with/DT no/NNS losses/NNS\n",
      " \n",
      "labeled  : UNK/NNS UNK/VBP to/TO a/DT UNK/NN of/IN the/DT eye/NN 's/POS natural/JJ lens/NN\n",
      "predicted: UNK/NN UNK/NN to/TO a/DT UNK/DT of/IN the/DT eye/NN 's/POS natural/JJ lens/NN\n",
      " \n",
      "labeled  : mr/NNP trudeau/NNP 's/POS attorney/NN norman/NNP k/NNP samnick/NNP said/VBD 0/NONE the/DT harassment/NN consists/VBZ mainly/RB of/IN the/DT guild/NN 's/POS year/JJ long/NNS threats/IN of/JJ disciplinary/NN\n",
      "predicted: mr/NNP trudeau/NNP 's/POS attorney/NN norman/NNP k/NNP samnick/NNP said/VBD 0/NONE the/DT harassment/NN consists/VBZ mainly/RB of/IN the/DT guild/NN 's/POS year/JJ long/NNS threats/IN of/JJ disciplinary/JJ\n",
      " \n",
      "labeled  : and/CC while/IN there/EX was/VBD no/DT profit/NN ich/NONE 1/DT this/NN year/IN from/VBN discontinued/NNS operations/JJ last/NN year/PRP they/VBD contributed/CD 34/CD million/NONE u/IN before/NN\n",
      "predicted: and/CC while/IN there/EX was/VBD no/DT profit/NN ich/NONE 1/NONE this/NN year/IN from/NNS discontinued/NNS operations/JJ last/NN year/PRP they/VBD contributed/CD 34/CD million/NONE\n",
      " \n",
      "labeled  : that/DT has/VBZ been/VBN particularly/RB true/JJ this/DT year/NN with/IN many/JJ companies/NNS raising/VBG their/PRP payouts/NNS more/JJR than/IN 10/CD PAD/NN\n",
      "predicted: that/DT has/VBZ been/VBN particularly/RB true/JJ this/DT year/NN with/IN many/JJ companies/NNS raising/VBG their/PRP payouts/NNS more/JJR than/IN 10/CD\n",
      " \n",
      "labeled  : the/DT fire/NN is/VBZ also/RB fueled/VBN 1/NONE by/IN growing/JJ international/JJ interest/NN in/IN japanese/JJ behavior/NN\n",
      "predicted: the/DT fire/NN is/VBZ also/RB fueled/VBN 1/NONE by/IN growing/VBG international/JJ interest/NN in/IN japanese/JJ behavior/NN\n",
      " \n",
      "labeled  : eaton/NNP corp/NNP said/VBD 0/NONE it/PRP sold/VBD its/PRP pacific/NNP sierra/NNP research/NNP corp/NNP unit/NN to/TO a/DT company/NN formed/VBN by/NONE employees/IN of/NNS that/IN unit/DT PAD/NN\n",
      "predicted: eaton/NNP corp/NNP said/VBD 0/NONE it/PRP sold/VBD its/PRP pacific/NNP sierra/NNP research/NNP corp/NNP unit/NN to/TO a/DT company/NN formed/VBN by/NONE employees/IN of/IN that/IN unit/NN\n",
      " \n",
      "labeled  : railroad/NN companies/NNS and/CC some/DT ports/NNS are/VBP UNK/VBG a/DT sudden/JJ UNK/NN of/IN business/NN\n",
      "predicted: railroad/NN companies/NNS and/CC some/DT ports/NNS are/VBP UNK/JJ a/DT sudden/JJ UNK/NN of/IN business/NN\n",
      " \n",
      "labeled  : if/IN not/RB for/IN a/DT 59/CD 6/NN surge/NN in/IN orders/NNS for/IN capital/NN goods/NNS by/IN defense/NN contractors/NNS factory/NN orders/NNS would/MD have/VB fallen/VBN 2/CD 1/NN\n",
      "predicted: if/IN not/RB for/IN a/DT 59/CD 6/NN surge/NN in/IN orders/NNS for/IN capital/NN goods/NNS by/IN defense/NN contractors/NNS factory/NN orders/NNS would/MD have/VB fallen/VBN 2/NONE 1/NONE\n",
      " \n",
      "labeled  : coincident/JJ with/IN the/DT talks/NNS the/DT state/NNP department/NNP said/VBD 0/NONE it/PRP has/VBZ permitted/VBN a/DT soviet/JJ bank/NN to/TO open/VB a/DT new/NNP york/NNP branch/NN\n",
      "predicted: coincident/JJ with/IN the/DT talks/NNS the/DT state/NN department/NN said/VBD 0/NONE it/PRP has/VBZ permitted/VBN a/DT soviet/JJ bank/NN to/TO open/VB a/DT new/JJ york/NNP branch/NN\n",
      " \n",
      "labeled  : but/CC the/DT exact/JJ amount/NN of/IN reliance/NNP 's/POS current/JJ holding/NN has/VBZ n't/RB been/VBN formally/RB disclosed/VBN 1/NONE\n",
      "predicted: but/CC the/DT exact/JJ amount/NN of/IN reliance/NNP 's/POS current/JJ holding/NN has/VBZ n't/RB been/VBN formally/RB disclosed/VBN 1/NONE\n",
      " \n",
      "labeled  : the/DT dutch/JJ chemical/NN group/NN said/VBD 0/NONE net/JJ income/NN gained/VBD to/TO 235/CD million/CD guilders/NNS lrb/LRB 113/CD 2/CD million/NONE u/RRB rrb/CC or/CD 6/NNS 70/DT guilders/NN a/IN share/CD from/CD 144/NNS million/CC guilders/CD or/NNS 4/DT 10/NN guilders/DT a/NN share/IN\n",
      "predicted: the/DT dutch/JJ chemical/NN group/NN said/VBD 0/NONE net/JJ income/NN gained/VBD to/TO 235/CD million/CD guilders/NNS lrb/LRB 113/CD 2/CD million/CD u/CC rrb/CC or/CD 6/CD 70/NNS guilders/NN a/NN share/IN from/CD 144/CD million/CD guilders/CC or/CD 4/CD 10/NN guilders/DT a/NN share/DT a/NN\n",
      " \n",
      "labeled  : but/CC in/IN one/CD minor/JJ matter/NN mr/NNP nixon/NNP appears/VBZ 1/NONE to/TO have/VB gained/VBN a/DT concession/NN\n",
      "predicted: but/CC in/IN one/CD minor/JJ matter/NN mr/NNP nixon/NNP appears/VBZ 1/NONE to/TO have/VB gained/VBN a/DT concession/NN\n",
      " \n",
      "labeled  : domestic/JJ sales/NNS of/IN construction/NN machinery/NN such/JJ as/IN power/NN shovels/NNS and/CC UNK/NNS rose/VBD to/TO 142/CD 84/CD billion/NN yen/IN from/CD 126/CD 15/NN\n",
      "predicted: domestic/JJ sales/NNS of/IN construction/NN machinery/NN such/JJ as/IN power/NN shovels/NNS and/CC UNK/VBD rose/VBD to/TO 142/CD 84/CD billion/NN yen/IN from/CD 126/CD\n",
      " \n",
      "labeled  : on/IN the/DT new/NNP york/NNP mercantile/NNP exchange/NNP heating/NN oil/NN for/IN december/NNP delivery/NN increased/VBD 1/CD 25/NNS cents/NONE 1/TO to/VB settle/IN at/CD 60/NNS 36/DT cents/NN\n",
      "predicted: on/IN the/DT new/NNP york/NNP mercantile/NNP exchange/NNP heating/NN oil/NN for/IN december/NNP delivery/NN increased/VBD 1/CD 25/CD cents/NNS 1/TO to/VB settle/IN at/CD 60/NNS 36/NNS cents/DT a/NN\n",
      " \n",
      "labeled  : with/IN slower/JJR economic/JJ growth/NN and/CC flat/JJ corporate/JJ earnings/NNS likely/JJ next/IN year/NN i/PRP would/MD n't/RB look/VB for/IN the/DT market/NN to/TO have/VB much/JJ upside/RB from/IN current/JJ levels/NNS\n",
      "predicted: with/IN slower/JJR economic/JJ growth/NN and/CC flat/JJ corporate/JJ earnings/NNS likely/JJ next/JJ year/NN i/PRP would/MD n't/RB look/VB for/IN the/DT market/NN to/TO have/VB much/JJ upside/RB from/IN current/JJ levels/NNS\n",
      " \n",
      "labeled  : judging/NONE from/VBG the/IN americana/DT in/NNS haruki/IN murakami/NNP 's/NNP a/POS wild/DT sheep/NNP chase/NNP ''/NNP lrb/'' kodansha/LRB 320/NNP pages/CD 18/NNS 95/CD u/NONE rrb/RRB baby/NN boomers/NNS on/IN both/DT sides/NNS of/IN the/DT pacific/NNP have/VBP a/DT lot/NN in/IN common/NN\n",
      "predicted: judging/NONE from/IN the/IN americana/DT in/IN haruki/IN murakami/NNP 's/POS a/DT wild/DT sheep/NNP chase/NNP ''/'' lrb/LRB kodansha/LRB 320/NNP pages/NNS 18/CD 95/NONE u/NONE rrb/RRB baby/NN boomers/NNS on/IN both/DT sides/NNS of/IN the/DT pacific/NN have/VBP a/DT lot/NN in/IN common/JJ\n",
      " \n",
      "labeled  : for/IN american/NNP express/NNP the/DT promotion/NN is/VBZ part/NN of/IN an/DT effort/NN to/NONE broaden/TO the/VB use/DT of/NN its/IN card/PRP for/NN retail/IN sales/JJ where/NNS the/WRB company/DT expects/NN 2/VBZ to/NONE get/TO much/VB of/RB the/IN future/DT growth/JJ in/NN its/IN card/PRP business/NN t/NN 1/NONE\n",
      "predicted: for/IN american/NNP express/NNP the/DT promotion/NN is/VBZ part/NN of/IN an/DT effort/NN to/NONE broaden/TO the/VB use/VB of/IN its/NN card/IN for/IN retail/NNS sales/NNS where/WRB the/DT company/NN expects/VBZ 2/NONE to/TO get/VB much/JJ of/IN the/IN future/NN growth/NN in/IN its/PRP card/NN business/NN t/NONE\n",
      " \n",
      "labeled  : some/DT analysts/NNS are/VBP worried/VBN that/IN reports/NNS of/IN the/DT grain/NN industry/NN 's/POS problems/NNS might/MD spark/VB investors/NNS to/TO begin/VB 1/NONE buying/VBG corn/NN futures/NNS contracts/NNS 2/NONE only/RB to/TO see/VB little/JJ appreciation/NN\n",
      "predicted: some/DT analysts/NNS are/VBP worried/VBN that/IN reports/NNS of/IN the/DT grain/NN industry/NN 's/POS problems/NNS might/MD spark/VB investors/NNS to/TO begin/VB 1/NONE buying/VBG corn/NN futures/NNS contracts/NNS 2/NONE only/RB to/TO see/VB little/JJ appreciation/NN\n",
      " \n",
      "labeled  : the/DT march/NNP delivery/NN which/WDT t/NONE 1/VBZ has/DT no/NNS limits/VBD settled/IN at/CD 14/NNS 53/RB cents/CD up/NN 0/DT 56/NN\n",
      "predicted: the/DT march/NNP delivery/NN which/WDT t/NONE 1/VBZ has/DT no/NNS limits/VBD settled/IN at/CD 14/NNS 53/NNS cents/IN up/DT 0/DT 56/NN\n",
      " \n",
      "labeled  : performing/VBG loans/NNS\n",
      "predicted: performing/VBG loans/NNS\n",
      " \n",
      "labeled  : but/CC a/DT 1986/CD law/NN that/WDT t/NONE 1/RB supposedly/VBD replaced/NNS lawsuits/IN over/NNS children/POS 's/NNS UNK/IN with/DT a/NN compensation/NN fund/VBZ has/RB UNK/VBN led/TO to/VB even/JJR more/NN\n",
      "predicted: but/CC a/DT 1986/CD law/NN that/WDT t/NONE 1/RB supposedly/RB replaced/NNS lawsuits/IN over/NNS children/POS 's/NN UNK/IN with/DT a/NN compensation/NN fund/VBZ has/VBZ UNK/NONE led/TO to/VB even/RB more/NN\n",
      " \n",
      "labeled  : world/NN sugar/NN futures/NNS prices/NNS soared/VBD on/IN rumors/NNS that/IN brazil/NNP a/DT major/JJ UNK/NN and/CC exporter/NN might/MD not/RB ship/VB sugar/NN this/DT crop/NN year/NN and/CC next/JJ\n",
      "predicted: world/NN sugar/NN futures/NNS prices/NNS soared/VBD on/IN rumors/NNS that/IN brazil/NNP a/DT major/JJ UNK/NN and/CC exporter/NN might/MD not/RB ship/VB sugar/NN this/NN crop/NN year/NN and/CC next/JJ\n",
      " \n",
      "labeled  : the/DT results/NNS reflected/VBD a/DT 24/CD gain/NN in/NN income/IN from/NN its/IN finance/PRP businesses/NN and/NNS a/CC 15/DT slide/CD in/NN income/NN from/IN insurance/NN operations/IN PAD/NN PAD/NNS\n",
      "predicted: the/DT results/NNS reflected/VBD a/DT 24/CD gain/NN in/NN income/IN from/PRP its/NN finance/NNS businesses/CC and/CC a/NN 15/CD slide/NN in/NN income/NN from/NN insurance/NN\n",
      " \n",
      "labeled  : many/JJ auto/NN dealers/NNS now/RB let/VBP car/NN buyers/NNS charge/VB part/NN or/CC all/DT of/IN their/PRP purchase/NN on/IN the/DT american/NNP express/NNP card/NN but/CC few/JJ card/NN holders/NNS realize/VBP this/DT mr/NNP riese/NNP says/VBZ 0/NONE t/NONE\n",
      "predicted: many/JJ auto/NN dealers/NNS now/RB let/VBG car/NN buyers/NNS charge/VBP part/NN or/CC all/DT of/IN their/PRP purchase/NN on/IN the/DT american/NNP express/NNP card/CC but/CC few/JJ card/NN holders/NNS realize/VBP this/DT mr/NNP riese/NNP says/VBZ 0/NONE t/NONE\n",
      " \n",
      "labeled  : the/DT company/NN also/RB adopted/VBD an/DT anti/JJ takeover/NN\n",
      "predicted: the/DT company/NN also/RB adopted/VBD an/DT anti/JJ takeover/NN\n",
      " \n",
      "labeled  : seoul/NNP also/RB has/VBZ instituted/VBN effective/JJ search/JJ and/NNS seizure/NONE procedures/NONE 0/TO to/VB aid/DT these/NNS teams/NONE t/PRP 2/VBD she/NONE said/NONE\n",
      "predicted: seoul/NNP also/RB has/VBZ instituted/VBN effective/JJ search/JJ and/NNS seizure/NONE procedures/NONE 0/TO to/VB aid/DT these/NNS teams/NONE t/NONE 2/PRP she/VBD said/NONE\n",
      " \n",
      "labeled  : a/DT medium/JJ sized/CD one/IN in/NNP brooklyn/PRP it/VBZ says/NONE 0/NONE t/MD 1/VB could/VBN be/NONE altered/TO 2/VB to/IN house/TO up/CD to/NNS 1/IN 000/DT inmates/JJR at/NN a/IN lower/NONE cost/VBG than/DT building/JJ a/NN new/IN prison/JJ in/NNP upstate/NNP\n",
      "predicted: a/DT medium/JJ sized/CD one/IN in/IN brooklyn/PRP it/VBZ says/NONE 0/NONE t/MD 1/MD could/VB be/NONE altered/TO 2/TO to/TO house/IN up/CD to/CD 1/NNS 000/IN inmates/IN at/DT a/NN lower/JJR cost/IN than/DT building/DT a/NN new/IN prison/IN in/NNP upstate/NNP\n",
      " \n",
      "labeled  : says/VBZ ich/NONE 1/NNP mr/NNP baldwin/PRP we/VBP recognize/IN that/PRP we/MD may/RB no/RBR longer/VBP have/RB as/JJ high/DT a/NN priority/IN in/NN church/NN life/CC and/NN experience/''\n",
      "predicted: says/VBZ ich/NONE 1/NNP mr/NNP baldwin/PRP we/PRP recognize/IN that/PRP we/MD may/VB no/RB longer/JJR have/IN as/DT high/DT a/NN priority/IN in/NN church/NN life/NN and/NN\n",
      " \n",
      "labeled  : however/RB third/NN quarter/NN operating/NN profit/VBD fell/CD 14/NN as/IN usx/NNP sold/VBD sizable/JJ UNK/NNS of/IN its/PRP diversified/JJ and/CC steel/NN segments/NNS eliminating/NONE income/VBG from/NN those/IN operations/DT PAD/NNS\n",
      "predicted: however/RB third/JJ quarter/NN operating/NN profit/NN fell/VBD 14/CD as/IN usx/NNP sold/VBD sizable/JJ UNK/NN of/IN its/PRP diversified/JJ and/CC steel/NN segments/NNS eliminating/NONE income/VBG from/IN those/IN operations/DT\n",
      " \n",
      "labeled  : at/IN one/CD point/NN mr/NNP phelan/NNP angered/VBD the/DT subcommittee/NN 's/POS chairman/NN rep/NNP edward/NNP markey/NNP lrb/LRB d/NNP mass/NNP rrb/RRB by/IN 2/NONE not/RB going/VBG much/RB beyond/IN what/WP t/NONE 1/RB already/VBD had/VBN been/VBN reported/NONE 131/IN in/DT the/NN morning/NNS\n",
      "predicted: at/IN one/CD point/NN mr/NNP phelan/NNP angered/VBD the/DT subcommittee/NN 's/POS chairman/NN rep/NNP edward/NNP markey/NNP lrb/LRB d/NNP mass/NNP rrb/NNP by/IN 2/NONE not/RB going/VBG much/RB beyond/NONE what/WP t/NONE 1/RB already/VBD had/VBN been/VBN reported/IN 131/IN in/DT the/NN\n",
      " \n",
      "labeled  : part/NN of/IN the/DT problem/NN is/VBZ that/IN chip/NN buyers/NNS are/VBP keeping/VBG inventories/NNS low/JJ because/IN of/IN jitters/NNS about/IN the/DT course/NN of/IN the/DT u/NNP s/NN\n",
      "predicted: part/NN of/IN the/DT problem/NN is/VBZ that/IN chip/NN buyers/NNS are/VBP keeping/VBG inventories/NNS low/JJ because/IN of/IN jitters/NNS about/IN the/DT course/NN of/IN the/DT u/NNP\n",
      " \n",
      "labeled  : if/IN you/PRP 'd/MD really/RB rather/RB have/VB a/DT buick/NNP do/NONE n't/VB leave/RB home/VB without/NN the/IN american/DT express/NNP card/NNP PAD/NN\n",
      "predicted: if/IN you/PRP 'd/MD really/RB rather/RB have/VB a/DT buick/NN do/VBP n't/RB leave/VB home/VB without/IN the/IN american/DT express/NNP card/NN\n",
      " \n",
      "labeled  : when/WRB you/PRP do/VBP that/DT t/NONE 1/EX there/VBZ is/RB not/DT a/NN cut/CC but/EX there/VBZ is/IN in/NN fact/DT a/NN program/NN increase/IN of/CD 5/CD million/NONE u/'' ''/DT each/IN for/DT the/NNP ftc/CC and/DT the/NNP justice/NNP department/NNP rep/NNP neal/NNP smith/LRB lrb/NNP d/NNP iowa/RRB rrb/VBD said/NONE 0/NONE t/IN 2/NNP during/NN\n",
      "predicted: when/WRB you/PRP do/VBP that/NONE t/NONE 1/VBZ there/VBZ is/RB not/DT a/NN cut/CC but/CC there/VBZ is/IN in/DT fact/DT a/NN program/NN increase/IN of/CD 5/CD million/NONE u/'' ''/DT each/IN for/DT the/NNP ftc/CC and/DT the/NNP justice/NNP department/NNP rep/NNP neal/NNP smith/NNP lrb/NNP d/NNP iowa/VBD rrb/VBD said/NONE 0/NONE t/IN 2/IN during/NN\n",
      " \n",
      "labeled  : spirit/NN of/IN perestroika/FW touches/VBZ design/NN world/NN\n",
      "predicted: spirit/NN of/IN perestroika/PRP touches/VBZ design/NN world/NN\n",
      " \n",
      "labeled  : mr/NNP bush/NNP 's/POS legislative/JJ package/NN promises/VBZ 1/NONE to/TO cut/VB emissions/NNS by/IN 10/CD million/CD tons/NNS basically/RB in/IN half/DT by/IN the/DT year/NN 2000/CD\n",
      "predicted: mr/NNP bush/NNP 's/POS legislative/JJ package/NN promises/VBZ 1/NONE to/TO cut/VB emissions/NNS by/IN 10/CD million/CD tons/NNS basically/RB in/IN half/DT by/IN the/DT year/NN 2000/CD\n",
      " \n",
      "labeled  : under/IN the/DT so/JJ called/NNP team/NNP taurus/NN approach/NNP mr/NNP veraldi/CC and/JJ other/NNP ford/NN product/NNS planners/VBD sought/DT the/NN involvement/IN of/NNS parts/NNS suppliers/NN assembly/NNS line/NN workers/NNS auto/CC designers/JJ and/NN financial/NNS staff/IN members/DT from/JJ the/NNS initial/IN stages/DT of/NN the/NN\n",
      "predicted: under/IN the/DT so/JJ called/NNP team/NN taurus/NN approach/NNP mr/NNP veraldi/CC and/JJ other/JJ ford/NN product/NNS planners/VBD sought/NN the/NN involvement/IN of/NNS parts/NNS suppliers/NN assembly/NN line/NN workers/NNS auto/NN designers/CC and/JJ financial/NNS staff/NNS members/IN from/DT the/NNS initial/IN stages/DT of/NN the/NN\n",
      " \n",
      "labeled  : he/PRP factors/VBZ that/DT into/IN the/DT market/NN yield/NN 1/NONE to/TO get/VB an/DT adjusted/VBN yield/NN of/IN about/RB 3/CD 6/NN\n",
      "predicted: he/PRP factors/NNS that/IN into/IN the/DT market/NN yield/NN 1/NONE to/TO get/VB an/DT adjusted/VBN yield/NN of/IN about/IN 3/CD 6/CD\n",
      " \n",
      "labeled  : at/IN the/DT same/JJ time/NN though/RB he/PRP chastised/VBD the/DT media/NN for/IN paying/NONE such/VBG close/JJ attention/JJ to/NN japanese/TO investment/JJ when/NN other/WRB foreign/JJ countries/JJ notably/NNS britain/RB are/NNP acquiring/VBP more/VBG american/JJR assets/JJ t/NNS 1/NONE\n",
      "predicted: at/IN the/DT same/JJ time/NN though/IN he/PRP chastised/VBD the/DT media/NNS for/IN paying/VBG such/JJ close/JJ attention/NN to/NN japanese/JJ investment/JJ when/JJ other/JJ foreign/JJ countries/NNS notably/NNS britain/VBP are/VBP acquiring/VBG more/JJR american/JJ assets/NNS t/NONE\n",
      " \n",
      "labeled  : the/DT mathematics/NN section/NN of/IN the/DT widely/RB used/JJ california/NNP achievement/NNP test/NNP asks/VBZ fifth/JJ graders/NNS what/WP is/VBZ t/NONE 1/DT another/NN name/IN for/DT the/JJ roman/NN numeral/CD ix/''\n",
      "predicted: the/DT mathematics/NN section/NN of/IN the/DT widely/RB used/VBN california/NNP achievement/NNP test/NN asks/VBZ fifth/VBZ graders/NNS what/WP is/VBZ t/NONE 1/DT another/NN name/IN for/DT the/NN roman/NN numeral/CD\n",
      " \n",
      "labeled  : but/CC like/IN mr/NNP egnuss/NNP few/JJ expect/VBP it/PRP to/TO be/VB halted/VBN 1/NONE entirely/RB and/CC a/DT surprising/JJ number/NN doubt/NN 0/NONE it/PRP should/MD be/VB PAD/NONE\n",
      "predicted: but/CC like/IN mr/NNP egnuss/NNP few/JJ expect/VBP it/PRP to/NONE be/VB halted/VBN 1/NONE entirely/RB and/CC a/DT surprising/JJ number/NN doubt/NN 0/NONE it/PRP should/MD be/VB\n",
      " \n",
      "labeled  : the/DT company/NN said/VBD 0/NONE its/PRP industrial/JJ unit/NN continues/VBZ 1/NONE to/TO face/VB margin/NN pressures/NNS and/CC lower/JJR demand/NN\n",
      "predicted: the/DT company/NN said/VBD 0/NONE its/PRP industrial/JJ unit/NN continues/VBZ 1/NONE to/TO face/VB margin/NN pressures/NNS and/CC lower/JJR demand/NN\n",
      " \n",
      "labeled  : gasoline/NN futures/NNS were/VBD mixed/VBN to/TO unchanged/JJ\n",
      "predicted: gasoline/NN futures/NNS were/VBD mixed/VBN to/TO unchanged/JJ\n",
      " \n",
      "labeled  : there/EX is/VBZ always/RB a/DT chance/NN of/IN recession/NN ''/'' added/VBD t/NONE 2/NNP mr/NNP guffey/CC but/IN if/PRP you/VBP ask/PRP me/NONE 3/TO to/VB put/DT a/NN percentage/IN on/PRP it/PRP i/MD would/VB think/NONE 0/PRP it/VBZ 's/RB well/IN below/DT a/CD 50/NN chance/NN\n",
      "predicted: there/EX is/VBZ always/RB a/DT chance/NN of/IN recession/NN ''/'' added/VBD t/NONE 2/NNP mr/NNP guffey/CC but/IN if/PRP you/PRP ask/PRP me/NONE 3/TO to/VB put/DT a/NN percentage/IN on/PRP it/PRP i/MD would/VB think/NONE 0/PRP it/VBZ 's/RB well/IN below/DT\n",
      " \n",
      "labeled  : consolidated/NNP rail/NNP corp/NNP said/VBD 0/NONE it/PRP would/MD spend/VB more/RBR than/IN 30/CD million/CD u/NONE on/IN 1/CD 000/VBN enclosed/NNS railcars/IN for/NONE transporting/VBG autos/NNS\n",
      "predicted: consolidated/NNP rail/NNP corp/NNP said/VBD 0/NONE it/PRP would/MD spend/VB more/JJR than/IN 30/CD million/CD u/NONE on/IN 1/CD 000/NNS enclosed/NNS railcars/IN for/NONE transporting/VBG autos/NNS\n",
      " \n",
      "labeled  : its/PRP index/NN inched/VBD up/RB to/TO 47/CD 6/NN in/IN october/NNP from/IN 46/CD in/NN september/IN PAD/NNP\n",
      "predicted: its/PRP index/NN inched/VBD up/IN to/TO 47/CD 6/NN in/IN october/IN from/IN 46/CD in/IN september/NNP\n",
      " \n",
      "labeled  : the/DT company/NN is/VBZ contesting/VBG the/DT fine/NN\n",
      "predicted: the/DT company/NN is/VBZ contesting/VBG the/DT fine/NN\n",
      " \n",
      "labeled  : a/DT spokesman/NN for/IN the/DT irs/NNP confirmed/VBD that/IN there/EX has/VBZ been/VBN correspondence/NN mailed/VBN about/NONE incomplete/IN 8300s/JJ ''/NNS but/'' he/CC declined/PRP 1/VBD to/NONE say/TO why/VB the/WRB letters/DT were/NNS sent/VBD 2/VBN to/NONE lawyers/TO now/NNS PAD/RB\n",
      "predicted: a/DT spokesman/NN for/IN the/DT irs/NNP confirmed/VBD that/IN there/EX has/VBZ been/VBN correspondence/NN mailed/VBN about/NONE incomplete/IN 8300s/JJ ''/CC but/CC he/PRP declined/NONE 1/NONE to/VB say/VB why/WRB the/NNS letters/NNS were/VBD sent/VBN 2/NONE to/TO lawyers/NNS now/RB\n",
      " \n",
      "labeled  : the/DT student/NN surrendered/VBD the/DT notes/NNS but/CC not/RB without/IN a/DT protest/NN\n",
      "predicted: the/DT student/NN surrendered/VBD the/DT notes/NNS but/CC not/RB without/IN a/DT protest/NN\n",
      " \n",
      "labeled  : 1/NONE encouraged/JJ 2/NONE by/IN mrs/NNP ward/NNP mrs/NNP yeargin/NNP taught/VBD honor/NN students/NNS in/IN the/DT state/NN teacher/NN cadet/NN ''/'' program/NN a/DT reform/NN creation/NN designed/VBN 3/NONE to/NONE encourage/TO good/VB students/JJ to/NNS consider/TO teaching/VB as/NN a/IN career/DT PAD/NN\n",
      "predicted: 1/NONE encouraged/JJ 2/NONE by/IN mrs/NNP ward/NNP mrs/NNP yeargin/NNP taught/VBD honor/NN students/NNS in/IN the/DT state/NN teacher/NN cadet/NN ''/'' program/NN a/DT reform/NN creation/NN designed/VBN 3/NONE to/TO encourage/TO good/VB students/NNS to/TO consider/TO teaching/VB as/IN a/DT career/DT PAD/NN\n",
      " \n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 372ms/step\n",
      "CPU times: user 1.53 s, sys: 46.1 ms, total: 1.57 s\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for test_batch in test_dataset:\n",
    "    inputs_b, outputs_b = test_batch\n",
    "    preds_b = best_model.predict(inputs_b)\n",
    "    # convert from categorical to list of ints\n",
    "    preds_b = np.argmax(preds_b, axis=-1)\n",
    "    outputs_b = np.argmax(outputs_b.numpy(), axis=-1)\n",
    "    for i, (pred_l, output_l) in enumerate(zip(preds_b, outputs_b)):\n",
    "        assert(len(pred_l) == len(output_l))\n",
    "        pad_len = np.nonzero(output_l)[0][0]\n",
    "        acc = np.count_nonzero(\n",
    "            np.equal(\n",
    "                output_l[pad_len:], pred_l[pad_len:]\n",
    "            )\n",
    "        ) / len(output_l[pad_len:])\n",
    "        accuracies.append(acc)\n",
    "        if is_first_batch:\n",
    "            words = [idx2word_s[x] for x in inputs_b.numpy()[i][pad_len:]]\n",
    "            postags_l = [idx2word_t[x] for x in output_l[pad_len:] if x > 0]\n",
    "            postags_p = [idx2word_t[x] for x in pred_l[pad_len:] if x > 0]\n",
    "            print(\"labeled  : {:s}\".format(\" \".join([\"{:s}/{:s}\".format(w, p) \n",
    "                for (w, p) in zip(words, postags_l)])))\n",
    "            print(\"predicted: {:s}\".format(\" \".join([\"{:s}/{:s}\".format(w, p) \n",
    "                for (w, p) in zip(words, postags_p)])))\n",
    "            print(\" \")\n",
    "    is_first_batch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "79aa250d-d4a8-4387-b683-0876d9e783a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos tagging accuracy: 0.981\n"
     ]
    }
   ],
   "source": [
    "accuracy_score = np.mean(np.array(accuracies))\n",
    "print(\"pos tagging accuracy: {:.3f}\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3495b9-c8c6-43fa-8f7d-e8319eb2cdef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Encoder-decoder architecture - seq2sea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a663d-4d6b-4558-b740-8ab285e2d072",
   "metadata": {},
   "source": [
    "### Example ‒ seq2seq without attention for machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc63dafc-d4ab-49a4-a821-a2df56fbdc7d",
   "metadata": {},
   "source": [
    "(code source: Chapter_5/seq2seq_wo_attn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8bdf1051-124d-4889-8bc1-05eb3189d734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2e69ace-cb46-4b5e-b225-b19cf33268d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_SENT_PAIRS = 30000\n",
    "EMBEDDING_DIM = 256\n",
    "ENCODER_DIM, DECODER_DIM = 1024, 1024\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "39eeb3f8-c043-44ac-b962-bc8a8f2c38ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_up_logs(data_dir):\n",
    "    checkpoint_dir = os.path.join(data_dir, \"checkpoints\")\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        shutil.rmtree(checkpoint_dir, ignore_errors=True)\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    return checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f8c1b249-3b91-4420-b829-cb6186657b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "checkpoint_dir = clean_up_logs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f98ced92-7151-4481-a186-e115c7319ede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent) \n",
    "        if unicodedata.category(c) != \"Mn\"])\n",
    "    sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    sent = sent.lower()\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad823c24-b9c2-4970-8150-c1458dc2e2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code from the repo ... it does not work ... the fixes to this function are in the next cell ..\n",
    "def download_and_read():\n",
    "    en_sents, fr_sents_in, fr_sents_out = [], [], []\n",
    "    local_file = os.path.join(\"datasets\", \"fra.txt\")\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "            en_sent, fr_sent = line.strip().split('\\t')\n",
    "            en_sent = [w for w in preprocess_sentence(en_sent).split()]\n",
    "            fr_sent = preprocess_sentence(fr_sent)\n",
    "            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\n",
    "            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\n",
    "            en_sents.append(en_sent)\n",
    "            fr_sents_in.append(fr_sent_in)\n",
    "            fr_sents_out.append(fr_sent_out)\n",
    "            if i >= num_sent_pairs - 1:\n",
    "                break\n",
    "    return en_sents, fr_sents_in, fr_sents_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e4834e6b-f28b-45b8-b0bf-747751d86c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_read():\n",
    "    en_sents, fr_sents_in, fr_sents_out = [], [], []\n",
    "    local_file = os.path.join(\"datasets\", \"fra.txt\")\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "            en_sent, fr_sent, _ = line.strip().split('\\t')\n",
    "            en_sent = [w for w in preprocess_sentence(en_sent).split()]\n",
    "            fr_sent = preprocess_sentence(fr_sent)\n",
    "            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\n",
    "            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\n",
    "            en_sents.append(en_sent)\n",
    "            fr_sents_in.append(fr_sent_in)\n",
    "            fr_sents_out.append(fr_sent_out)\n",
    "            if i >= NUM_SENT_PAIRS - 1:\n",
    "                break\n",
    "    return en_sents, fr_sents_in, fr_sents_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeacc9db-0677-4dbf-9c37-829edf8d090d",
   "metadata": {},
   "source": [
    "The function download_and_read does NOT download the file. You must manually download the file, from the download_url, then unzip the 'fra.txt' file into the datasets subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3c7fba4-fa4d-4b42-8db6-08064c4aaccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data preparation\n",
    "download_url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "sents_en, sents_fr_in, sents_fr_out = download_and_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1394ab5a-ed4f-43e5-a22e-68397e2a9882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en)\n",
    "data_en = tokenizer_en.texts_to_sequences(sents_en)\n",
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08e11834-2f95-456b-aa2f-ae660b1b54e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters=\"\", lower=False)\n",
    "tokenizer_fr.fit_on_texts(sents_fr_in)\n",
    "tokenizer_fr.fit_on_texts(sents_fr_out)\n",
    "data_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\n",
    "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding=\"post\")\n",
    "data_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\n",
    "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42247718-f432-4dc4-b458-04b9f3a6b0f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size (en): 4366, vocab size (fr): 7665\n"
     ]
    }
   ],
   "source": [
    "vocab_size_en = len(tokenizer_en.word_index)\n",
    "vocab_size_fr = len(tokenizer_fr.word_index)\n",
    "word2idx_en = tokenizer_en.word_index\n",
    "idx2word_en = {v:k for k, v in word2idx_en.items()}\n",
    "word2idx_fr = tokenizer_fr.word_index\n",
    "idx2word_fr = {v:k for k, v in word2idx_fr.items()}\n",
    "print(\"vocab size (en): {:d}, vocab size (fr): {:d}\".format(\n",
    "    vocab_size_en, vocab_size_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1c01485c-a214-4ab0-a06e-0247bffb3493",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqlen (en): 7, (fr): 16\n"
     ]
    }
   ],
   "source": [
    "maxlen_en = data_en.shape[1]\n",
    "maxlen_fr = data_fr_out.shape[1]\n",
    "print(\"seqlen (en): {:d}, (fr): {:d}\".format(maxlen_en, maxlen_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c90d2287-436d-44c5-a97c-9decea9ed734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data_en, data_fr_in, data_fr_out))\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = NUM_SENT_PAIRS // 4\n",
    "test_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True)\n",
    "train_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "30adad81-45b8-4b35-ae5a-4d00e177b1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_timesteps, \n",
    "            encoder_dim, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, embedding_dim, input_length=num_timesteps)\n",
    "        self.rnn = tf.keras.layers.GRU(\n",
    "            encoder_dim, return_sequences=False, return_state=True)\n",
    "\n",
    "    def call(self, x, state):\n",
    "        x = self.embedding(x)\n",
    "        x, state = self.rnn(x, initial_state=state)\n",
    "        return x, state\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.encoder_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "64e2a584-0a52-4292-a745-b45d88480dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_timesteps,\n",
    "            decoder_dim, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, embedding_dim, input_length=num_timesteps)\n",
    "        self.rnn = tf.keras.layers.GRU(\n",
    "            decoder_dim, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, state):\n",
    "        x = self.embedding(x)\n",
    "        x, state = self.rnn(x, state)\n",
    "        x = self.dense(x)\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c39289fe-afe1-47d0-b5f4-3546cef7f92e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check encoder/decoder dimensions\n",
    "embedding_dim = EMBEDDING_DIM\n",
    "encoder_dim, decoder_dim = ENCODER_DIM, DECODER_DIM\n",
    "\n",
    "encoder = Encoder(vocab_size_en+1, embedding_dim, maxlen_en, encoder_dim)\n",
    "decoder = Decoder(vocab_size_fr+1, embedding_dim, maxlen_fr, decoder_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "065051bd-44a2-4f2d-89ea-a9a66aadaf34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for encoder_in, decoder_in, decoder_out in train_dataset:\n",
    "#     encoder_state = encoder.init_state(batch_size)\n",
    "#     encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "#     decoder_state = encoder_state\n",
    "#     decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
    "#     break\n",
    "# print(\"encoder input          :\", encoder_in.shape)\n",
    "# print(\"encoder output         :\", encoder_out.shape, \"state:\", encoder_state.shape)\n",
    "# print(\"decoder output (logits):\", decoder_pred.shape, \"state:\", decoder_state.shape)\n",
    "# print(\"decoder output (labels):\", decoder_out.shape)\n",
    "# # encoder input          : (64, 8)\n",
    "# # encoder output         : (64, 1024) state: (64, 1024)\n",
    "# # decoder output (logits): (64, 16, 7658) state: (64, 1024)\n",
    "# # decoder output (labels): (64, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6eaab178-b30d-445a-b8a7-c7cafb44fcaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "279729be-3d84-4014-a8c2-b35ab5c3e526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(ytrue, ypred):\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = scce(ytrue, ypred, sample_weight=mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "62154428-2e4c-4dcf-9890-4cfffb5ca34a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
    "        loss = loss_fn(decoder_out, decoder_pred)\n",
    "    \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fe9a2bdb-6f04-4903-9dbe-fc8b5b1a1b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(encoder, decoder, batch_size, \n",
    "        sents_en, data_en, sents_fr_out, \n",
    "        word2idx_fr, idx2word_fr):\n",
    "    random_id = np.random.choice(len(sents_en))\n",
    "    print(\"input    : \",  \" \".join(sents_en[random_id]))\n",
    "    print(\"label    : \", \" \".join(sents_fr_out[random_id]))\n",
    "\n",
    "    encoder_in = tf.expand_dims(data_en[random_id], axis=0)\n",
    "    decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\n",
    "\n",
    "    encoder_state = encoder.init_state(1)\n",
    "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "    decoder_state = encoder_state\n",
    "\n",
    "    decoder_in = tf.expand_dims(\n",
    "        tf.constant([word2idx_fr[\"BOS\"]]), axis=0)\n",
    "    pred_sent_fr = []\n",
    "    while True:\n",
    "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
    "        decoder_pred = tf.argmax(decoder_pred, axis=-1)\n",
    "        pred_word = idx2word_fr[decoder_pred.numpy()[0][0]]\n",
    "        pred_sent_fr.append(pred_word)\n",
    "        if pred_word == \"EOS\":\n",
    "            break\n",
    "        decoder_in = decoder_pred\n",
    "    \n",
    "    print(\"predicted: \", \" \".join(pred_sent_fr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c606e07c-b81f-4151-8697-aea08a824414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_bleu_score(encoder, decoder, test_dataset, \n",
    "        word2idx_fr, idx2word_fr):\n",
    "\n",
    "    bleu_scores = []\n",
    "    smooth_fn = SmoothingFunction()\n",
    "    for encoder_in, decoder_in, decoder_out in test_dataset:\n",
    "        encoder_state = encoder.init_state(batch_size)\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
    "\n",
    "        # compute argmax\n",
    "        decoder_out = decoder_out.numpy()\n",
    "        decoder_pred = tf.argmax(decoder_pred, axis=-1).numpy()\n",
    "\n",
    "        for i in range(decoder_out.shape[0]):\n",
    "            ref_sent = [idx2word_fr[j] for j in decoder_out[i].tolist() if j > 0]\n",
    "            hyp_sent = [idx2word_fr[j] for j in decoder_pred[i].tolist() if j > 0]\n",
    "            # remove trailing EOS\n",
    "            ref_sent = ref_sent[0:-1]\n",
    "            hyp_sent = hyp_sent[0:-1]\n",
    "            bleu_score = sentence_bleu([ref_sent], hyp_sent, \n",
    "                smoothing_function=smooth_fn.method1)\n",
    "            bleu_scores.append(bleu_score)\n",
    "\n",
    "    return np.mean(np.array(bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d475cc85-8ceb-4cd4-8cf7-6663ea2efc91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.2167\n",
      "input    :  tom left me .\n",
      "label    :  tom m a largue . EOS\n",
      "predicted:  tom a l air . EOS\n",
      "Eval Score (BLEU): 2.225e-02\n",
      "Epoch: 2, Loss: 1.0092\n",
      "input    :  it s a bad time .\n",
      "label    :  c est une mauvaise heure . EOS\n",
      "predicted:  c est un gros . EOS\n",
      "Eval Score (BLEU): 3.058e-02\n",
      "Epoch: 3, Loss: 0.7458\n",
      "input    :  they re mistaken .\n",
      "label    :  ils ont tort . EOS\n",
      "predicted:  ils sont en train de gagner . EOS\n",
      "Eval Score (BLEU): 3.753e-02\n",
      "Epoch: 4, Loss: 0.5897\n",
      "input    :  why would i laugh ?\n",
      "label    :  pourquoi rirais je ? EOS\n",
      "predicted:  pourquoi devrais je demissionner ? EOS\n",
      "Eval Score (BLEU): 4.488e-02\n",
      "Epoch: 5, Loss: 0.4656\n",
      "input    :  we have wine .\n",
      "label    :  nous avons du vin . EOS\n",
      "predicted:  nous sommes en retard . EOS\n",
      "Eval Score (BLEU): 5.860e-02\n",
      "Epoch: 6, Loss: 0.3923\n",
      "input    :  we re married now .\n",
      "label    :  nous sommes desormais mariees . EOS\n",
      "predicted:  nous sommes desormais maries . EOS\n",
      "Eval Score (BLEU): 7.099e-02\n",
      "Epoch: 7, Loss: 0.2922\n",
      "input    :  the earth rotates .\n",
      "label    :  la terre tourne . EOS\n",
      "predicted:  la terre tourne . EOS\n",
      "Eval Score (BLEU): 8.710e-02\n",
      "Epoch: 8, Loss: 0.2939\n",
      "input    :  what was it for ?\n",
      "label    :  c etait pour quoi ? EOS\n",
      "predicted:  c etait pour quoi ? EOS\n",
      "Eval Score (BLEU): 1.002e-01\n",
      "Epoch: 9, Loss: 0.2165\n",
      "input    :  is tom successful ?\n",
      "label    :  tom a t il du succes ? EOS\n",
      "predicted:  tom a t il reussi ? EOS\n",
      "Eval Score (BLEU): 1.112e-01\n",
      "Epoch: 10, Loss: 0.1943\n",
      "input    :  i don t have a cat .\n",
      "label    :  je n ai pas de chat . EOS\n",
      "predicted:  je n ai pas de chat . EOS\n",
      "Eval Score (BLEU): 1.180e-01\n",
      "Epoch: 11, Loss: 0.1782\n",
      "input    :  he s coming .\n",
      "label    :  il est en train d arriver . EOS\n",
      "predicted:  il arrive . EOS\n",
      "Eval Score (BLEU): 1.267e-01\n",
      "Epoch: 12, Loss: 0.1689\n",
      "input    :  we re impartial .\n",
      "label    :  nous sommes impartiaux . EOS\n",
      "predicted:  nous sommes impartiaux . EOS\n",
      "Eval Score (BLEU): 1.315e-01\n",
      "Epoch: 13, Loss: 0.1656\n",
      "input    :  we might die .\n",
      "label    :  il se peut que nous mourions . EOS\n",
      "predicted:  il se peut que nous mourions . EOS\n",
      "Eval Score (BLEU): 1.368e-01\n",
      "Epoch: 14, Loss: 0.1168\n",
      "input    :  termites eat wood .\n",
      "label    :  les termites mangent du bois . EOS\n",
      "predicted:  les termites mangent du bois . EOS\n",
      "Eval Score (BLEU): 1.386e-01\n",
      "Epoch: 15, Loss: 0.1487\n",
      "input    :  is it nearby ?\n",
      "label    :  est ce proche ? EOS\n",
      "predicted:  est ce proche ? EOS\n",
      "Eval Score (BLEU): 1.405e-01\n",
      "Epoch: 16, Loss: 0.1507\n",
      "input    :  you re not dead .\n",
      "label    :  tu n es pas morte . EOS\n",
      "predicted:  vous n etes pas mort . EOS\n",
      "Eval Score (BLEU): 1.424e-01\n",
      "Epoch: 17, Loss: 0.1178\n",
      "input    :  they won .\n",
      "label    :  elles ont gagne . EOS\n",
      "predicted:  elles ont gagne . EOS\n",
      "Eval Score (BLEU): 1.452e-01\n",
      "Epoch: 18, Loss: 0.1198\n",
      "input    :  i caused this .\n",
      "label    :  je l ai cause . EOS\n",
      "predicted:  j ai cause ceci . EOS\n",
      "Eval Score (BLEU): 1.478e-01\n",
      "Epoch: 19, Loss: 0.1210\n",
      "input    :  do cats dream ?\n",
      "label    :  les chats revent ils ? EOS\n",
      "predicted:  les chats revent ils ? EOS\n",
      "Eval Score (BLEU): 1.475e-01\n",
      "Epoch: 20, Loss: 0.1386\n",
      "input    :  i made muffins .\n",
      "label    :  j ai fait des muffins . EOS\n",
      "predicted:  j ai fait des muffins . EOS\n",
      "Eval Score (BLEU): 1.496e-01\n",
      "Epoch: 21, Loss: 0.1127\n",
      "input    :  stop overreacting .\n",
      "label    :  cesse d exagerer . EOS\n",
      "predicted:  cessez de reagir de facon excessive . EOS\n",
      "Eval Score (BLEU): 1.511e-01\n",
      "Epoch: 22, Loss: 0.1575\n",
      "input    :  i don t gossip .\n",
      "label    :  je ne commere pas . EOS\n",
      "predicted:  je ne commere pas . EOS\n",
      "Eval Score (BLEU): 1.520e-01\n",
      "Epoch: 23, Loss: 0.1189\n",
      "input    :  they re cute .\n",
      "label    :  elles sont mignonnes . EOS\n",
      "predicted:  ils sont mignons . EOS\n",
      "Eval Score (BLEU): 1.537e-01\n",
      "Epoch: 24, Loss: 0.1347\n",
      "input    :  we both love you .\n",
      "label    :  nous t aimons tous les deux . EOS\n",
      "predicted:  nous t aimons tous les deux . EOS\n",
      "Eval Score (BLEU): 1.543e-01\n",
      "Epoch: 25, Loss: 0.0968\n",
      "input    :  was i snoring ?\n",
      "label    :  etais je en train de ronfler ? EOS\n",
      "predicted:  etais je en train de ronfler ? EOS\n",
      "Eval Score (BLEU): 1.530e-01\n",
      "Epoch: 26, Loss: 0.1278\n",
      "input    :  go help tom .\n",
      "label    :  allez aider tom . EOS\n",
      "predicted:  allez aider tom . EOS\n",
      "Eval Score (BLEU): 1.538e-01\n",
      "Epoch: 27, Loss: 0.1373\n",
      "input    :  what if i say no ?\n",
      "label    :  et si je dis non ? EOS\n",
      "predicted:  et si je dis non ? EOS\n",
      "Eval Score (BLEU): 1.548e-01\n",
      "Epoch: 28, Loss: 0.1337\n",
      "input    :  i m timid .\n",
      "label    :  je suis timide . EOS\n",
      "predicted:  je suis timide . EOS\n",
      "Eval Score (BLEU): 1.533e-01\n",
      "Epoch: 29, Loss: 0.1406\n",
      "input    :  stop overreacting .\n",
      "label    :  cessez de reagir de facon excessive . EOS\n",
      "predicted:  cessez d exagerer . EOS\n",
      "Eval Score (BLEU): 1.537e-01\n",
      "Epoch: 30, Loss: 0.1223\n",
      "input    :  meet me there .\n",
      "label    :  rencontrez moi la bas . EOS\n",
      "predicted:  rencontrez moi la bas . EOS\n",
      "Eval Score (BLEU): 1.541e-01\n",
      "CPU times: user 2min 13s, sys: 4.39 s, total: 2min 17s\n",
      "Wall time: 4min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = NUM_EPOCHS\n",
    "eval_scores = []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    encoder_state = encoder.init_state(batch_size)\n",
    "\n",
    "    for batch, data in enumerate(train_dataset):\n",
    "        encoder_in, decoder_in, decoder_out = data\n",
    "        # print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\n",
    "        loss = train_step(\n",
    "            encoder_in, decoder_in, decoder_out, encoder_state)\n",
    "    \n",
    "    print(\"Epoch: {}, Loss: {:.4f}\".format(e + 1, loss.numpy()))\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    predict(encoder, decoder, batch_size, sents_en, data_en,\n",
    "        sents_fr_out, word2idx_fr, idx2word_fr)\n",
    "\n",
    "    eval_score = evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_fr, idx2word_fr)\n",
    "    print(\"Eval Score (BLEU): {:.3e}\".format(eval_score))\n",
    "    # eval_scores.append(eval_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0ed30bf4-7a45-4067-8b27-fea76fb4f160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/checkpoints/ckpt-4'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a7ace2-dbfa-4896-952a-1402d78323e9",
   "metadata": {},
   "source": [
    "## Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd61e01-ff6b-432b-bd7f-4ad84e75c5ff",
   "metadata": {},
   "source": [
    "### Example ‒ seq2seq with attention for machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cb968c-e8a1-4d36-96d3-804405e70e7f",
   "metadata": {},
   "source": [
    "(code source: Chapter_5/seq2seq_with_attn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9d7f18f8-bf56-4260-8bb1-e7edb3a81625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import unicodedata\n",
    "import zipfile\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "82a9e17a-f0fd-4458-a0f3-36d1d6925f98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NUM_SENT_PAIRS = 100\n",
    "# EMBEDDING_DIM = 32\n",
    "# ENCODER_DIM, DECODER_DIM = 64, 64\n",
    "# BATCH_SIZE = 8\n",
    "# NUM_EPOCHS = 3\n",
    "\n",
    "NUM_SENT_PAIRS = 30000\n",
    "EMBEDDING_DIM = 256\n",
    "ENCODER_DIM, DECODER_DIM = 1024, 1024\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9921d0d9-ec90-40d6-8174-461f2a9d1f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_up_logs(data_dir):\n",
    "    checkpoint_dir = os.path.join(data_dir, \"checkpoints\")\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        shutil.rmtree(checkpoint_dir, ignore_errors=True)\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    return checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0cd90363-a595-424d-b41a-be94cb48c48a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "checkpoint_dir = clean_up_logs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "04209a3d-7eaa-46a4-add0-c84aa23719cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test code for attention classes\n",
    "# batch_size = BATCH_SIZE\n",
    "# num_timesteps = MAXLEN_EN\n",
    "# num_units = ENCODER_DIM\n",
    "\n",
    "# query = np.random.random(size=(batch_size, num_units))\n",
    "# values = np.random.random(size=(batch_size, num_timesteps, num_units))\n",
    "\n",
    "# # check out dimensions for Bahdanau attention\n",
    "# b_attn = BahdanauAttention(num_units)\n",
    "# context, alignments = b_attn(query, values)\n",
    "# print(\"Bahdanau: context.shape:\", context.shape, \"alignments.shape:\", alignments.shape)\n",
    "# # Bahdanau: context.shape: (64, 1024) alignments.shape: (64, 8, 1)\n",
    "\n",
    "# # check out dimensions for Luong attention\n",
    "# l_attn = LuongAttention(num_units)\n",
    "# context, alignments = l_attn(query, values)\n",
    "# print(\"Luong: context.shape:\", context.shape, \"alignments.shape:\", alignments.shape)\n",
    "# # Luong: context.shape: (64, 1024) alignments.shape: (64, 8, 1)\n",
    "# End test code for attention classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e988e4bb-bc5c-4dbc-81cc-bcb38de0dd35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent) \n",
    "        if unicodedata.category(c) != \"Mn\"])\n",
    "    sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    sent = sent.lower()\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "370e27f5-ef5e-42e6-a106-ac4622f74938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# original version ... not gonna use ... instead use the next version ..\n",
    "def download_and_read(url, num_sent_pairs=30000):\n",
    "    local_file = url.split('/')[-1]\n",
    "    if not os.path.exists(local_file):\n",
    "        os.system(\"wget -O {:s} {:s}\".format(local_file, url))\n",
    "        with zipfile.ZipFile(local_file, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "    local_file = os.path.join(\".\", \"fra.txt\")\n",
    "    en_sents, fr_sents_in, fr_sents_out = [], [], []\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "            en_sent, fr_sent = line.strip().split('\\t')\n",
    "            en_sent = [w for w in preprocess_sentence(en_sent).split()]\n",
    "            fr_sent = preprocess_sentence(fr_sent)\n",
    "            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\n",
    "            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\n",
    "            en_sents.append(en_sent)\n",
    "            fr_sents_in.append(fr_sent_in)\n",
    "            fr_sents_out.append(fr_sent_out)\n",
    "            if i >= num_sent_pairs - 1:\n",
    "                break\n",
    "    return en_sents, fr_sents_in, fr_sents_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b0bc674a-6608-41d2-810e-434d55fb9ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_read(url, num_sent_pairs=30000):\n",
    "    # local_file = url.split('/')[-1]\n",
    "    # if not os.path.exists(local_file):\n",
    "    #     os.system(\"wget -O {:s} {:s}\".format(local_file, url))\n",
    "    #     with zipfile.ZipFile(local_file, \"r\") as zip_ref:\n",
    "    #         zip_ref.extractall(\".\")\n",
    "    # local_file = os.path.join(\".\", \"fra.txt\")\n",
    "    local_file = os.path.join(\"datasets\", \"fra.txt\")\n",
    "    en_sents, fr_sents_in, fr_sents_out = [], [], []\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "            en_sent, fr_sent, _ = line.strip().split('\\t')\n",
    "            en_sent = [w for w in preprocess_sentence(en_sent).split()]\n",
    "            fr_sent = preprocess_sentence(fr_sent)\n",
    "            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\n",
    "            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\n",
    "            en_sents.append(en_sent)\n",
    "            fr_sents_in.append(fr_sent_in)\n",
    "            fr_sents_out.append(fr_sent_out)\n",
    "            if i >= NUM_SENT_PAIRS - 1:\n",
    "                break\n",
    "    return en_sents, fr_sents_in, fr_sents_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "08f5fa6d-d3e8-4ef7-b597-0a1140bcf8df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data preparation\n",
    "download_url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "sents_en, sents_fr_in, sents_fr_out = download_and_read(download_url, num_sent_pairs=NUM_SENT_PAIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c3bd0380-1365-4eab-9bac-78a8f0b3491b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en)\n",
    "data_en = tokenizer_en.texts_to_sequences(sents_en)\n",
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fdf57a51-982a-4400-849d-a433215a68cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters=\"\", lower=False)\n",
    "tokenizer_fr.fit_on_texts(sents_fr_in)\n",
    "tokenizer_fr.fit_on_texts(sents_fr_out)\n",
    "data_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\n",
    "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding=\"post\")\n",
    "data_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\n",
    "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cd17ddc8-b65b-4900-9161-bc0829c00c23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqlen (en): 7, (fr): 16\n"
     ]
    }
   ],
   "source": [
    "maxlen_en = data_en.shape[1]\n",
    "maxlen_fr = data_fr_out.shape[1]\n",
    "print(\"seqlen (en): {:d}, (fr): {:d}\".format(maxlen_en, maxlen_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d8c2bd8c-318a-4121-bb72-9e506a8e00e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data_en, data_fr_in, data_fr_out))\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = NUM_SENT_PAIRS // 4\n",
    "test_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True)\n",
    "train_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fc80c138-9354-40f9-8f97-a5197c28ad5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size (en): 4366, vocab size (fr): 7665\n"
     ]
    }
   ],
   "source": [
    "vocab_size_en = len(tokenizer_en.word_index)\n",
    "vocab_size_fr = len(tokenizer_fr.word_index)\n",
    "word2idx_en = tokenizer_en.word_index\n",
    "idx2word_en = {v:k for k, v in word2idx_en.items()}\n",
    "word2idx_fr = tokenizer_fr.word_index\n",
    "idx2word_fr = {v:k for k, v in word2idx_fr.items()}\n",
    "print(\"vocab size (en): {:d}, vocab size (fr): {:d}\".format(\n",
    "    vocab_size_en, vocab_size_fr))\n",
    "# vocab size (en): 57, vocab size (fr): 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "73477206-81ac-434c-ae94-b025a64a7fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check encoder/decoder dimensions\n",
    "embedding_dim = EMBEDDING_DIM\n",
    "encoder_dim, decoder_dim = ENCODER_DIM, DECODER_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "34c9e577-f88a-40bc-9a53-9ed365e4509f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, num_timesteps, \n",
    "            embedding_dim, encoder_dim, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, embedding_dim, input_length=num_timesteps)\n",
    "        self.rnn = tf.keras.layers.GRU(\n",
    "            encoder_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, x, state):\n",
    "        x = self.embedding(x)\n",
    "        x, state = self.rnn(x, initial_state=state)\n",
    "        return x, state\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.encoder_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9f3852ac-75c4-4694-b699-79390bb4c206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(num_units)\n",
    "        self.W2 = tf.keras.layers.Dense(num_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query is the decoder state at time step j\n",
    "        # query.shape: (batch_size, num_units)\n",
    "        # values are encoder states at every timestep i\n",
    "        # values.shape: (batch_size, num_timesteps, num_units)\n",
    "\n",
    "        # add time axis to query: (batch_size, 1, num_units)\n",
    "        query_with_time_axis = tf.expand_dims(query, axis=1)\n",
    "        # compute score:\n",
    "        score = self.V(tf.keras.activations.tanh(\n",
    "            self.W1(values) + self.W2(query_with_time_axis)))\n",
    "        # compute softmax\n",
    "        alignment = tf.nn.softmax(score, axis=1)\n",
    "        # compute attended output\n",
    "        context = tf.reduce_sum(\n",
    "            tf.linalg.matmul(\n",
    "                tf.linalg.matrix_transpose(alignment),\n",
    "                values\n",
    "            ), axis=1\n",
    "        )\n",
    "        context = tf.expand_dims(context, axis=1)\n",
    "        return context, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ef1e1c9b-b7e6-4f85-a14e-c67b6dae1679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W = tf.keras.layers.Dense(num_units)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # add time axis to query\n",
    "        query_with_time_axis = tf.expand_dims(query, axis=1)\n",
    "        # compute score\n",
    "        score = tf.linalg.matmul(\n",
    "            query_with_time_axis, self.W(values), transpose_b=True)\n",
    "        # compute softmax\n",
    "        alignment = tf.nn.softmax(score, axis=2)\n",
    "        # compute attended output\n",
    "        context = tf.matmul(alignment, values)\n",
    "        return context, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "421dce0c-99ac-41a5-8898-a0f6a4484e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_timesteps,\n",
    "            decoder_dim, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.decoder_dim = decoder_dim\n",
    "\n",
    "        # self.attention = LuongAttention(embedding_dim)\n",
    "        self.attention = BahdanauAttention(embedding_dim)\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, embedding_dim, input_length=num_timesteps)\n",
    "        self.rnn = tf.keras.layers.GRU(\n",
    "            decoder_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "        self.Wc = tf.keras.layers.Dense(decoder_dim, activation=\"tanh\")\n",
    "        self.Ws = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, state, encoder_out):\n",
    "        x = self.embedding(x)\n",
    "        context, alignment = self.attention(x, encoder_out)\n",
    "        x = tf.expand_dims(\n",
    "                tf.concat([\n",
    "                    x, tf.squeeze(context, axis=1)\n",
    "                ], axis=1), \n",
    "            axis=1)\n",
    "        x, state = self.rnn(x, state)\n",
    "        x = self.Wc(x)\n",
    "        x = self.Ws(x)\n",
    "        return x, state, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "be4c3ea0-8adb-40c7-b314-d313038f660f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(ytrue, ypred):\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = scce(ytrue, ypred, sample_weight=mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "41939ad6-60c6-43da-a222-5f4150066486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(decoder_out.shape[1]):\n",
    "            decoder_in_t = decoder_in[:, t]\n",
    "            decoder_pred_t, decoder_state, _ = decoder(decoder_in_t,\n",
    "                decoder_state, encoder_out)\n",
    "            loss += loss_fn(decoder_out[:, t], decoder_pred_t)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss / decoder_out.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b6331fe8-b1e5-4355-8822-9528c6a06989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu_score(encoder, decoder, test_dataset, \n",
    "        word2idx_fr, idx2word_fr):\n",
    "\n",
    "    bleu_scores = []\n",
    "    smooth_fn = SmoothingFunction()\n",
    "\n",
    "    for encoder_in, decoder_in, decoder_out in test_dataset:\n",
    "        encoder_state = encoder.init_state(batch_size)\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "\n",
    "        ref_sent_ids = np.zeros_like(decoder_out)\n",
    "        hyp_sent_ids = np.zeros_like(decoder_out)\n",
    "        for t in range(decoder_out.shape[1]):\n",
    "            decoder_out_t = decoder_out[:, t]\n",
    "            decoder_in_t = decoder_in[:, t]\n",
    "            decoder_pred_t, decoder_state, _ = decoder(\n",
    "                decoder_in_t, decoder_state, encoder_out)\n",
    "            decoder_pred_t = tf.argmax(decoder_pred_t, axis=-1)\n",
    "            for b in range(decoder_pred_t.shape[0]):\n",
    "                ref_sent_ids[b, t] = decoder_out_t.numpy()[0]\n",
    "                hyp_sent_ids[b, t] = decoder_pred_t.numpy()[0][0]\n",
    "        \n",
    "        for b in range(ref_sent_ids.shape[0]):\n",
    "            ref_sent = [idx2word_fr[i] for i in ref_sent_ids[b] if i > 0]\n",
    "            hyp_sent = [idx2word_fr[i] for i in hyp_sent_ids[b] if i > 0]\n",
    "            # remove trailing EOS\n",
    "            ref_sent = ref_sent[0:-1]\n",
    "            hyp_sent = hyp_sent[0:-1]\n",
    "            bleu_score = sentence_bleu([ref_sent], hyp_sent,\n",
    "                smoothing_function=smooth_fn.method1)\n",
    "            bleu_scores.append(bleu_score)\n",
    "\n",
    "    return np.mean(np.array(bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "537591d7-3596-43aa-8ae2-9795a48656d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder, decoder, batch_size, \n",
    "        sents_en, data_en, sents_fr_out, \n",
    "        word2idx_fr, idx2word_fr):\n",
    "    random_id = np.random.choice(len(sents_en))\n",
    "    print(\"input    : \",  \" \".join(sents_en[random_id]))\n",
    "    print(\"label    : \", \" \".join(sents_fr_out[random_id]))\n",
    "\n",
    "    encoder_in = tf.expand_dims(data_en[random_id], axis=0)\n",
    "    decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\n",
    "\n",
    "    encoder_state = encoder.init_state(1)\n",
    "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "    decoder_state = encoder_state\n",
    "\n",
    "    pred_sent_fr = []\n",
    "    decoder_in = tf.expand_dims(\n",
    "        tf.constant(word2idx_fr[\"BOS\"]), axis=0)\n",
    "\n",
    "    while True:\n",
    "        decoder_pred, decoder_state, _ = decoder(\n",
    "            decoder_in, decoder_state, encoder_out)\n",
    "        decoder_pred = tf.argmax(decoder_pred, axis=-1)\n",
    "        pred_word = idx2word_fr[decoder_pred.numpy()[0][0]]\n",
    "        pred_sent_fr.append(pred_word)\n",
    "        if pred_word == \"EOS\":\n",
    "            break\n",
    "        decoder_in = tf.squeeze(decoder_pred, axis=1)\n",
    "\n",
    "    print(\"predicted: \", \" \".join(pred_sent_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cf4d4217-727e-4117-85a7-8c99e7255c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size_en+1, embedding_dim, maxlen_en, encoder_dim)\n",
    "decoder = Decoder(vocab_size_fr+1, embedding_dim, maxlen_fr, decoder_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2b7f14c0-625d-4e58-aef1-c6a3b4191374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Test code for encoder and decoder with attention\n",
    "# for encoder_in, decoder_in, decoder_out in train_dataset:\n",
    "#     print(\"inputs:\", encoder_in.shape, decoder_in.shape, decoder_out.shape)\n",
    "#     encoder_state = encoder.init_state(batch_size)\n",
    "#     encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "#     decoder_state = encoder_state\n",
    "#     decoder_pred = []\n",
    "#     for t in range(decoder_out.shape[1]):\n",
    "#         decoder_in_t = decoder_in[:, t]\n",
    "#         decoder_pred_t, decoder_state, _ = decoder(decoder_in_t,\n",
    "#             decoder_state, encoder_out)\n",
    "#         decoder_pred.append(decoder_pred_t.numpy())\n",
    "#     decoder_pred = tf.squeeze(np.array(decoder_pred), axis=2)\n",
    "#     break\n",
    "# print(\"encoder input          :\", encoder_in.shape)\n",
    "# print(\"encoder output         :\", encoder_out.shape, \"state:\", encoder_state.shape)\n",
    "# print(\"decoder output (logits):\", decoder_pred.shape, \"state:\", decoder_state.shape)\n",
    "# print(\"decoder output (labels):\", decoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fa52c6b8-76dd-4232-9490-0ebc09e48970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bahdanau:\n",
    "# encoder input          : (64, 8)\n",
    "# encoder output         : (64, 8, 1024) state: (64, 1024)\n",
    "# decoder output (logits): (8, 64, 7658) state: (64, 1024)\n",
    "# decoder output (labels): (64, 16)\n",
    "# \n",
    "# Luong:\n",
    "# encoder input          : (64, 8)\n",
    "# encoder output         : (64, 8, 1024) state: (64, 1024)\n",
    "# decoder output (logits): (8, 64, 7658) state: (64, 1024)\n",
    "# decoder output (labels): (64, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d4da289a-ffdb-485a-aac0-8d0d238d1342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c64eef4a-2ceb-494a-b5c4-e08e1267fdac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.2397\n",
      "input    :  now i understand .\n",
      "label    :  maintenant je comprends . EOS\n",
      "predicted:  je suis en aller . EOS\n",
      "Eval Score (BLEU): 2.170e-02\n",
      "Epoch: 2, Loss: 1.0483\n",
      "input    :  can i do anything ?\n",
      "label    :  y a t il quelque chose que je puisse faire ? EOS\n",
      "predicted:  puis je faire ? EOS\n",
      "Eval Score (BLEU): 2.493e-02\n",
      "Epoch: 3, Loss: 0.7820\n",
      "input    :  she needs you .\n",
      "label    :  elle a besoin de toi . EOS\n",
      "predicted:  nous sommes desormais . EOS\n",
      "Eval Score (BLEU): 3.712e-02\n",
      "Epoch: 4, Loss: 0.6431\n",
      "input    :  merry christmas !\n",
      "label    :  joyeux noel ! EOS\n",
      "predicted:  la police ! EOS\n",
      "Eval Score (BLEU): 3.528e-02\n",
      "Epoch: 5, Loss: 0.5511\n",
      "input    :  she went out .\n",
      "label    :  elle sortit . EOS\n",
      "predicted:  elle est allee en train de danser . EOS\n",
      "Eval Score (BLEU): 4.079e-02\n",
      "Epoch: 6, Loss: 0.4753\n",
      "input    :  they want more .\n",
      "label    :  elles veulent davantage . EOS\n",
      "predicted:  ils veulent davantage . EOS\n",
      "Eval Score (BLEU): 6.315e-02\n",
      "Epoch: 7, Loss: 0.3988\n",
      "input    :  hi .\n",
      "label    :  salut ! EOS\n",
      "predicted:  prends de precautions . EOS\n",
      "Eval Score (BLEU): 7.047e-02\n",
      "Epoch: 8, Loss: 0.3915\n",
      "input    :  no one is coming .\n",
      "label    :  personne ne vient . EOS\n",
      "predicted:  personne ne se moque . EOS\n",
      "Eval Score (BLEU): 7.546e-02\n",
      "Epoch: 9, Loss: 0.2833\n",
      "input    :  he swindled her .\n",
      "label    :  il l a escroquee . EOS\n",
      "predicted:  il l a escroquee . EOS\n",
      "Eval Score (BLEU): 7.509e-02\n",
      "Epoch: 10, Loss: 0.2845\n",
      "input    :  this is awesome .\n",
      "label    :  c est super ! EOS\n",
      "predicted:  c est genial ! EOS\n",
      "Eval Score (BLEU): 9.612e-02\n",
      "Epoch: 11, Loss: 0.2546\n",
      "input    :  time crept on .\n",
      "label    :  le temps s ecoulait . EOS\n",
      "predicted:  l hiver approche . EOS\n",
      "Eval Score (BLEU): 1.039e-01\n",
      "Epoch: 12, Loss: 0.2273\n",
      "input    :  we agreed .\n",
      "label    :  nous sommes tombes d accord . EOS\n",
      "predicted:  nous avons ete tombes d accord . EOS\n",
      "Eval Score (BLEU): 9.780e-02\n",
      "Epoch: 13, Loss: 0.2364\n",
      "input    :  choose something .\n",
      "label    :  choisissez quelque chose . EOS\n",
      "predicted:  choisis quelque chose . EOS\n",
      "Eval Score (BLEU): 1.116e-01\n",
      "Epoch: 14, Loss: 0.2144\n",
      "input    :  that s peculiar .\n",
      "label    :  c est bizarre . EOS\n",
      "predicted:  c est etrange . EOS\n",
      "Eval Score (BLEU): 1.045e-01\n",
      "Epoch: 15, Loss: 0.1943\n",
      "input    :  i want that one .\n",
      "label    :  je veux celle la . EOS\n",
      "predicted:  je veux celle la . EOS\n",
      "Eval Score (BLEU): 1.096e-01\n",
      "Epoch: 16, Loss: 0.2133\n",
      "input    :  who kissed you ?\n",
      "label    :  qui vous a embrasses ? EOS\n",
      "predicted:  qui t a embrasse ? EOS\n",
      "Eval Score (BLEU): 1.385e-01\n",
      "Epoch: 17, Loss: 0.1625\n",
      "input    :  children need love .\n",
      "label    :  les enfants ont besoin d amour . EOS\n",
      "predicted:  les enfants ont besoin d amour . EOS\n",
      "Eval Score (BLEU): 1.200e-01\n",
      "Epoch: 18, Loss: 0.1476\n",
      "input    :  i wasn t sleeping .\n",
      "label    :  je n etais pas en train de dormir . EOS\n",
      "predicted:  je n etais pas en train de dormir . EOS\n",
      "Eval Score (BLEU): 1.082e-01\n",
      "Epoch: 19, Loss: 0.1407\n",
      "input    :  listen carefully .\n",
      "label    :  ecoutez attentivement ! EOS\n",
      "predicted:  ecoute moi bien ! EOS\n",
      "Eval Score (BLEU): 1.151e-01\n",
      "Epoch: 20, Loss: 0.1781\n",
      "input    :  they caught tom .\n",
      "label    :  elles ont attrape tom . EOS\n",
      "predicted:  ils ont jete un coup d il a l interieur . EOS\n",
      "Eval Score (BLEU): 1.252e-01\n",
      "Epoch: 21, Loss: 0.1477\n",
      "input    :  don t make faces .\n",
      "label    :  ne fais pas de grimaces ! EOS\n",
      "predicted:  ne fais pas de grimaces ! EOS\n",
      "Eval Score (BLEU): 1.288e-01\n",
      "Epoch: 22, Loss: 0.1667\n",
      "input    :  tom isn t married .\n",
      "label    :  tom n est pas marie . EOS\n",
      "predicted:  tom n est pas marie . EOS\n",
      "Eval Score (BLEU): 1.268e-01\n",
      "Epoch: 23, Loss: 0.1482\n",
      "input    :  he helps her .\n",
      "label    :  il l aide . EOS\n",
      "predicted:  il l aide . EOS\n",
      "Eval Score (BLEU): 1.409e-01\n",
      "Epoch: 24, Loss: 0.1554\n",
      "input    :  i freaked out .\n",
      "label    :  j ai eu la trouille . EOS\n",
      "predicted:  j ai eu les foies . EOS\n",
      "Eval Score (BLEU): 1.486e-01\n",
      "Epoch: 25, Loss: 0.1125\n",
      "input    :  tom sat nearby .\n",
      "label    :  tom etait assis a proximite . EOS\n",
      "predicted:  tom etait assis tout pres . EOS\n",
      "Eval Score (BLEU): 1.148e-01\n",
      "Epoch: 26, Loss: 0.1587\n",
      "input    :  it s a shame .\n",
      "label    :  c est dommage . EOS\n",
      "predicted:  c est honteux . EOS\n",
      "Eval Score (BLEU): 1.444e-01\n",
      "Epoch: 27, Loss: 0.1423\n",
      "input    :  i know the feeling .\n",
      "label    :  je connais ce sentiment . EOS\n",
      "predicted:  je connais cette sensation . EOS\n",
      "Eval Score (BLEU): 1.327e-01\n",
      "Epoch: 28, Loss: 0.1586\n",
      "input    :  were you excited ?\n",
      "label    :  etiez vous excite ? EOS\n",
      "predicted:  etiez vous excitees ? EOS\n",
      "Eval Score (BLEU): 1.375e-01\n",
      "Epoch: 29, Loss: 0.1589\n",
      "input    :  they re fine .\n",
      "label    :  ils vont bien . EOS\n",
      "predicted:  ils vont bien . EOS\n",
      "Eval Score (BLEU): 1.446e-01\n",
      "Epoch: 30, Loss: 0.1682\n",
      "input    :  i want the same .\n",
      "label    :  je veux la meme chose . EOS\n",
      "predicted:  je veux la meme chose . EOS\n",
      "Eval Score (BLEU): 1.340e-01\n",
      "CPU times: user 16min 33s, sys: 1min 15s, total: 17min 49s\n",
      "Wall time: 19min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = NUM_EPOCHS\n",
    "eval_scores = []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    encoder_state = encoder.init_state(batch_size)\n",
    "\n",
    "    for batch, data in enumerate(train_dataset):\n",
    "        encoder_in, decoder_in, decoder_out = data\n",
    "        # print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\n",
    "        loss = train_step(\n",
    "            encoder_in, decoder_in, decoder_out, encoder_state)\n",
    "    \n",
    "    print(\"Epoch: {}, Loss: {:.4f}\".format(e + 1, loss.numpy()))\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    predict(encoder, decoder, batch_size, sents_en, data_en,\n",
    "        sents_fr_out, word2idx_fr, idx2word_fr)\n",
    "\n",
    "    eval_score = evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_fr, idx2word_fr)\n",
    "    print(\"Eval Score (BLEU): {:.3e}\".format(eval_score))\n",
    "    # eval_scores.append(eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "686b99d0-2b9c-4225-afa2-c60933cfcb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/checkpoints/ckpt-4'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9f0e3f62-4007-4301-8183-26fb7bd3cfc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Run Date: Wednesday, February 15, 2023\n",
      "# Run Time: 00:26:11\n"
     ]
    }
   ],
   "source": [
    "endTime = time.time()\n",
    "\n",
    "elapsedTime = time.strftime(\"%H:%M:%S\", time.gmtime(endTime - startTime))\n",
    "\n",
    "print(todaysDate.strftime('# Run Date: %A, %B %d, %Y'))\n",
    "print(f\"# Run Time: {elapsedTime}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
