{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b230e3-b365-4613-bada-8e8bc2f262cd",
   "metadata": {},
   "source": [
    "This notebook was constructed from the python code samples found in the Chapter_4 folder of the repo\n",
    "\n",
    "https://github.com/PacktPublishing/Deep-Learning-with-TensorFlow-and-Keras-3rd-edition\n",
    "\n",
    "Numerous tweaks were needed to get the code to run without errors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f0277-19a8-4ea5-93b7-c4144e62934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All runs without errors. \n",
    "# Run time is quick because all of the needed data and models have already been downloaded.\n",
    "\n",
    "# Run Date: Tuesday, February 14, 2023\n",
    "# Run Time: 00:01:26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd157663-2000-4d92-87eb-8463a03cd68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import date\n",
    "\n",
    "startTime = time.time()\n",
    "todaysDate = date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0a6a69-92ba-4172-b8b7-7d42e8f1aac7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create your own embeddings using Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b76fa00-745e-41af-8b40-b550451abb73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf2f88-73b0-4541-959e-b1769751947c",
   "metadata": {},
   "source": [
    "(Code source: Chapter_4/create_embedding_with_text8.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd53dab0-808f-41d4-95f8-0502a75a26e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6755cda-3495-49ba-897f-6bd02203ea68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "464cfb71-8e6b-4164-b41a-9d9c187657b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "info = api.info(\"text8\")\n",
    "assert(len(info) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd11e0b1-99a6-4e0c-b6f5-5eba709a31f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.86 ms, sys: 1.2 ms, total: 3.06 ms\n",
      "Wall time: 78.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = api.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a24b069d-adf6-4fa3-9f04-93335003e7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 42s, sys: 356 ms, total: 1min 42s\n",
      "Wall time: 34.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bde224d-a410-40f9-bd6c-f3c823c44592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(\"data/text8-word2vec.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c3501f-3f17-4e8e-914b-02c8ad6de993",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploring the embedding space with Gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541994eb-7815-4ee3-a0a3-fc9f7dc8c4d5",
   "metadata": {},
   "source": [
    "(Code source: Chapter_4/explore_text8_embedding.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16926d94-dc13-418d-b204-aba3950955fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cebca5bd-0a29-4a03-bae5-675e9bd75dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load(\"data/text8-word2vec.bin\")\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef2eb1-a3c3-41f6-af98-117a111c7f0b",
   "metadata": {},
   "source": [
    "We can take a look at the first few words in the vocabulary and check to see if specific words are\n",
    "available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d850cd8-8dda-4729-9b16-1597867f4226",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mmi'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get words in the vocabulary ... this next line throws this error ... \n",
    "# words = word_vectors.vocab.keys()\n",
    "\n",
    "# AttributeError: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\n",
    "# Use KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\n",
    "# See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\n",
    "import random\n",
    "random_word = random.choice(model.wv.index_to_key)\n",
    "random_word\n",
    "\n",
    "# these next 2 lines will also not work \n",
    "# print([x for i, x in enumerate(words) if i < 10])\n",
    "# assert(\"king\" in words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bf89825-5c5b-40b7-9c28-f387e339f8ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579\n",
      "2819\n",
      "71290\n"
     ]
    }
   ],
   "source": [
    "rock_idx = model.wv.key_to_index[\"rock\"]  \n",
    "rock_cnt = model.wv.get_vecattr(\"rock\", \"count\")  \n",
    "vocab_len = len(model.wv) \n",
    "print(rock_idx)\n",
    "print(rock_cnt)\n",
    "print(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a70d27a9-f9e6-4322-9a11-8d12222c9b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero', 'nine', 'two']\n"
     ]
    }
   ],
   "source": [
    "# print the first 10 words of the model\n",
    "print([x for i, x in enumerate(model.wv.index_to_key) if i < 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48f95275-f4ea-47b8-8f28-5c419e0095d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_most_similar(word_conf_pairs, k):\n",
    "    for i, (word, conf) in enumerate(word_conf_pairs):\n",
    "        print(\"{:.3f} {:s}\".format(conf, word))\n",
    "        if i >= k-1:\n",
    "            break\n",
    "    if k < len(word_conf_pairs):\n",
    "        print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "307d318e-b819-4c37-b146-c3ee274fc18c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.7539743781089783),\n",
       " ('queen', 0.730122447013855),\n",
       " ('throne', 0.7136245369911194),\n",
       " ('emperor', 0.7093201875686646),\n",
       " ('kings', 0.6822531223297119),\n",
       " ('pharaoh', 0.6782265901565552),\n",
       " ('regent', 0.6712974309921265),\n",
       " ('elector', 0.6605132818222046),\n",
       " ('vii', 0.6580088138580322),\n",
       " ('herod', 0.6558883190155029)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c822218-db33-4824-a5cd-378d5f3a7dae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words similar to king\n",
      "0.754 prince\n",
      "0.730 queen\n",
      "0.714 throne\n",
      "0.709 emperor\n",
      "0.682 kings\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(\"# words similar to king\")\n",
    "print_most_similar(word_vectors.most_similar(\"king\"), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f23e56f-0cd2-477b-9280-7d5423a9cf55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# vector arithmetic with words (cosine similarity)\n",
      "# france + berlin - paris = ?\n",
      "0.773 germany\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(\"# vector arithmetic with words (cosine similarity)\")\n",
    "print(\"# france + berlin - paris = ?\")\n",
    "print_most_similar(word_vectors.most_similar(\n",
    "    positive=[\"france\", \"berlin\"], negative=[\"paris\"]), 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e409b9f-7556-4440-be30-014b1373af7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# vector arithmetic with words (Levy and Goldberg)\n",
      "# france + berlin - paris = ?\n",
      "0.944 germany\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(\"# vector arithmetic with words (Levy and Goldberg)\")\n",
    "print(\"# france + berlin - paris = ?\")\n",
    "print_most_similar(word_vectors.most_similar_cosmul(\n",
    "    positive=[\"france\", \"berlin\"], negative=[\"paris\"]), 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35a3c947-dd90-4d5d-a73a-654d3b63288a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# find odd one out\n",
      "# [hindus, parsis, singapore, christians]\n",
      "singapore\n"
     ]
    }
   ],
   "source": [
    "print(\"# find odd one out\")\n",
    "print(\"# [hindus, parsis, singapore, christians]\")\n",
    "print(word_vectors.doesnt_match([\"hindus\", \"parsis\", \n",
    "    \"singapore\", \"christians\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03a17c89-4652-4d0f-a90d-2221db435a65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# similarity between words\n",
      "similarity(man, woman) = 0.739\n",
      "similarity(man, dog) = 0.441\n",
      "similarity(man, whale) = 0.280\n",
      "similarity(man, tree) = 0.262\n"
     ]
    }
   ],
   "source": [
    "print(\"# similarity between words\")\n",
    "for word in [\"woman\", \"dog\", \"whale\", \"tree\"]:\n",
    "    print(\"similarity({:s}, {:s}) = {:.3f}\".format(\n",
    "        \"man\", word,\n",
    "        word_vectors.similarity(\"man\", word)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7578b444-1407-4af1-b8fe-c574915ca5b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# similar by word\n",
      "0.867 malaysia\n",
      "0.846 indonesia\n",
      "0.809 thailand\n",
      "0.806 zambia\n",
      "0.804 philippines\n",
      "...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"# similar by word\")\n",
    "print(print_most_similar(\n",
    "    word_vectors.similar_by_word(\"singapore\"), 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfa7f940-4db5-43e3-8512-342a61d12a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# distance between vectors\n",
      "distance(singapore, malaysia) = 0.133\n"
     ]
    }
   ],
   "source": [
    "print(\"# distance between vectors\")\n",
    "print(\"distance(singapore, malaysia) = {:.3f}\".format(\n",
    "    word_vectors.distance(\"singapore\", \"malaysia\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd86faee-a290-43fd-9f20-be7d5fceb198",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# output vector obtained directly, shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "vec_song = word_vectors[\"song\"]\n",
    "print(\"\\n# output vector obtained directly, shape:\", vec_song.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5db06735-dfaf-47ab-bb3a-aad5e48c45b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# output vector obtained using word_vec, shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "# this next line throws an error ....\n",
    "#vec_song_2 = word_vectors.word_vec(\"song\", use_norm=True)\n",
    "# TypeError: get_vector() got an unexpected keyword argument 'use_norm'\n",
    "\n",
    "# Removing 'use_norm=True)' generates another warning ...\n",
    "# vec_song_2 = word_vectors.word_vec(\"song\")\n",
    "# DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
    "# vec_song_2 = word_vectors.word_vec(\"song\")\n",
    "\n",
    "# This is the correct implementation of this ...\n",
    "vec_song_2 = word_vectors.get_vector(\"song\")\n",
    "print(\"# output vector obtained using word_vec, shape:\", vec_song_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a66a76a-8635-4df6-a409-d0f0113eadd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using word embeddings for spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec1d293-bd68-4879-80d0-88301f52964f",
   "metadata": {},
   "source": [
    "(Code source: Chapter_4/spam_classifier.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5d5beb-fbfa-4aa7-8649-db22e511577e",
   "metadata": {},
   "source": [
    "(The code in the above file differs from the code shown in the book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efde8cab-7ceb-4c60-bcab-a1a72f0d89fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 17:33:52.129218: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-14 17:33:52.240194: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91bd6f-aaef-4068-8f7b-3c7d3901c7d4",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac70e4ea-cb44-4a89-971e-e8fb2362cb98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    p = tf.keras.utils.get_file(local_file, url, \n",
    "        extract=True, cache_dir=\".\")\n",
    "    labels, texts = [], []\n",
    "    local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split('\\t')\n",
    "            labels.append(1 if label == \"spam\" else 0)\n",
    "            texts.append(text)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0152e4e7-cb6a-463f-9087-475d159aba80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\n",
    "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "791829f9-fab4-450f-b8ce-1ea8b7a6f651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data distribution is 4827 ham and 747 spam (total 5574), which \n",
    "# works out to approx 87% ham and 13% spam, so we take reciprocals\n",
    "# and this works out to being each spam (1) item as being approximately\n",
    "# 8 times as important as each ham (0) message.\n",
    "CLASS_WEIGHTS = { 0: 1, 1: 8 }\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--mode\", help=\"run mode\",\n",
    "#     choices=[\n",
    "#         \"scratch\",\n",
    "#         \"vectorizer\",\n",
    "#         \"finetuning\"\n",
    "#     ])\n",
    "# args = parser.parse_args()  # This line blows up in a notebook!\n",
    "# run_mode = args.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d689950-f918-4a1e-addb-ad451eedc436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b348a7c-a101-4c28-8229-ddc84b3db304",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--mode'], dest='mode', nargs=None, const=None, default=None, type=None, choices=['scratch', 'vectorizer', 'finetuning'], help='run mode', metavar=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument(\"--mode\", help=\"run mode\",\n",
    "    choices=[\n",
    "        \"scratch\",\n",
    "        \"vectorizer\",\n",
    "        \"finetuning\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b4d1a9c-c585-4dff-bca1-25f0e12eb600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  https://stackoverflow.com/questions/46477770/jupyternotebook-with-args-parser\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "143afed2-5c27-442b-936a-d64d9832eb78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/51039271/how-to-use-argument-parser-in-jupyter-notebook#51043537\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "487a7c75-638d-4c38-b586-f80ad9faf8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_mode = args.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b054f384-8053-4d55-93b7-b25205b9d121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "texts, labels = download_and_read(DATASET_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cbc0bf-fe9d-4ec2-9fb7-62b4d375d387",
   "metadata": {},
   "source": [
    "### Making the data ready for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56b8c389-2e03-41ea-a143-54b7f85236c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574 sentences, max length: 189\n"
     ]
    }
   ],
   "source": [
    "# tokenize and pad text\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n",
    "num_records = len(text_sequences)\n",
    "max_seqlen = len(text_sequences[0])\n",
    "print(\"{:d} sentences, max length: {:d}\".format(num_records, max_seqlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa8bad67-1cf1-4f54-b943-2e8ed037fedf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# labels\n",
    "cat_labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d627cda3-ef4f-48c3-adb7-941c1b275184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 9010\n"
     ]
    }
   ],
   "source": [
    "# vocabulary\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word[0] = \"PAD\"\n",
    "vocab_size = len(word2idx)\n",
    "print(\"vocab size: {:d}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d29f8380-aed2-40c5-9eaf-2068325525e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 17:33:53.661235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-14 17:33:53.661433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-14 17:33:53.663750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-14 17:33:53.663965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-14 17:33:53.664109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-14 17:33:53.664244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-14 17:33:53.664355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2006] Ignoring visible gpu device (device: 1, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1) with core count: 5. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\n",
      "2023-02-14 17:33:53.664911: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-14 17:33:53.665181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-14 17:33:53.665329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-14 17:33:53.665465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-14 17:33:54.073978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-14 17:33:54.074163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-14 17:33:54.074299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-14 17:33:54.074418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6649 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = num_records // 4\n",
    "val_size = (num_records - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "edc0ff50-3dee-4975-bdea-b9a5ac1b6fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c83b5-5b54-438a-b1b6-ec2687f61822",
   "metadata": {},
   "source": [
    "### Building the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e38b734f-9659-4c61-bef8-d67fdd0b3f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(sequences, word2idx, embedding_dim, \n",
    "        embedding_file):\n",
    "    if os.path.exists(embedding_file):\n",
    "        E = np.load(embedding_file)\n",
    "    else:\n",
    "        vocab_size = len(word2idx)\n",
    "        E = np.zeros((vocab_size, embedding_dim))\n",
    "        word_vectors = api.load(EMBEDDING_MODEL)\n",
    "        for word, idx in word2idx.items():\n",
    "            try:\n",
    "                E[idx] = word_vectors.word_vec(word)\n",
    "            except KeyError:   # word not in embedding\n",
    "                pass\n",
    "            # except IndexError: # UNKs are mapped to seq over VOCAB_SIZE as well as 1\n",
    "            #     pass\n",
    "        np.save(embedding_file, E)\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e3faeaf-c4e2-4e04-b534-4bfacebe2a16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix: (9010, 300)\n"
     ]
    }
   ],
   "source": [
    "# embedding\n",
    "E = build_embedding_matrix(text_sequences, word2idx, EMBEDDING_DIM,\n",
    "    EMBEDDING_NUMPY_FILE)\n",
    "print(\"Embedding matrix:\", E.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda64329-8047-4194-8976-a79132c57a48",
   "metadata": {},
   "source": [
    "### Defining the spam classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "674abca9-a192-4e56-9c9f-ce0d3feb693a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpamClassifierModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_sz, embed_sz, input_length,\n",
    "            num_filters, kernel_sz, output_sz, \n",
    "            run_mode, embedding_weights, \n",
    "            **kwargs):\n",
    "        super(SpamClassifierModel, self).__init__(**kwargs)\n",
    "        if run_mode == \"scratch\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                embed_sz,\n",
    "                input_length=input_length,\n",
    "                trainable=True)\n",
    "        elif run_mode == \"vectorizer\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                embed_sz,\n",
    "                input_length=input_length,\n",
    "                weights=[embedding_weights],\n",
    "                trainable=False)\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                embed_sz,\n",
    "                input_length=input_length,\n",
    "                weights=[embedding_weights],\n",
    "                trainable=True)\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "        self.conv = tf.keras.layers.Conv1D(filters=num_filters,\n",
    "            kernel_size=kernel_sz,\n",
    "            activation=\"relu\")\n",
    "        self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
    "        self.dense = tf.keras.layers.Dense(output_sz, \n",
    "            activation=\"softmax\"\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        return self.dense(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3505fc4c-cc07-472f-9e9c-8c242d4c5346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model definition\n",
    "conv_num_filters = 256\n",
    "conv_kernel_size = 3\n",
    "model = SpamClassifierModel(\n",
    "    vocab_size, EMBEDDING_DIM, max_seqlen, \n",
    "    conv_num_filters, conv_kernel_size, NUM_CLASSES,\n",
    "    run_mode, E)\n",
    "# model.build(input_shape=(None, max_seqlen))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bb69d80-3e85-4b09-bf8b-a5a9baca05d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "\n",
    "#     class ResNet(tf.keras.Model):\n",
    "\n",
    "#         def __init__(self, num_classes=1000):\n",
    "#             super(ResNet, self).__init__()\n",
    "#             self.block_1 = ResNetBlock()\n",
    "#             self.block_2 = ResNetBlock()\n",
    "#             self.global_pool = layers.GlobalAveragePooling2D()\n",
    "#             self.classifier = Dense(num_classes)\n",
    "\n",
    "#         def call(self, inputs):\n",
    "#             x = self.block_1(inputs)\n",
    "#             x = self.block_2(x)\n",
    "#             x = self.global_pool(x)\n",
    "#             return self.classifier(x)\n",
    "\n",
    "\n",
    "#     resnet = ResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "892f9516-8011-4d79-a901-198a9769d525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This next lines throws an error ....\n",
    "# model.build(input_shape=(None, max_seqlen))\n",
    "#    NotImplementedError: Unimplemented `tf.keras.Model.call()`: if you intend to create a `Model` with the Functional API, \n",
    "#    please provide `inputs` and `outputs` arguments. Otherwise, subclass `Model` with an overridden `call()` method.\n",
    "model.build(input_shape=(None, max_seqlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e8f2bcd-7995-4616-9914-4c369c7b30d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"spam_classifier_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  2703000   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  multiple                 0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " conv1d (Conv1D)             multiple                  230656    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  multiple                 0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,934,170\n",
      "Trainable params: 2,934,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da99ade5-ce2e-4100-8cdb-21bfd06cbf6b",
   "metadata": {},
   "source": [
    "Finally, we compile the model using the categorical cross entropy loss function and the Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4793a180-9ad8-4fb0-9cd6-7532e0111cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile and train\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1831e1-e10f-4673-b88d-1f1e53894913",
   "metadata": {},
   "source": [
    "### Training and evalutating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2fefd4f9-8525-4c38-bc9c-6ea7e9c373f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 17:33:55.057450: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 2s 14ms/step - loss: 0.4713 - accuracy: 0.8761 - val_loss: 0.0817 - val_accuracy: 0.9766\n",
      "Epoch 2/3\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.2117 - accuracy: 0.9636 - val_loss: 0.0353 - val_accuracy: 0.9896\n",
      "Epoch 3/3\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.0977 - accuracy: 0.9833 - val_loss: 0.0676 - val_accuracy: 0.9818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f60168df0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(train_dataset, epochs=NUM_EPOCHS, \n",
    "    validation_data=val_dataset,\n",
    "    class_weight=CLASS_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00a86d54-de83-4da9-b95a-3477d379c5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate against test set\n",
    "labels, predictions = [], []\n",
    "for Xtest, Ytest in test_dataset:\n",
    "    Ytest_ = model.predict_on_batch(Xtest)\n",
    "    ytest = np.argmax(Ytest, axis=1)\n",
    "    ytest_ = np.argmax(Ytest_, axis=1)\n",
    "    labels.extend(ytest.tolist())\n",
    "    predictions.extend(ytest.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03f5f94c-ab53-4ca1-a190-97e4381aba74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 1.000\n",
      "confusion matrix\n",
      "[[1091    0]\n",
      " [   0  189]]\n"
     ]
    }
   ],
   "source": [
    "print(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e2ede1-e877-4c80-818c-1f1321401233",
   "metadata": {},
   "source": [
    "### Running the spam detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "820a2e32-3b7e-4e04-b966-f7b79d4e3570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spam_classifier --mode [scratch|vectorizer|finetune]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e3447b-1b19-4d8e-ad5a-ad8d591aee85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural embeddings - not just for words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c01467-43d8-490d-830c-2926253db106",
   "metadata": {},
   "source": [
    "### node2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d5216-3f4d-4bd4-ab9b-7ccd411f2af4",
   "metadata": {},
   "source": [
    "(code source: Chapter_4/neurips_papers_node2vec.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9fc277d-656d-4979-ad04-f29c2a341e03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "# from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28ab4cd8-e7b7-4cd6-891a-10354de6f65d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a524b5c-58ee-4b4e-b05c-841e70df4211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "UCI_DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00371/NIPS_1987-2015.csv\"\n",
    "\n",
    "NUM_WALKS_PER_VERTEX = 32\n",
    "MAX_PATH_LENGTH = 40\n",
    "RESTART_PROB = 0.15\n",
    "\n",
    "RANDOM_WALKS_FILE = os.path.join(DATA_DIR, \"random-walks.txt\")\n",
    "W2V_MODEL_FILE = os.path.join(DATA_DIR, \"w2v-neurips-papers.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c9d299a-0139-473c-b06f-8c7c76dd4894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    p = tf.keras.utils.get_file(local_file, url, cache_dir=\".\")\n",
    "    row_ids, col_ids, data = [], [], []\n",
    "    rid = 0\n",
    "    f = open(p, \"r\")\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"\\\"\\\",\"):\n",
    "            # header\n",
    "            continue\n",
    "        if rid % 100 == 0:\n",
    "            print(\"{:d} rows read\".format(rid))\n",
    "        # compute non-zero elements for current row\n",
    "        counts = np.array([int(x) for x in line.split(',')[1:]])\n",
    "        nz_col_ids = np.nonzero(counts)[0]\n",
    "        nz_data = counts[nz_col_ids]\n",
    "        nz_row_ids = np.repeat(rid, len(nz_col_ids))\n",
    "        rid += 1\n",
    "        # add data to big lists\n",
    "        row_ids.extend(nz_row_ids.tolist())\n",
    "        col_ids.extend(nz_col_ids.tolist())\n",
    "        data.extend(nz_data.tolist())\n",
    "    print(\"{:d} rows read, COMPLETE\".format(rid))\n",
    "    f.close()\n",
    "    TD = csr_matrix((\n",
    "        np.array(data), (\n",
    "            np.array(row_ids), np.array(col_ids)\n",
    "            )\n",
    "        ),\n",
    "        shape=(rid, counts.shape[0]))\n",
    "    return TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fcf4b9b7-8cc9-4a5a-9a02-4f6804a390dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows read\n",
      "100 rows read\n",
      "200 rows read\n",
      "300 rows read\n",
      "400 rows read\n",
      "500 rows read\n",
      "600 rows read\n",
      "700 rows read\n",
      "800 rows read\n",
      "900 rows read\n",
      "1000 rows read\n",
      "1100 rows read\n",
      "1200 rows read\n",
      "1300 rows read\n",
      "1400 rows read\n",
      "1500 rows read\n",
      "1600 rows read\n",
      "1700 rows read\n",
      "1800 rows read\n",
      "1900 rows read\n",
      "2000 rows read\n",
      "2100 rows read\n",
      "2200 rows read\n",
      "2300 rows read\n",
      "2400 rows read\n",
      "2500 rows read\n",
      "2600 rows read\n",
      "2700 rows read\n",
      "2800 rows read\n",
      "2900 rows read\n",
      "3000 rows read\n",
      "3100 rows read\n",
      "3200 rows read\n",
      "3300 rows read\n",
      "3400 rows read\n",
      "3500 rows read\n",
      "3600 rows read\n",
      "3700 rows read\n",
      "3800 rows read\n",
      "3900 rows read\n",
      "4000 rows read\n",
      "4100 rows read\n",
      "4200 rows read\n",
      "4300 rows read\n",
      "4400 rows read\n",
      "4500 rows read\n",
      "4600 rows read\n",
      "4700 rows read\n",
      "4800 rows read\n",
      "4900 rows read\n",
      "5000 rows read\n",
      "5100 rows read\n",
      "5200 rows read\n",
      "5300 rows read\n",
      "5400 rows read\n",
      "5500 rows read\n",
      "5600 rows read\n",
      "5700 rows read\n",
      "5800 rows read\n",
      "5900 rows read\n",
      "6000 rows read\n",
      "6100 rows read\n",
      "6200 rows read\n",
      "6300 rows read\n",
      "6400 rows read\n",
      "6500 rows read\n",
      "6600 rows read\n",
      "6700 rows read\n",
      "6800 rows read\n",
      "6900 rows read\n",
      "7000 rows read\n",
      "7100 rows read\n",
      "7200 rows read\n",
      "7300 rows read\n",
      "7400 rows read\n",
      "7500 rows read\n",
      "7600 rows read\n",
      "7700 rows read\n",
      "7800 rows read\n",
      "7900 rows read\n",
      "8000 rows read\n",
      "8100 rows read\n",
      "8200 rows read\n",
      "8300 rows read\n",
      "8400 rows read\n",
      "8500 rows read\n",
      "8600 rows read\n",
      "8700 rows read\n",
      "8800 rows read\n",
      "8900 rows read\n",
      "9000 rows read\n",
      "9100 rows read\n",
      "9200 rows read\n",
      "9300 rows read\n",
      "9400 rows read\n",
      "9500 rows read\n",
      "9600 rows read\n",
      "9700 rows read\n",
      "9800 rows read\n",
      "9900 rows read\n",
      "10000 rows read\n",
      "10100 rows read\n",
      "10200 rows read\n",
      "10300 rows read\n",
      "10400 rows read\n",
      "10500 rows read\n",
      "10600 rows read\n",
      "10700 rows read\n",
      "10800 rows read\n",
      "10900 rows read\n",
      "11000 rows read\n",
      "11100 rows read\n",
      "11200 rows read\n",
      "11300 rows read\n",
      "11400 rows read\n",
      "11463 rows read, COMPLETE\n",
      "(5811, 5811)\n",
      "CPU times: user 28.6 s, sys: 340 ms, total: 29 s\n",
      "Wall time: 28.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read data and convert to Term-Document matrix\n",
    "TD = download_and_read(UCI_DATA_URL)\n",
    "# compute undirected, unweighted edge matrix\n",
    "E = TD.T * TD\n",
    "# binarize\n",
    "E[E > 0] = 1\n",
    "print(E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d3c520d6-ac4f-4766-91c8-e8222a8f1b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_random_walks(E, n, alpha, l, ofile):\n",
    "    \"\"\" NOTE: takes a long time to do, consider using some parallelization\n",
    "        for larger problems.\n",
    "    \"\"\"\n",
    "    if os.path.exists(ofile):\n",
    "        print(\"random walks generated already, skipping\")\n",
    "        return\n",
    "    f = open(ofile, \"w\")\n",
    "    for i in range(E.shape[0]):  # for each vertex\n",
    "        if i % 100 == 0:\n",
    "            print(\"{:d} random walks generated from {:d} starting vertices\"\n",
    "                .format(n * i, i))\n",
    "        if i <= 3273:\n",
    "            continue\n",
    "        for j in range(n):       # construct n random walks\n",
    "            curr = i\n",
    "            walk = [curr]\n",
    "            target_nodes = np.nonzero(E[curr])[1]\n",
    "            for k in range(l):   # each of max length l, restart prob alpha\n",
    "                # should we restart?\n",
    "                if np.random.random() < alpha and len(walk) > 5:\n",
    "                    break\n",
    "                # choose one outgoing edge and append to walk\n",
    "                try:\n",
    "                    curr = np.random.choice(target_nodes)\n",
    "                    walk.append(curr)\n",
    "                    target_nodes = np.nonzero(E[curr])[1]\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            f.write(\"{:s}\\n\".format(\" \".join([str(x) for x in walk])))\n",
    "\n",
    "    print(\"{:d} random walks generated from {:d} starting vertices, COMPLETE\"\n",
    "        .format(n * i, i))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b2127f-04c0-4cd2-a53f-230ded6a95d2",
   "metadata": {},
   "source": [
    "This next cell takes a real long time to run, and the source repo already has a copy of the output file, so simply replicate that 'random-walks.txt' from the source repo into the data subfolder, BEFORE running this next cell. (You can thank me later ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a238b6ed-531c-4afd-850c-df8c11082246",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random walks generated already, skipping\n",
      "CPU times: user 236 µs, sys: 4 µs, total: 240 µs\n",
      "Wall time: 155 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# construct random walks (caution: long process!)\n",
    "construct_random_walks(E, NUM_WALKS_PER_VERTEX, RESTART_PROB, \n",
    "    MAX_PATH_LENGTH, RANDOM_WALKS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "276c7f51-460a-4167-8d9f-6b1bba5213cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Documents(object):\n",
    "    def __init__(self, input_file):\n",
    "        self.input_file = input_file\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.input_file, \"r\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i % 1000 == 0:\n",
    "                    if i % 1000 == 0:\n",
    "                        logging.info(\"{:d} random walks extracted\".format(i))\n",
    "                yield line.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "219cd4ef-48d4-4c85-8844-c9c76f9421d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_word2vec_model(random_walks_file, model_file):\n",
    "    if os.path.exists(model_file):\n",
    "        print(\"Model file {:s} already present, skipping training\"\n",
    "            .format(model_file))\n",
    "        return\n",
    "    docs = Documents(random_walks_file)\n",
    "    model = gensim.models.Word2Vec(\n",
    "        docs,\n",
    "        vector_size=128,    # size of embedding vector\n",
    "        window=10,   # window size\n",
    "        sg=1,        # skip-gram model\n",
    "        min_count=2,\n",
    "        workers=4\n",
    "    )\n",
    "    model.train(\n",
    "        docs, \n",
    "        total_examples=model.corpus_count,\n",
    "        epochs=50)\n",
    "    model.save(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f7364-b93b-4a55-a2b0-fa06df858822",
   "metadata": {},
   "source": [
    "The next cell uses TensorFlow but it does NOT run on the GPU ... \n",
    "\n",
    "It generates the ./data/w2v-neurips-papers.model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b49cf49-9194-4c27-9583-0eb968c5cf18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file ./data/w2v-neurips-papers.model already present, skipping training\n",
      "CPU times: user 253 µs, sys: 5 µs, total: 258 µs\n",
      "Wall time: 222 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train model\n",
    "train_word2vec_model(RANDOM_WALKS_FILE, W2V_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e6d5ad0-8803-49f7-b28f-bfa228f2d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code that came with the repo. It has a lot of problems ... \n",
    "def evaluate_model(td_matrix, model_file, source_id):\n",
    "    model = gensim.models.Word2Vec.load(model_file).wv\n",
    "    most_similar = model.most_similar(str(source_id))\n",
    "    scores = [x[1] for x in most_similar]\n",
    "    target_ids = [x[0] for x in most_similar]\n",
    "    # compare top 10 scores with cosine similarity between source and each target\n",
    "    X = np.repeat(td_matrix[source_id].todense(), 10, axis=0)\n",
    "    Y = td_matrix[target_ids].todense()\n",
    "    cosims = [cosine_similarity(X[i], Y[i])[0, 0] for i in range(10)]\n",
    "    for i in range(10):\n",
    "        print(\"{:d} {:s} {:.3f} {:.3f}\".format(\n",
    "            source_id, target_ids[i], cosims[i], scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "115f06a6-6fcc-4dd6-877c-87cbc55dba2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# And this is the fixed version of the above pos code ... yes, it runs just fine. \n",
    "def evaluate_model(td_matrix, model_file, source_id):\n",
    "    model = gensim.models.Word2Vec.load(model_file).wv\n",
    "    most_similar = model.most_similar(str(source_id))\n",
    "    scores = [x[1] for x in most_similar]\n",
    "    # target_ids = [x[0] for x in most_similar]\n",
    "    target_ids = [int(x[0]) for x in most_similar]\n",
    "    # compare top 10 scores with cosine similarity between source and each target\n",
    "    X = np.repeat(td_matrix[source_id].todense(), 10, axis=0)\n",
    "    Y = td_matrix[target_ids].todense()\n",
    "    cosims = [cosine_similarity(np.asarray(X[i]), np.asarray(Y[i]))[0, 0] for i in range(10)]\n",
    "    for i in range(10):\n",
    "        print(\"{:d} {:d} {:.3f} {:.3f}\".format(source_id, target_ids[i], cosims[i], scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5210e0f-0914-4135-a935-1a962bf076ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 17:34:26,122 : INFO : loading Word2Vec object from ./data/w2v-neurips-papers.model\n",
      "2023-02-14 17:34:26,125 : INFO : loading wv recursively from ./data/w2v-neurips-papers.model.wv.* with mmap=None\n",
      "2023-02-14 17:34:26,126 : INFO : setting ignored attribute cum_table to None\n",
      "2023-02-14 17:34:26,155 : INFO : Word2Vec lifecycle event {'fname': './data/w2v-neurips-papers.model', 'datetime': '2023-02-14T17:34:26.155721', 'gensim': '4.3.0', 'python': '3.8.10 (default, Jun 22 2022, 20:18:18) \\n[GCC 9.4.0]', 'platform': 'Linux-5.15.0-60-generic-x86_64-with-glibc2.29', 'event': 'loaded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5239 3956 0.004 0.342\n",
      "5239 17 0.027 0.324\n",
      "5239 3285 0.005 0.319\n",
      "5239 4552 0.008 0.314\n",
      "5239 806 0.012 0.305\n",
      "5239 2408 0.046 0.303\n",
      "5239 5509 0.072 0.300\n",
      "5239 4682 0.031 0.300\n",
      "5239 5498 0.003 0.299\n",
      "5239 4164 0.006 0.296\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "source_id = np.random.choice(E.shape[0])\n",
    "evaluate_model(TD, W2V_MODEL_FILE, source_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587288a-eefa-49fc-aba9-15b5a6028bb6",
   "metadata": {},
   "source": [
    "## Dynamic embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c23e79-cf3b-4b94-9366-702452ab8317",
   "metadata": {},
   "source": [
    "(code source: Chapter_4/elmo_from_tfhub.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c60cdc3-2d86-48a7-b508-51752192c6ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 17:34:26,196 : INFO : Using /tmp/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d8b1471-d15f-46cb-bc72-168141510c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 7, 1024)\n"
     ]
    }
   ],
   "source": [
    "embeddings = elmo.signatures[\"default\"](\n",
    "    tf.constant([\n",
    "      \"i like green eggs and ham\",\n",
    "      \"would you eat them in a box\"\n",
    "    ]))[\"elmo\"]\n",
    "\n",
    "print(embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf6b30-7beb-49d2-93f0-709c69ee0e7b",
   "metadata": {},
   "source": [
    "(code source: Chapter_4/elmo_keraslayer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac96130c-bfc8-4db2-8673-fd448dff3137",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 17:34:28,639 : WARNING : Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "embed = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/elmo/3\",\n",
    "    input_shape=[],     # Expects a tensor of shape [batch_size] as input.\n",
    "    dtype=tf.string)    # Expects a tf.string input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3706d75b-f06b-4e01-bc1f-d19d700c7ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1024)              93600852  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,600,852\n",
      "Trainable params: 0\n",
      "Non-trainable params: 93,600,852\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([embed])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a16927d8-c3c1-4e3e-916c-c06bdc95b778",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 323ms/step\n",
      "(2, 1024)\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.predict([\n",
    "   \"i i like green eggs and ham\",\n",
    "   \"would you eat them in a box\"\n",
    "])\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497bef6-9925-48ed-afe3-2045af256273",
   "metadata": {},
   "source": [
    "## Sentence and paragraph embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dce682-1628-4d1e-b986-c7ef42f8a037",
   "metadata": {},
   "source": [
    "(code source: Chapter_4/google_sent_enc_from_tfub.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17089e70-279d-4c1d-a903-6f94cb6c464f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32ddc25a-a586-4e5a-b12d-9f1cb127304e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 512)\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed([\n",
    "   \"i like green eggs and ham\",\n",
    "   \"would you eat them in a box\"\n",
    "])[\"outputs\"]\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eefc2abe-a590-4741-8cca-73a8ff9e0003",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Run Date: Tuesday, February 14, 2023\n",
      "# Run Time: 00:01:26\n"
     ]
    }
   ],
   "source": [
    "endTime = time.time()\n",
    "\n",
    "elapsedTime = time.strftime(\"%H:%M:%S\", time.gmtime(endTime - startTime))\n",
    "\n",
    "print(todaysDate.strftime('# Run Date: %A, %B %d, %Y'))\n",
    "print(f\"# Run Time: {elapsedTime}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
